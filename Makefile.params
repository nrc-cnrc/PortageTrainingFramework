# vim:noet:ts=3:nowrap
# @author Samuel Larkin
# @brief Master parameter file where all user specific parameters should be set.
#
# Technologies langagieres interactives / Interactive Language Technologies
# Inst. de technologie de l'information / Institute for Information Technology
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2008, 2012, Sa Majeste la Reine du Chef du Canada
# Copyright 2008, 2012, Her Majesty in Right of Canada

# Print the PortageII Copyright no matter where we start from -- but only once.
ifeq (${MAKELEVEL},0)
   $(shell portage_info)
   # subprograms launched by the framework don't need to bleet all the time either...
   export PORTAGE_INTERNAL_CALL=1
endif

################################################################################
# User definable variables

# This is the from/source language (must be two lowercase letters)
SRC_LANG ?= en
# This is the to/target language (must be two lowercase letters)
TGT_LANG ?= fr

# Here we specify the stem of the corpora files.
# Files should look like this: <PREFIX>_<LANGUAGE>.al
# e.g. test1_fr.al
# Warning: TRAIN_TC, TUNE_DECODE, TUNE_RESCORE, TUNE_CE may not contain
#          more than one <PREFIX>.

# TRAIN_LM is used to create language models. 
# NOTE: Do not include stems for any files listed in LM_PRETRAINED_TGT_LMS below.
TRAIN_LM      ?= lm-train

# LM_PRETRAINED_TGT_LMS specifies file paths to additional pre-trained target
# language LMs to use with the lm module, for example a generic model LM.
# These file names should end with the extension .binlm.gz and must include
# the string _${TGT_LANG}, where ${TGT_LANG} is the target language, e.g. _fr .
# Paths may be absolute or relative to the location of this Makefile.params file.
#LM_PRETRAINED_TGT_LMS ?= ${PORTAGE}/generic-model/lm/generic1.0_fr.binlm.gz

# MIXLM is used to create a mixture language model which itself is composed of
# several other language models.
# NOTE: Do not include stems for any files listed in MIXLM_PRETRAINED_TGT_LMS below.
#MIXLM         ?= sublm1 sublm2 sublm3

# MIXLM_PRETRAINED_TGT_LMS specifies file paths to additional pre-trained target
# language LMs to use with the mixlm module, for example a generic model LM.
# These file names should end with the extension .binlm.gz and must include
# the string _${TGT_LANG}, where ${TGT_LANG} is the target language, e.g. _fr .
# Paths may be absolute or relative to the location of this Makefile.params file.
# Note: The corresponding source language LMs are also required, should have
# the same name except for the language code, and should be located in the same
# directory.
#MIXLM_PRETRAINED_TGT_LMS ?= ${PORTAGE}/generic-model/lm/generic1.0_fr.binlm.gz

# TRAIN_TC is used to create a truecasing model.
# By default, we use the first name listed in TRAIN_LM, thus if you do not, 
# uncomment and define TRAIN_TC
# => TRAIN_TC = first word of TRAIN_LM
#TRAIN_TC ?= tc-train

# TRAIN_TM is used to create the translation tables.
# NOTE: Do not include stems for any files listed in TM_PRETRAINED_TMS below.
TRAIN_TM      ?= tm-train

# TM_PRETRAINED_TMS specifies file paths to additional pre-trained translation
# tables to use with the tm module, for example a generic model TM.
# These file names start with cpt. and end in .${SRC_LANG}2${TGT_LANG}.gz,
# where ${SRC_LANG} is the source language and ${TGT_LANG} is the target
# language, e.g. .en2fr.gz .
# Paths may be absolute or relative to the location of this Makefile.params file.
#TM_PRETRAINED_TMS ?= ${PORTAGE}/generic-model/tm/cpt.generic1.0.en2fr.gz

# MIXTM is used to create a mixture translation model from several other
# translation models.
# Specify the in-domain corpus first (its word alignment models are needed),
# or specify an alternate training corpus in MIXTM_TRAIN_MIX below.
# NOTE: Do not include stems for any files listed in MIXTM_PRETRAINED_TMS below.
#MIXTM         ?= subtm1 subtm2

# MIXTM_PRETRAINED_TMS specifies file paths to additional pre-trained translation
# tables to use in creating a mixture translation model, for example a generic 
# model TM.
# These file names should start with cpt.
# Paths may be absolute or relative to the location of this Makefile.params file.
#MIXTM_PRETRAINED_TMS ?= ${PORTAGE}/generic-model/tm/cpt.generic1.0.en2fr.gz

# MIXTM_TRAIN_MIX is an in-domain corpus used to train the weights for mixing 
# the component translation models (its word alignment models are needed)
# By default, we use the first name listed in MIXTM; if you want to use a
# different corpus, uncomment and define MIXTM_TRAIN_MIX.
# => MIXTM_TRAIN_MIX = first word of MIXTM
#MIXTM_TRAIN_MIX ?= subtm1

# TRAIN_LDM is used to train a Lexicalized Distortion Model (LDM)
# (the word alignment models are needed).
# By default, we use the corpora listed in TRAIN_TM and MIXTM; if you want to
# use different corpora, uncomment and define TRAIN_LDM.
# => TRAIN_LDM = TRAIN_TM MIXTM
#TRAIN_LDM ?= ldm-train

# TRAIN_HLDM is used to train a Hierarchical Lexicalized Distortion Model (HLDM)
# (the word alignment models are needed).
# By default, we use the corpora listed in TRAIN_LDM if it is defined, or
# TRAIN_TM and MIXTM if TRAIN_LDM is not defined; if you want to use different
# corpora, uncomment and define TRAIN_LDM.
# => TRAIN_HLDM = TRAIN_LDM or TRAIN_TM MIXTM
#TRAIN_HLDM ?= hldm-train

# TUNE_DECODE is used to tune the decoding weights (using tune.py).
TUNE_DECODE   ?= dev1
# TUNE_DECODE_VARIANTS is used to tune and test using multiple tuning sets.
# Typically, each variant is a 90% sample subset of the TUNE_DECODE set.
# Ex. If TUNE_DECODE is dev1, include "a b" in TUNE_DECODE_VARIANTS to tune 
#     with dev1a and dev1b in addition to dev1.
# If variants are specified, 90% sample variants of the tuning set are created
# automatically in the corpora directory if the variant files do not already exist.
#TUNE_DECODE_VARIANTS  ?= a b c d

# PLIVE_DECODE_VARIANT is used to select which tuning run (which one of
# TUNE_DECODE_VARIANTS) to use for PortageLive.
# Leave PLIVE_DECODE_VARIANT undefined or blank to use weights from the main
# tuning run based on TUNE_DECODE, or specify one of the variants from
# TUNE_DECODE_VARIANTS to use weights from the tuning run corresponding to
# that variant.
# Recommended usage: after tuning, set this to the variant giving the best
# bleu score on the test sets.
#PLIVE_DECODE_VARIANT ?=

# TUNE_RESCORE is used to tune the rescoring weights (using rat.sh).
#TUNE_RESCORE  ?= dev2

# TUNE_CE is used to train confidence estimation.
# Note: it's OK for TUNE_DECODE and TUNE_RESCORE to be the same file, but
# TUNE_CE must be completely distinct, not only from all training data, but
# also from all other tuning data.
#TUNE_CE       ?= dev3

# TEST_SET files are used to estimate the translation quality of the system.
TEST_SET      ?= test1 test2
# Uncomment if you have source text, that doesn't have a reference, to translate.
#TRANSLATE_SET ?=

# Uncomment if you have multiple references for your heldout sets.
# Note that all dev and test sets must have the same number of references.
# <stem>_<tgt_language>#.al
# Where # is some reference identifier.
# i.e. dev_fr1.al, dev_fr2.al, dev_fr3.al & dev_fr4.al
#REFERENCE_INDICES ?= 1 2 3 4

# Define USE_STATIC_MIXLM to translate using the dev1 mixlm weights instead
# of computing dynamic weights for each test set; this applies to mixlms only.
#USE_STATIC_MIXLM ?= 1

# Train and apply rescoring if this variable is defined.
# Comment out to disable rescoring; uncomment to enable.
# Expensive!  Use only if the last small BLEU increment is important to you.
#DO_RESCORING = 1

# Tune and apply confidence estimation if this variable is defined.
# Comment out to disable confidence estimation; uncomment to enable.
#DO_CE = 1

# Train and use a Lexicalized Distortion Model (LDM).
# Comment out to disable using an LDM; uncomment to enable.
# Expensive!  Use only if you know it's worthwhile.
#USE_LDM = 1

# Train and use a Hierarchical Lexicalized Distortion Model (HLDM).
# Comment out to disable using an HLDM; uncomment to enable.
# Expensive!  Use only if you know it's worthwhile.
# HLDM seems to be quite effective in most scenarios, so we enable it by default.
USE_HLDM = 1

# Train and apply truecasing if this variable is defined.
# Comment out to disable truecasing; uncomment to enable.
DO_TRUECASING = 1

# Source language locale used during truecasing.
#SRC_LOCALE ?= ${SRC_LANG}_CA.utf8
#SRC_LOCALE ?= da_DK.utf8

# Target language locale used during truecasing.
#TGT_LOCALE ?= ${TGT_LANG}_CA.utf8

# If USE_SIGPRUNING is set, phrase tables will be filtered using significance
# pruning before they are used.  Significance pruning removes phrase pairs that
# are statistically well attested in the training corpus.  Sig-pruning results
# in much smaller phrase tables, usually without loss in BLEU, sometimes in
# fact with a gain in BLEU.
#USE_SIGPRUNING = 1

# If you have your own (de)tokenizer and you want to use it, then define the
# following variables to your pgm and its arguments.  Note that the variable
# names contain the source or target language two-letter identifier.  For,
# example, for Spanish source you could define TOKENIZER_es.  More generally,
# {DE|}TOKENIZER_{${SRC_LANG}|${TGT_LANG}}.
# Defining what tokenizers we want to use:
#TOKENIZER_en ?= opennlp TokenizerME /modeldir/en-model.bin
#TOKENIZER_fr ?= utokenize.pl -noss -lang=fr
#TOKENIZER_ch ?= { set -o pipefail; iconv -c -f UTF-8 -t CN-GB | ictclas_preprocessing.pl | ictclas | ictclas_postprocessing.pl | iconv -c -f CN-GB -t UTF-8; }
# Defining what detokenizers we want to use:
#DETOKENIZER_en ?= opennlp DetokenizerME /modeldir/en-model.bin
#DETOKENIZER_fr ?= udetokenize.pl -lang=fr

# If you have ictclas installed and want to use it, uncomment to enable ictclas.
# USE_ICTCLAS ?= 1

# Language specific set of command to mark source devs/tests.
#MARK_RULE_en ?= canoe-escapes.pl -add
#MARK_RULE_fr ?= canoe-escapes.pl -add
#MARK_RULE_ch ?= { chinese_rule_markup.pl | chinese_rule_create.pl; }

# Change LM_TOOLKIT's value depending on the LM toolkit you have.  If you use
# SRILM or MITLM, their executable scripts and programs must be on your PATH.
# LM_TOOLKIT={SRI,IRST,MIT} 
# where SRI  => SRILM toolkit
#       IRST => IRSTLM toolkit
#       MIT  => MITLM toolkit
LM_TOOLKIT = MIT

# The prefix_root where we can find IRSTLM/bin, which must also be on your
# PATH.  (Only needed if you are using IRSTLM - see next variable.)
IRSTLM ?= $(PORTAGE)/pkgs/irstlm

# How many CPUs should each PortageLive request use?
# You can increase this parameter if your PortageLive server has multiple
# cores.  Monitor use to make sure your server does not get saturated.  You can
# adjust this parameter later for a running system by changing the '-n <N>'
# parameter in soap-translate.sh for each installed PortageLive context.  To
# retroactively add parallelism to previously trained PortageLive contexts, add
# "-w=5 -n=<n>" to their soap-translate.sh.
PARALLELISM_LEVEL_PORTAGELIVE ?= 1

# If you are on a cluster that is run-parallel.sh friendly, define the
# following to force cluster mode.  You normally don't need to do so, though,
# since clusters are detected automatically below.
#USING_CLUSTER ?= 1

# If you are on a cluster but you want to force single computer mode,
# uncomment the following line:
#NOCLUSTER ?= 1

# Automatically detects if we are on a cluster.
ifeq ($(strip $(shell which-test.sh qsub && echo "true")),true)
   USING_CLUSTER ?= 1
endif
ifdef NOCLUSTER
   USING_CLUSTER =
endif

OSTYPE ?= $(shell uname -s)

ifdef USING_CLUSTER
PARALLELISM_LEVEL_CORPORA ?= 10
PARALLELISM_LEVEL_LM ?= 5
PARALLELISM_LEVEL_LDM ?= 30
PARALLELISM_LEVEL_TM ?= 5
PARALLELISM_LEVEL_TUNE_DECODE  ?= 10
PARALLELISM_LEVEL_TUNE_RESCORE ?= 10
PARALLELISM_LEVEL_TUNE_CONFIDENCE ?= 10
# Be careful not to over-parallelize for translation if models take long to load, especially if translating many test files.
# One can run canoe-timing-stats.pl on the resulting logs to help assess.
PARALLELISM_LEVEL_TRANSLATE    ?= 1
ifeq (${MAKELEVEL},0)
   $(info Running in cluster mode.)
endif
else
# Make sure we run in serial mode.
.NOTPARALLEL:
# Autodetect the number of available cpus on this machine.
ifneq (${OSTYPE},Darwin)
NCPUS := $(shell test -n "$$OMP_NUM_THREADS" && echo $$OMP_NUM_THREADS || grep processor /proc/cpuinfo | wc -l)
else
NCPUS := $(shell test -n "$$OMP_NUM_THREADS" && echo $$OMP_NUM_THREADS || sysctl -n hw.ncpu)
endif
PARALLELISM_LEVEL_CORPORA ?= ${NCPUS}
PARALLELISM_LEVEL_LM ?= ${NCPUS}
PARALLELISM_LEVEL_LDM ?= ${NCPUS}
PARALLELISM_LEVEL_TM ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_DECODE  ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_RESCORE ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_CONFIDENCE ?= ${NCPUS}
# Be careful not to over-parallelize for translation if models take long to load, especially if translating many test files.
# One can run canoe-timing-stats.pl on the resulting logs to help assess.
PARALLELISM_LEVEL_TRANSLATE    ?= 1
# Disable CLUSTER mode in all of PortageII's software.
export PORTAGE_NOCLUSTER=1
ifeq (${MAKELEVEL},0)
   $(info Running in local mode.)
endif
endif


########################################
# Sanitizing user inputs.
# Removing accidental user spaces which would otherwise confuse make.
SRC_LANG := $(strip ${SRC_LANG})
TGT_LANG := $(strip ${TGT_LANG})
TRAIN_LM := $(strip ${TRAIN_LM})
MIXLM := $(strip ${MIXLM})
TRAIN_TM := $(strip ${TRAIN_TM})
MIXTM := $(strip ${MIXTM})
TUNE_DECODE := $(strip ${TUNE_DECODE})
TUNE_DECODE_VARIANTS := $(strip ${TUNE_DECODE_VARIANTS})
TUNE_RESCORE := $(strip ${TUNE_RESCORE})
TUNE_CE := $(strip ${TUNE_CE})


################################################################################
# Advanced configuration variables

# Compress extension.
GZ ?= .gz

# Raw file's extension.
ALIGNX ?= .al

# Extension for rule files which must also be source file.
RULEX ?= _${SRC_LANG}.rule

# Language extension for phrase table corpora.
# Extension for corpora.
LANGX  ?= .lc
# Extension for compressed corpora.
LANGXZ ?= ${LANGX}${GZ}

# Language pair for this system.
LANGS ?= ${SRC_LANG} ${TGT_LANG}

USE_MIXTM := $(if $(strip ${MIXTM} ${MIXTM_PRETRAINED_TMS}),1)
ifdef USE_MIXTM
# By default we assume that the user wants to use the first MIXTM corpus (its 
# word alignment models are needed) to train the weights for mixing the 
# component translation models.
MIXTM_TRAIN_MIX ?= $(firstword ${MIXTM})
MIXTM_TRAIN_MIX := $(strip ${MIXTM_TRAIN_MIX})

# We need a dev set for tuning the mixture weights in a mixtm.
TUNE_MIXTM ?= ${TUNE_DECODE}
TUNE_MIXTM := $(strip ${TUNE_MIXTM})

# It may be beneficial to use a global Word Alignment Model for training a
# MIXTM. To do so, define MIXTM_USE_GLOBAL_WORD_ALIGNMENT_MODEL (uncomment):
#MIXTM_USE_GLOBAL_WORD_ALIGNMENT_MODEL ?= 1

ifdef REFERENCE_INDICES
TUNE_MIXTM_MULTIPLE_REFERENCES ?= $(strip ${TUNE_MIXTM}).multiple.references
endif
endif

# In the case where you want a merged_cpt, you will need to define the following:
# MERGED_CPT_ZN_MODEL & MERGED_CPT_JPT_TYPES
# What word alignment model to use for Zens-Ney's smoother when building a merged_cpt?
# MERGED_CPT_ZN_MODEL can be one of: IBM4, HMM3, HMM2, HMM1, IBM2 or IBM1
MERGED_CPT_ZN_MODEL ?= HMM3
# What type of jpts should be used to create the final merged_cpt?
# MERGED_CPT_JPT_TYPES can be sevaral of: IBM4, HMM3, HMM2, HMM1, IBM2 or IBM1
# Using IBM4 models in addition to IBM2 and HMM3 seems to give a small boost
# in BLEU for most scenarios, but training IBM4 models can be very expensive,
# so we do not include IBM4 by default for MERGED_CPT_JPT_TYPES.
MERGED_CPT_JPT_TYPES ?= IBM2 HMM3
# Are we using alignment indicator features?
MERGED_CPT_USE_ALIGNMENT_INDICATORS ?= 0

# Specify the alignment symmetrization strategy:
# The former default, IBMOchAligner 3, gives denser alignments and smaller phrase
# tables It works best for very large corpora.  This is equivalent to "diag" in
# Koehn, Och and Marcu (2003), and is often referred to as "grow-diag-final".
#ALIGNMENT_SYMMETRIZATION_OPTIONS = -a "IBMOchAligner 3"
# IBMOchAligner 4 gives sparser, higher confidence alignments, and larger
# phrase tables.  It works best for small to large corpora: at hundreds of
# thousands of sentence pairs, this is known to still be the better option.
# This is equivalent to "diag-and" in Koehn, Och and Marcu (2003), and is often
# referred to as "grow-diag-final-and".
ALIGNMENT_SYMMETRIZATION_OPTIONS = -a "IBMOchAligner 4"

# Define what type of phrase table we want to generate.
# Can be one or more: ibm2_cpt, hmm1_cpt, hmm2_cpt, hmm3_cpt, ibm4_cpt,
# merged_cpt, or indicator_cpt.
# Typically, merged_cpt or indicator_cpt is used alone.
# WARNING: changes here must be manually reflected in
# models/rescore/rescore-model.template and models/confidence/ce-notm.template:
# look for upper case tokens like HMM3FWD and similar ones nearby.
# PT_TYPES ?= ibm2_cpt hmm3_cpt
ifeq (${MERGED_CPT_USE_ALIGNMENT_INDICATORS},1)
   PT_TYPES ?= $(strip $(if ${TRAIN_TM}, indicator_cpt) \
                       $(if ${USE_MIXTM}, mix_cpt))
else
   PT_TYPES ?= $(strip $(if $(strip ${TRAIN_TM}), merged_cpt) \
                       $(if ${USE_MIXTM}, mix_cpt))
endif

# By default, we assume that the user wants to use the corpora listed in 
# TRAIN_TM and MIXTM to build the Lexicalized Distortion Model (LDM).
ifdef USE_LDM
   TRAIN_LDM ?= $(sort ${TRAIN_TM} ${MIXTM})
endif
TRAIN_LDM := $(strip ${TRAIN_LDM})

# By default, we assume that the user wants to use the corpora listed in
# TRAIN_LDM to build the Hierarchical Lexicalized Distortion Model (HLDM)
# if TRAIN_LDM is defined, or the corpora listed in TRAIN_TM and MIXTM if
# TRAIN_LDM is not defined.
ifdef USE_HLDM
   TRAIN_HLDM ?= $(or ${TRAIN_LDM}, $(sort ${TRAIN_TM} ${MIXTM}))
endif
TRAIN_HLDM := $(strip ${TRAIN_HLDM})

# Define what type of language model we want to generate.
LM_TYPES ?= binlm

# Parameters for models/decode/Makefile
#TEMPLATE_DIR    ?= ${ROOT_DIR}/models/decode
PREFIX_DEV_COW  ?= ${TUNE_DECODE}
PREFIX_DEV_RAT  ?= ${TUNE_RESCORE}

# By default we assume that the user wants to use the first corpus in TRAIN_LM
# to build the truecasing models.
ifdef DO_TRUECASING
   TRAIN_TC ?= $(firstword ${TRAIN_LM})
endif
TRAIN_TC := $(strip ${TRAIN_TC})

# Defines the truecasing model filename.
TRUECASING_MAP ?= ${TRAIN_TC}_${TGT_LANG}.map
TRUECASING_LM  ?= ${TRAIN_TC}_${TGT_LANG}-kn-3g.binlm${GZ}
# Should we also use source language information in truecasing?
# NOTE: use of source language models in truecasing is not compatible with rescoring.
# Comment out to disable use of source language info; uncomment to enable.
TC_USE_SRC_MODELS = 1
ifdef TC_USE_SRC_MODELS
TRUECASING_NC1_SRC_LM  ?= ${TRAIN_TC}_${SRC_LANG}.nc1.binlm${GZ}
endif

# Determines whether TMs will include the alignment (a=) field or not.
# This field is used for truecasing and tags transfer.
TMS_WITH_ALIGNMENT_FIELD ?= 1

# When working with TMX files, we assume the language code in the TMX is the
# upper case of $SRC_LANG/$TGT_LANG followed by -CA.  When that's not true,
# uncomment the following and declare the correct strings here.
#TMX_SRC = EN-CA
#TMX_TGT = FR-CA

# If we are lucky enough to have a cluster, we'll change the shell for certain
# commands and allow them to run on nodes.
ifdef USING_CLUSTER
FRAMEWORK_SHELL = run-parallel.sh
else
FRAMEWORK_SHELL = /bin/bash
endif

# Some commands shouldn't be run with the cluster shell, will use this one
# instead.
LOCAL_SHELL = /bin/bash


# Auto-detecting if PortageII was compiled with ICU.
ifeq (${MAKELEVEL},0)
   PORTAGE_VERSION:=$(shell portage_info -version)
   ifeq (${PORTAGE_VERSION},)
      $(warning Cannot find portage_info; make sure some version Portage is installed.)
   endif
endif
ifeq ($(strip $(shell portage_info -with-icu > /dev/null && echo "true")),true)
   ICU = 1
   ifeq (${MAKELEVEL},0)
      $(info ${PORTAGE_VERSION} was compiled with ICU)
   endif
else
   ifeq (${MAKELEVEL},0)
      $(info ${PORTAGE_VERSION} was compiled without ICU)
   endif
endif



# We include src_lang specific configuration just before we validate the configuration.
-include Makefile.params.${SRC_LANG}

########################################
# VALIDATION
ifdef DO_CE
ifeq (${TUNE_CE},)
$(error When asking for confidence estimation, you must also define a TUNE_CE!)
endif
ifneq (${REFERENCE_INDICES},)
$(error Multiple references is not supported with confidence estimation!)
endif
endif

ifdef DO_RESCORING
ifeq (${TUNE_RESCORE},)
$(error When asking for rescoring, you must also define a TUNE_RESCORE!)
endif
ifdef TC_USE_SRC_MODELS
$(error When asking for rescoring, you must not define TC_USE_SRC_MODELS!)
endif
endif

ifeq ($(strip ${TRAIN_LM} ${LM_PRETRAINED_TGT_LMS} ${MIXLM} ${MIXLM_PRETRAINED_TGT_LMS}),)
$(error You must always define a training corpus and/or pretrained models for language models)
endif

ifeq ($(strip ${TRAIN_TM} ${TM_PRETRAINED_TMS} ${MIXTM} ${MIXTM_PRETRAINED_TMS}),)
$(error You must always define a training corpus and/or pretrained models for translation models)
endif

ifdef USE_MIXTM
ifeq (${MIXTM_TRAIN_MIX},)
$(error With a mixtm, you must always define a training corpus for training the mixture weights)
endif
ifneq ($(words ${MIXTM_TRAIN_MIX}),1)
$(error You cannot provide more than one corpora in MIXTM_TRAIN_MIX)
endif
ifeq (${TUNE_MIXTM},)
$(error With a mixtm, you must always define a tuning corpus for tuning the mixture weights)
endif
endif

ifdef DO_TRUECASING
ifeq (${TRAIN_TC},)
$(error With truecasing, you must always define a training corpus for truecasing models)
endif
endif

ifdef USE_LDM
ifeq (${TRAIN_LDM},)
$(error With USE_LDM, you must always define a training corpus for training the distortion model)
endif
endif

ifdef USE_HLDM
ifeq (${TRAIN_HLDM},)
$(error With USE_HLDM, you must always define a training corpus for training the distortion model)
endif
endif

ifeq (${TUNE_DECODE},)
$(error You must always define a tuning corpus to train the decoder)
endif

ifeq (${SRC_LANG},)
$(error You must provide a SRC_LANG!)
endif

ifeq (${TGT_LANG},)
$(error You must provide a TGT_LANG!)
endif

ifeq (${SRC_LANG},${TGT_LANG})
$(error SRC_LANG=${SRC_LANG} cannot be the same as TGT_LANG=${TGT_LANG}!)
endif

# Warn the user that the IBM4 feature is not available to him since he doesn't have mgiza.
ifneq ($(or $(filter IBM4, ${MERGED_CPT_ZN_MODEL} ${MERGED_CPT_JPT_TYPES}), $(filter ibm4_cpt, ${PT_TYPES})),)
ifneq ($(strip $(shell which-test.sh mgiza && echo "true")),true)
$(error You cannot use the IBM4 feature since you don't have mgiza installed.)
endif
endif

