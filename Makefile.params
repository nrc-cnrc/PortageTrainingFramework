# vim:noet:ts=3:nowrap
# $Id$
# @author Samuel Larkin
# @brief Master parameter file where all user specific parameters should be set.
#
# Technologies langagieres interactives / Interactive Language Technologies
# Inst. de technologie de l'information / Institute for Information Technology
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2008, Sa Majeste la Reine du Chef du Canada
# Copyright 2008, Her Majesty in Right of Canada

# Print the Portage Copyright no matter where we start from -- but only once.
ifeq (${MAKELEVEL},0)
   $(shell portage_info)
endif

################################################################################
# User definable variables

# This is the from/source language
SRC_LANG ?= en 
# This is the to/target language
TGT_LANG ?= fr

# Here we specify the stem of the corpora files.
# Files should look like this: <PREFIX>_<LANGUAGE>.al
# e.g. test1_fr.al
# Warning: only TEST_SET may contain more than one <PREFIX>.

# TRAIN_LM is used to create a language model.
TRAIN_LM      ?= lm-train

# TRAIN_TC is used to create a truecasing model.
# By default, we reuse TRAIN_LM thus if you do not uncomment and define it
# => TRAIN_TC = TRAIN_LM
#TRAIN_TC := tc-train

# MIXLM is used to create a mixture language model which itself is composed of
# several other language models.
#MIXLM         ?= sublm1 sublm2 sublm3

# TRAIN_TM is used to create the translation table.
TRAIN_TM      ?= tm-train
# TUNE_DECODE is used to train cow.
TUNE_DECODE   ?= dev1
# TUNE_RESCORE is used to train rat.
TUNE_RESCORE  ?= dev2
# TUNE_CE is used to train confidence estimation.
# Note: it's OK for TUNE_DECODE and TUNE_RESCORE to be the same file, but
# TUNE_CE must be completely distinct, not only from all training data, but
# also from all other tuning data.
#TUNE_CE       ?= dev3
# TEST_SET are used to estimate the quality of the system.
TEST_SET      ?= test1 test2
# Uncomment if you have source text, that doesn't have a reference, to translate.
#TRANSLATE_SET ?=

# The prefix_root where we can find IRSTLM/bin, which must also be on your
# PATH.  (Only need if you are using IRSTLM - see next variable.)
IRSTLM ?= $(PORTAGE)/pkgs/irstlm

# Change LM_TOOLKIT's value depending on the available lm toolkit that you
# have.  If you use SRILM or MITLM, their executable scripts and programs
# should be on your PATH.
# LM_TOOLKIT={SRI,IRST,MIT} 
# where SRI  => SRILM's toolkit
#       IRST => IRSTLM's toolkit
#       MIT  => MITLM's toolkit
LM_TOOLKIT = MIT

# Train and apply rescoring if this variable is defined.
# Comment out to disable rescoring.
DO_RESCORING = 1

# Tune and apply confidence estimation if this variable is defined.
# Comment out to enable confidence estimation.
#DO_CE = 1

# Train and use a Lexicalized Distortion Model.
# Comment out to disable using a Lexicalized Distortion Model.
#USE_LDM = 1

# Train and apply truecassing if this variable is defined.
# Comment out to disable truecasing.
DO_TRUECASING = 1

# Let say you have your own (de)tokenizer and you want to use it.  Define the
# following variables to your pgm and its arguments.  Note that the variable
# names contain your source or target language two letters identifyer.  Thus,
# if you do spanish you could define TOKENIZER_es.  More generally,
# {DE|}TOKENIZER_{${SRC_LANG}|$TGT_LANG}}.
# Defining what tokenizers we want to use.
#TOKENIZER_en = opennlp TokenizerME /modeldir/en-model.bin
#TOKENIZER_fr = utokenize.pl -noss -lang=fr
# Defining what detokenizers we want to use.
#DETOKENIZER_en = opennlp DetokenizerME /modeldir/en-model.bin
#DETOKENIZER_fr = udetokenizer.pl -lang=fr

# If you are on a cluster that is run-parallel.sh friendly, define the
# following to force cluster mode.  You normally don't need to do so, though,
# since clusters are detected automatically below.
#USING_CLUSTER = 1

# If you are on a cluster but you want to force single computer mode,
# uncomment the following line:
#NOCLUSTER = 1

# Automatically detects if we are on a cluster.
ifeq ($(strip $(shell which-test.sh qsub && echo "true")),true)
   USING_CLUSTER ?= 1
endif
ifdef NOCLUSTER
   USING_CLUSTER =
endif

OSTYPE = $(shell uname -s)

ifdef USING_CLUSTER
PARALLELISM_LEVEL_CORPORA ?= 10
PARALLELISM_LEVEL_LM ?= 5
PARALLELISM_LEVEL_TM ?= 5
PARALLELISM_LEVEL_TUNE_DECODE  ?= 10
PARALLELISM_LEVEL_TUNE_RESCORE ?= 10
PARALLELISM_LEVEL_TUNE_CONFIDENCE ?= 10
PARALLELISM_LEVEL_TRANSLATE    ?= 10
ifeq (${MAKELEVEL},0)
   $(info Running in cluster mode.)
endif
else
# Make sure we run in serial mode.
.NOTPARALLEL:
# Autodetect the number of available cpus on this machine.
ifneq (${OSTYPE},Darwin)
NCPUS := $(shell grep processor /proc/cpuinfo | wc -l)
else
NCPUS := $(shell sysctl -n hw.ncpu)
endif
PARALLELISM_LEVEL_CORPORA ?= ${NCPUS}
PARALLELISM_LEVEL_LM ?= ${NCPUS}
PARALLELISM_LEVEL_TM ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_DECODE  ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_RESCORE ?= ${NCPUS}
PARALLELISM_LEVEL_TUNE_CONFIDENCE ?= ${NCPUS}
PARALLELISM_LEVEL_TRANSLATE    ?= ${NCPUS}
# Disable CLUSTER mode in all of Portage's software.
export PORTAGE_NOCLUSTER=1
ifeq (${MAKELEVEL},0)
   $(info Running in local mode.)
endif
endif


########################################
# Sanitizing user inputs.
# Removing accidental user spaces which would otherwise confuse make.
SRC_LANG := $(strip ${SRC_LANG})
TGT_LANG := $(strip ${TGT_LANG})
TRAIN_LM := $(strip ${TRAIN_LM})
TRAIN_TM := $(strip ${TRAIN_TM})
TUNE_DECODE := $(strip ${TUNE_DECODE})
TUNE_RESCORE := $(strip ${TUNE_RESCORE})
TUNE_CE := $(strip ${TUNE_CE})


################################################################################
# Advanced configuration variables

# Compress extension.
GZ ?= .gz

# Raw file's extension.
ALIGNX ?= .al

# Extension for rule files which must also be source file.
RULEX ?= _${SRC_LANG}.rule

# Language extension for phrase table corpora.
# Extension for corpora.
LANGX  ?= .lc
# Extension for compressed corpora.
LANGXZ ?= ${LANGX}${GZ}

# Language pair for this system.
LANGS ?= ${SRC_LANG} ${TGT_LANG}

# By default will assume that the user wants to use the first corpora to build
# the truecasing models.
TRAIN_TC ?= $(firstword ${TRAIN_LM})
TRAIN_TC := $(strip ${TRAIN_TC})

# Define the corpora.
TRAIN_SET    ?= $(sort ${TRAIN_LM} ${TRAIN_TM} ${TRAIN_TC})
HELDOUT_SET  ?= $(sort ${TUNE_DECODE} ${TUNE_RESCORE} ${TUNE_CE} ${TEST_SET})
CORPORA_SET  ?= $(sort ${TRAIN_SET} ${HELDOUT_SET})

# Define what type of phrase table we want to generate.
# Can be: {ibm2_cpt | hmm1_cpt | hmm2_cpt | hmm3_cpt}
# WARNING: changes here must be manually reflected in
# models/rescore/rescore-model.template and models/confidence/ce-notm.template:
# look for upper case tokens like HMM3FWD and similar ones nearby.
PT_TYPES ?= ibm2_cpt hmm3_cpt

# Define what type of language model we want to generate.
LM_TYPES ?= binlm

# Parameters for models/decode/Makefile
#TEMPLATE_DIR    ?= ${ROOT_DIR}/models/decode
PREFIX_DEV_COW  ?= ${TUNE_DECODE}
PREFIX_DEV_RAT  ?= ${TUNE_RESCORE}

# Defines the truecasing model filename.
TRUECASING_MAP ?= ${TRAIN_TC}_${TGT_LANG}.map
TRUECASING_LM  ?= ${TRAIN_TC}_${TGT_LANG}-kn-3g.binlm${GZ}




# If we are lucky enough to have a cluster, we'll change the shell for certain
# commands and allow them to run on nodes.
ifdef USING_CLUSTER
FRAMEWORK_SHELL = run-parallel.sh
else
FRAMEWORK_SHELL = /bin/bash
endif

# Some commands shouldn't be run with the cluster shell, will use this one
# instead.
GUARD_SHELL = /bin/bash


# Auto-detecting if Portage was compiled with ICU.
ifeq ($(strip $(shell portage_info -with-icu > /dev/null && echo "true")),true)
   ICU = 1
   ifeq (${MAKELEVEL},0)
      $(info Portage was compiled with ICU)
   endif
else
   ifeq (${MAKELEVEL},0)
      $(info Portage was compiled without ICU)
   endif
endif



########################################
# VALIDATION
ifdef DO_CE
ifeq (${TUNE_CE},)
$(error "When asking for confidence estimation, you must also define a TUNE_CE!")
endif
endif

ifdef DO_RESCORING
ifeq (${TUNE_RESCORE},)
$(error "When asking for rescoring, you must also define a TUNE_RESCORE!")
endif
endif

ifeq ($(strip ${TRAIN_LM} ${MIXLM}),)
$(error "You must always define a training corpus for language model.")
endif

ifeq (${TRAIN_TM},)
$(error "You must always define a training corpus for translation model.")
endif

ifeq (${TRAIN_TC},)
$(error "You must always define a training corpus for truecasing  models.")
endif

ifeq (${TUNE_DECODE},)
$(error "You must always define a tuning corpus train the decoder.")
endif

ifeq (${SRC_LANG},)
$(error "You must provide a SRC_LANG!")
endif

ifeq (${TGT_LANG},)
$(error "You must provide a TGT_LANG!")
endif

ifeq (${SRC_LANG},${TGT_LANG})
$(error "SRC_LANG=${SRC_LANG} cannot be the same as TGT_LANG=${TGT_LANG}!")
endif

