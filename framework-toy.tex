\documentclass[11pt,letterpaper]{article}
\usepackage{isolatin1}
\usepackage{xspace}

\newcommand{\phs}{\tild{s}}   % source phrase
\newcommand{\pht}{\tild{t}}   % target phrase
% Official typesetting of PORTAGEshared
\newcommand{\PS}{PORTAGE\-\emph{shared}\xspace}

\usepackage{ifpdf}
\ifpdf
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\else
\fi

\title{A toy experiment using the \PS \\
       experimental framework (v1.3)}
\date{December, 2008}
\author{Eric Joanis}

\begin{document}

\vfill

\maketitle

\vfill

\begin{center}
An adaption of George Foster's \emph{Running Portage: A Toy Example} \\
to Samuel Larkin's experimental framework, updated to reflect recommended usage
of \PS.
\end{center}

\vfill
\vfill

\begin{center}
{~} \\ \footnotesize
   Technologies langagi{\`e}res interactives /
      Interactive Language Technologies \\
   Institut de technologie de l'information /
      Institute for Information Technology \\
   Conseil national de recherches Canada /
      National Research Council Canada \\
   Copyright \copyright 2008, Sa Majest{\'e} la Reine du Chef du Canada \\
   Copyright \copyright 2008, Her Majesty in Right of Canada
\end{center}

\vfill

\newpage

\vfill

\tableofcontents

\vfill

\newpage

\section{Introduction}

This document describes how to run a toy experiment using this experimental
framework from beginning to end.  It is intended as a tutorial in using \PS, as
well as a starting point for further experiments.  Although the framework
automates most of the steps described below, we go through them one by one here
in order to better explain how to use the \PS software suite.

\PS can be viewed as a set of programs for turning a bilingual corpus into a
translation system.  This document describes this process performed on a
trivially small data set.  The example is for French to English translation,
using text from the Hansard corpus.  It is too small for good translation, but
large enough to give the flavour of a more realistic setup. Running time is
several hours.

\subsection{Making sure \PS is installed}

To begin, you must build or obtain the \PS executables and ensure that they are
in your path, typically by sourcing the \texttt{SETUP.bash/ .tcsh} file as
customized for your environment during installation of \PS.  You should also
set your environment variable \texttt{\$PORTAGE} to the directory where \PS is
installed, which is also done by \texttt{SETUP.bash/ .tcsh}.  Follow the
instructions in \texttt{INSTALL} before you proceed with this document.

To make sure \PS is installed properly, try \texttt{canoe -h}, \texttt{cow.sh
-h} and \texttt{tokenize.pl -h}.  If you get usage information for each of
these programs, you should be ready to proceed.  If one of them gives you an
error message, then some part of your installation is incomplete.  See section
\emph{Verifying your installation} in \texttt{INSTALL} for troubleshooting
suggestions.

\subsection{Running the toy experiment}

Once you know \PS is correctly installed, you should make a complete copy of
this framework directory hierarchy, because it is designed to work in place,
creating the models within this hierarchy itself.  The philosophy of this
framework is that each experiment is done in a separate copy of the framework.

For example:
\begin{verbatim}
   > cd \$PORTAGE
   > cp -pr framework toy.experiment
   > cd toy.experiment
\end{verbatim}
All commands provided in the rest of this document are assumed to run in
toy.experiment or in a subdirectory thereof, which will all be specified
relative to toy.experiment.

As you work through the example, the commands that you are supposed to type are
preceded by a prompt, \texttt{>}, and the system's response is not, though
system output is not always fully reproduced here, for brevity's sake.  When it
is, results (especially numbers) may vary a little from the ones shown, due to
platform differences.

Many of the commands are expressed as \texttt{make} targets. This has the
advantage of requiring less typing, while still allowing you to see the actual
commands executed by the system, which are always echoed by \texttt{make} (and
which of course you could also type directly). \texttt{make} also lets you skip
steps (except for the first one, since it is done manually).  For example, if
you are not interested in any steps before decoder weight optimization, you can
go to section~\ref{COW} and type \texttt{make tune} to begin at that point
(although you will have to wait until the system executes the necessary
commands to catch up). Here are some other useful make commands:
\begin{itemize}
\item \texttt{make all} runs all remaining steps at any point.
\item \texttt{make clean} cleans up and returns the directory to its initial state
\item \texttt{make -j} \emph{anything} builds the target \emph{anything} by running
      commands in parallel whenever possible. This can be much faster if you
      have a multiprocessor machine.
\item \texttt{make help} displays some help and the main targets available in the
      makefile.
\end{itemize}

\subsection{Overview of the Process}

Here is an overview of the \PS process, as described in the following
sections (section numbers are in brackets):
\begin{enumerate}
\item Corpus preparation (\ref{CorpusPreparation}): includes tokenization,
      alignment, lowercasing, and splitting (\ref{Splitting}).
\item Model training (\ref{Training}): includes language model (\ref{LM}),
      translation model (\ref{TM}), truecasing model (\ref{TC}), decoder weight
      optimization (\ref{COW}), and rescoring model training (\ref{RAT}).
\item Translating and testing (\ref{TranslatingTesting}): includes
      translating (\ref{Translating}), truecasing (\ref{Truecasing}) and
      testing (\ref{Testing}).
\end{enumerate}
These steps are fairly standard, but of course there are many variants on the
sample process illustrated here. For more information, see the Portage HTML
user documentation. For detailed information on a particular program, run it
with the \texttt{-h} flag.

\section{Corpus Preparation} \label{CorpusPreparation}

Corpus preparation involves converting raw document pairs into tokenized,
sentence-aligned files in the format expected by Portage.  We provide a
tokenizer, a sentence aligner and sample corpus pre-processing scripts with
\PS, which can help you with these steps.  For details, see section \emph{Text
Processing} in the user manual (found in
\texttt{PORTAGEshared/doc/user-manual.html} on the CD).

\subsection{Splitting the Corpus} \label{Splitting}

The tokenized, sentence-aligned corpus must be split into separate portions
in order to run experiments. Distinct, non-overlapping sub-corpora are required
for model training (see section~\ref{Training}), for optimizing decoder weights
(section~\ref{COW}), for training a rescoring model (section~\ref{RAT}), and
one  testing (section~\ref{Testing}). Typically, the latter three corpora are
around 1000 segments each.  If the corpus is chronological, then it is a good
idea to choose the test corpus from the most recent material, which is likely
to resemble future text more closely.

Since proper splitting of your corpus needs to take into account the structure
and nature of your data, we don't perform those steps in this toy experiment.
Instead, we've provided small sets that can be found here:
\texttt{PORTAGEshared/test-suite/unit-testing/framework-toy}.  These sets
are very small to reduce running time, so you should not be surprised when the
results of this toy experiment are of poor quality.

To drop these sets into the framework, simply copy them (or make symbolic
links) into your copy's corpora directory:
\begin{verbatim}
   > cp $PORTAGE/test-suite/unit-testing/framework-toy/*.al corpora/
   > wc corpora/*.al
       100    2115   11589 corpora/dev1_en.al
       100    2330   13680 corpora/dev1_fr.al
       100    2095   11462 corpora/dev2_en.al
       100    2443   13905 corpora/dev2_fr.al
       100    2379   13292 corpora/test_en.al
       100    2616   15622 corpora/test_fr.al
      8992  184569  994516 corpora/train_en.al
      8992  211240 1194385 corpora/train_fr.al
     18584  409787 2268451 total
\end{verbatim}

In your own experiments, the files you need to copy into \texttt{corpora}
should be in original truecase (if you're using truecasing), tokenized,
sentence-split and aligned, just like the ones we provide here.

\subsection{Setting framework parameters} \label{FrameworkParams}

Now you need to tell the framework what you've called your test set, since we
didn't use the default.  Edit \texttt{Makefile.params} and change the
\texttt{TEST\_SET} variable to say
\texttt{test} instead of \texttt{test1 test2}.
This variable lists the prefix of all your test sets, and the framework
assumes you have two by default.
You will also need to set \texttt{TRAIN\_LM} and \texttt{TRAIN\_TM} to
\texttt{train} instead of \texttt{lm-train} and \texttt{tm-train} since we
won't be using a separate corpora to train the translation and language models.
After editing the file:
\begin{verbatim}
   > grep "export TEST_SET" Makefile.params
   export TEST_SET      ?= test
   > grep "export TRAIN_.M" Makefile.params
   export TRAIN_LM      ?= train
   export TRAIN_TM      ?= train
\end{verbatim}

While you're there, you should get familiar with all the variables in the
\emph{User definable variables} section of this file.  It is where most of the
configurable behaviours are set, such as whether to do rescoring and/or
truecasing, what LM toolkit you are using (if any), etc.  

In particular, the example is for translating from French to English, while the
default in the framework is the other direction.  To proceed as described
below, you should reverse the values of \texttt{SRC\_LANG} and
\texttt{TGT\_LANG}.
\begin{verbatim}
   > grep "_LANG " Makefile.params
   export SRC_LANG ?= fr
   export TGT_LANG ?= en
\end{verbatim}

In this toy experiment, we'll use the default value for most other parameters.

We recommend you use SRILM as your language modeling toolkit, if your
licensing requirements permit it.  If not, you can use IRSTLM instead.  In this
case, you need to comment out the \texttt{LM\_TOOLKIT\_SRI} variable and set
the \texttt{IRSTLM} variable to indicate where you installed it.  If you have
neither toolkit, leave both variables undefined; we provide trained language
models that you can drop into the framework to finish this toy experiment while
skipping language model training.

If you are using IRSTLM, after setting the \texttt{IRSTLM} variable in
\texttt{Makefile.params}, you need to type \texttt{make help} and cut and paste
the two export commands, unless you've already done the equivalent steps while
installing IRSTLM:
\begin{verbatim}
   > make help | head -3
   please run in order for this framework to run properly:
   export PATH=$PORTAGE/pkgs/irstlm/bin:$PATH
   export IRSTLM=$PORTAGE/pkgs/irstlm
   > export PATH=$PORTAGE/pkgs/irstlm/bin:$PATH
   > export IRSTLM=$PORTAGE/pkgs/irstlm
\end{verbatim}

Another set of parameters you might want to adjust are the various
\texttt{PARALLELISM\_LEVEL\_*} variables.  \PS is written in such a way as to
take advantage of multi-processor computers and/or multi-node computing
clusters, doing tasks in parellel where possible.  If you're not running on a
cluster, the number of CPUs on your machine is a good choice for all five
variables (the default is conservatively set to 1).  If your are running on a
cluster, the framework uses \texttt{qsub} to submit jobs, via the
\texttt{run-parallel.sh} and \texttt{psub} scripts, and you can set the five
variables according to resources available to you.

One more thing you will notice about this framework is that most commands run
by make are preceeded by \texttt{RP\_PSUB\_OPTS=...} or \texttt{\_LOCAL=1}.
These only have an impact when running on a cluster, and are written in such a
way that they are ignored otherwise.  When working on a cluster, commands
preceded by \texttt{\_LOCAL=1}, which are all very short commands, will be run
directly instead of being submitted to the queue, while
\texttt{RP\_PSUB\_OPTS=...} is used to specify additional options to
\texttt{psub}, which is used in \PS to encapsulate the invocation of
\texttt{qsub}.  If your cluster has specific usage rules or your require
additional parameters to \texttt{qsub}, you might want to customize
\texttt{psub} itself or add options as required in this framework.

\subsection{Lowercasing and adding escapes}

To reduce data sparseness, we convert all files to lowercase.  We keep the
lowercase and truecase versions separately, because we'll use the truecase
version to 
\begin{verbatim}
   > cd corpora
   > make lc
   cat dev1_fr.al | lc-utf8.pl > dev1_fr.lc
   [...]
\end{verbatim}

The decoder, canoe, treats \texttt{<}, \texttt{>} and \texttt{\\} as special
characters, in order to support markups for special translation rules.  We
won't use any markups for this example, but we still need to escape the three
special characters, in the source language only, since this is only for the
input to canoe.

\begin{verbatim}
   > make rule
   canoe-escapes.pl -add dev1_fr.lc > dev1_fr.rule
   canoe-escapes.pl -add dev2_fr.lc > dev2_fr.rule
   canoe-escapes.pl -add test_fr.lc > test_fr.rule
\end{verbatim}



\section{Training} \label{Training}

This step creates various models and parameter files that are required for
translation. There are five steps in training: creating a language model,
creating a truecasing model, creating translation models, optimizing decoder
weights, and creating a rescoring model.

\subsection{Creating a Language Model} \label{LM}

Portage does not come with programs for language model training. However, it
accepts models in the widely-used ``DARPA'' format which is produced by popular
toolkits such as CMU SLM (www.speech.cs.cmu.edu/ SLM\_info.html), SRILM
(www.speech.sri.com/projects/srilm), or IRSTLM (irstlm.sourceforge.net).

If you are using the SRILM toolkit, here is the command used to produce this
model:
\begin{verbatim}
   > make lm
   make -C models/lm all 
   Creating ARPA text format train_en-kn-5g.lm.gz
   ngram-count -interpolate -kndiscount -order 5 \
      -text ../../corpora/train_en.lc.gz -lm train_en-kn-5g.lm.gz \
      >& log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
The first command executed, \texttt{ngram-count}, produces the language model
itself in ``DARPA'' format.  The second command, \texttt{arpalm2binlm},
converts it into the Portage binary language model format, like in the IRSTLM
example.

If you are using the IRSTLM toolkit, here is the command used to produce the
model needed for this example, \texttt{train\_en-kn-5g.binlm.gz}:
\begin{verbatim}
   > cd ..             # only needed if you were still in corpora
   > make lm
   make -C models/lm all
   Marking ../../corpora/train_en.lc.gz
   zcat -f ../../corpora/train_en.lc.gz \
      | add-start-end.sh \
      | gzip -c \
      > train_en.marked.gz
   Creating IRSTLM train_en-kn-5g.ilm.gz
   build-lm.sh -p -t stat.$$ -n 5 -k 1 -s kneser-ney \
      -i "gunzip -c train_en.marked.gz" -o train_en-kn-5g.ilm.gz \
      >& log.train_en-kn-5g.ilm
   Creating ARPA text format train_en-kn-5g.lm.gz
   compile-lm --text yes train_en-kn-5g.ilm.gz unsorted.train_en-kn-5g.lm \
      >& log.train_en-kn-5g.lm \
   set -o pipefail; \
      compile-lm --text yes train_en-kn-5g.ilm.gz /dev/stdout \
      | egrep -v '^Saving in txt format to' \
      | lm_sort.pl | gzip > train_en-kn-5g.lm.gz
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
The first three commands executed, \texttt{add-start-end.sh},
\texttt{build-lm.sh} and \texttt{compile-lm}, construct the language model in
``DARPA'' format using IRSTLM software.  The script
\texttt{lm\_sort.pl} reorders the contents of the LM to maximize the
compression ratio \texttt{gzip} can achieve.  The sort step is unecessary with
a small language model, but can significantly reduce final file sizes with very
large language models.\footnote{We didn't do this sorting step with SRILM,
since its output already compresses well.}  The last command,
\texttt{arpalm2binlm}, converts the language model into the Portage binary
language model format.  This binary format contains the same information, but
is much faster to load in \PS software.

With either toolkit, this example assumes that the target language is English;
for English to French translation, a similar command could be used to create a
French language model.

If you have not installed any LM toolkit, we provide the file
\texttt{train\_en-kn-5g.lm.gz} in
\texttt{PORTAGEshared/test-suite/unit-testing/framework-toy/lm}.  You can copy
that file into \texttt{models/lm} and then type \texttt{make lm} to proceed
with the rest of this example, although you won't be able to complete your own
experiments without a language modeling toolkit.

In this toy example, we only use the target language part of the parallel
training corpus to train the language model, but this is not the recommended
practice.  If you have access to larger corpora of monolingual text in your
target language, you can use them to train additional language models.
The framework does not support training multiple language models, but you can
train them externally and add them to the \texttt{canoe.ini.template} file (see
section~\ref{COW}).

\subsection{Creating a Truecasing Model} \label{TC}

Decoding is done in lowercase to reduce data sparseness, but at the end of the
translation process, you might want to restore proper mixed case to your
output.  We call this step truecasing.  To train a truecasing model, you need
both a lowercased and the original ``truecase'' version of the training corpus
in the target language.

The truecasing model consists of two different models: a ``casemap'', which
maps each lower case words to its possible truecase variants, and a standard
language model trained on the truecase corpus.

\begin{verbatim}
   > make tc
   make -C models/tc all
   compile_truecase_map ../../corpora/train_en.tc.gz \
      ../../corpora/train_en.lc.gz \
      > train_en.map
   [commands to train the truecase LM]
\end{verbatim}
The first command, \texttt{compile\_truecase\_map}, compiles the casemap using
by reading the truecase and lowercased version of the corpus simultaneously.
The next commands will depend on your choice of LM toolkit, training a language
model as in section~\ref{LM} above, but using the truecase corpus,
\texttt{train\_en.tc.gz}, instead of the lowercased version,
\texttt{train\_en.lc.gz}, as above.

If you have not installed any LM toolkit, we provide the file
\texttt{train\_en-kn-3g.lm.gz} in
\texttt{PORTAGEshared/test-suite/unit-testing/framework-toy/tc}.  You can copy
that file into \texttt{models/tc} and and type \texttt{make tc} to proceed with
the rest of this example, although you won't be able to complete your own
experiments without a language modeling toolkit.

\subsection{Creating a Translation Model} \label{TM}

Creating a translation model involves two main steps: training IBM and HMM word
alignment models in both directions, then using them to extract phrase pairs
from the corpus.  This step was once fairly simple, but not the recommended
practice is to train and use two sets of phrase tables: one from IBM2 word
alignments, and one from HMM word alignments.  You can do all this by typing
\texttt{make tm} in your \texttt{toy.experiment} directory, but we'll break it
down in several steps here.

\subsubsection{Creating a Translation Model Using IBM2 Alignments} \label{IBM2}

First we train IBM2 word alignment models, which require training IBM1 models
as a prerequisite.  We do this for both directions.
\begin{verbatim}
   > cd models/tm
   > make ibm2_model
   Generating ibm1.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -n1 5 -n2 0 ibm1.train.en_given_fr.gz 
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.en_given_fr
   Generating ibm2.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -m -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -i ibm1.train.en_given_fr.gz ibm2.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.en_given_fr
   Generating ibm1.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 5 -n2 0 ibm1.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.fr_given_en
   Generating ibm2.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -i ibm1.train.fr_given_en.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.fr_given_en
\end{verbatim}
The \texttt{train\_ibm} program could have been used directly, but we use
\texttt{cat.sh} instead, so that we can take advantage of parallelism.  Here
the options \texttt{-n 1 -pn 1} disable all parallelism, because we ran it on a
non-clustered computer, but the degree of parallelism used is controlled by the
\texttt{PARALLELISM\_LEVEL\_TM} variable in \texttt{Makefile.params}.

The commands above write log files containing information about training.  For
example, the IBM log files show convergence and pruning statistics for each
iteration:
\begin{verbatim}
   > grep -h ppx log.ibm[12]*en_given_fr
   iter 1 (IBM1): prev ppx = 359.673, size = 1476170 word pairs; took 7 second(s).
   iter 2 (IBM1): prev ppx = 109.736, size = 1364397 word pairs; took 6 second(s).
   iter 3 (IBM1): prev ppx = 66.6961, size = 1194591 word pairs; took 6 second(s).
   iter 4 (IBM1): prev ppx = 53.0032, size = 1025745 word pairs; took 5 second(s).
   iter 5 (IBM1): prev ppx = 48.0923, size = 882560 word pairs; took 5 second(s).
   parallel iter (IBM2): prev ppx = 45.9329, size = 882290 word pairs.
   parallel iter (IBM2): prev ppx = 29.5655, size = 881943 word pairs.
   parallel iter (IBM2): prev ppx = 23.6397, size = 857564 word pairs.
   parallel iter (IBM2): prev ppx = 20.9975, size = 748588 word pairs.
   parallel iter (IBM2): prev ppx = 19.6976, size = 623553 word pairs.
\end{verbatim}

The IBM models are written to files \texttt{ibm[12].*} and contain word
translation/alignment probabilities.

Now we extract the joint-count phrase table from the same parallel corpus,
using IBM2 word alignment models in both directions together.
\begin{verbatim}
   > make ibm2_jpt
   Generating jpt.ibm2.train.fr-en.gz
   gen-jpt-parallel.sh -n 1 -o jpt.ibm2.train.fr-en.gz GPT \
      -v -j -w 1 -m 8 -ibm 2 -1 fr -2 en \
      ibm2.train.en_given_fr.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      2> log.jpt.ibm2.train.fr-en
\end{verbatim}
The joint-count phrase table contains the raw joint occurrence frequency for
each phrase pair observed in the parallel corpus.

Finally we compute the conditional phrase table, using three different
smoothers to calculate the probabilities.  In general, we obtain better
translation quality when using phrase probabilities estimated using relative
frequencies (``RFSmoother''), relative frequencies with Kneser-Ney smoothing
(``KNSmoother'') and the lexical smoothing proposed and Zens and Ney
(``ZNSmoother'').  Although output together in the same phrase table, they are
separate probability models, whose relative weights will be tuned during
decoder weight optimization (see section~\ref{COW}).
\begin{verbatim}
   > make ibm2_cpt
   Generating cpt.ibm2-rf-zn-kn3.train.fr2en.gz
   joint2cond_phrase_tables -sort -prune1 100 -ibm 2 -v -i -z \
      -1 fr -2 en -s RFSmoother -s ZNSmoother -s "KNSmoother 3" \
      -multipr fwd -o cpt.ibm2-rf-zn-kn3.train \
      -ibm_l2_given_l1  ibm2.train.en_given_fr.gz \
      -ibm_l1_given_l2  ibm2.train.fr_given_en.gz \
      jpt.ibm2.train.fr-en.gz \
      >& log.cpt.ibm2-rf-zn-kn3.train.fr2en
\end{verbatim}
The phrasetable \texttt{jpt.ibm2.train.fr-en.gz}, jointly with its HMM variant
(see below), is the main source of information for French to English
translation.  Each of its lines is of the form:
\begin{verbatim}
   fr ||| en ||| p(fr|en) p(en|fr)
\end{verbatim}
where \texttt{fr} is a French source phrase, \texttt{en} is an English target
phrase, \texttt{p(fr|en)} is the probability that \texttt{fr} is the
translation of \texttt{en}, and \texttt{p(en|fr)} is the probability that
\texttt{en} is the translation of \texttt{fr}.
Here are two sample lines from this file:
\begin{footnotesize}
\begin{verbatim}
   > zgrep '| proposed regulations |' cpt.ibm2-rf-zn-kn3.train.fr2en.gz 
   projets de règlement ||| proposed regulations ||| 0.66 0.012 0.35 1 0.01 0.52
   règlements proposés ||| proposed regulations ||| 0.33 1.2e-6 0.05 1 0.39 0.14
\end{verbatim}
\end{footnotesize}
You'll notice there are in fact six numbers per line.  That's because we used
three smoothers: we have three estimates of \texttt{p(fr|en)} followed by three
estimates of \texttt{p(en|fr)}.  Also, this example is carefully chosen;
because of the small size of the training corpus, many other entries in the
phrase table are incorrect.

\subsubsection{Creating a Translation Model Using HMM Alignments} \label{HMM}

Warning: we won't use the default settings here, so before you proceed, please
edit \texttt{Makefile.params} and change the \texttt{PT\_TYPES} variable to the
value \texttt{ibm2\_cpt hmm2\_cpt}.

Now we repeat the steps we did previously, but using HMM word alignment models,
choosing the ``liang'' variant (\texttt{hmm2}).
\begin{verbatim}
   > make hmm2_cpt
   Generating hmm2.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -n1 0 -n2 5 -mimic liang \
      -i ibm1.train.en_given_fr.gz hmm2.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.hmm2.train.en_given_fr
   Generating hmm2.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 0 -n2 5 -mimic liang \
      -i ibm1.train.fr_given_en.gz hmm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.hmm2.train.fr_given_en
   Generating jpt.hmm2.train.fr-en.gz
   gen-jpt-parallel.sh -n 1 -o jpt.hmm2.train.fr-en.gz \
      GPT -v -j -w 1 -m 8 -hmm -1 fr -2 en \
      hmm2.train.en_given_fr.gz hmm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      2> log.jpt.hmm2.train.fr-en
   Generating cpt.hmm2-rf-zn-kn3.train.fr2en.gz
   joint2cond_phrase_tables -sort -prune1 100 -hmm -v -i -z \
      -1 fr -2 en -s RFSmoother  -s ZNSmoother  -s "KNSmoother 3" \
      -multipr fwd -o  cpt.hmm2-rf-zn-kn3.train  \
      -ibm_l2_given_l1  hmm2.train.en_given_fr.gz \
      -ibm_l1_given_l2  hmm2.train.fr_given_en.gz \
      jpt.hmm2.train.fr-en.gz \
      >& log.cpt.hmm2-rf-zn-kn3.train.fr2en
\end{verbatim}
There are many variants to the HMM models.  It is not clear which one is the
best, so we don't try to cover all possibilities in this framework.
In this framework, we've include recipes to produce two variants: the one based
on Liang et al's baseline (enabled by specifying \texttt{-mimic liang}, and
invoked via the \texttt{hmm2\_*} targets in this framework); and the variant
with lexically conditioned jump parameters, following He (WMT-2006), which is
sometimes better (enabled by specifying \texttt{-mimic he-lex} as above, and
invoked via the \texttt{hmm1\_*} targets in this framework).

To read about other variants, type \texttt{train\_ibm -h}.  You'll need to
modify \texttt{models/tm/Makefile} to select the variants you want.  It's
probably easiest to modify the commands associated with the
\texttt{hmm1\_model} and \texttt{hmm2\_model} targets instead of creating new
targets.

For this toy experiment, the ``liang'' variant works best, which is why we
instructed you to type \texttt{make hmm2\_cpt} above and asked you to modify
\texttt{PT\_TYPES}: this variable enumerates which models you want to generate
when you type \texttt{make all}.  All models you generate will be used in
subsequent steps, so only generated the ones you intend to use.  (You should
work in different copies of the framework if you want to experiment with
different variants.)

\subsection{Optimizing Decoder Weights} \label{COW}

The language and translation models are the main sources of information used
for translation, which is performed by the \texttt{canoe} decoder. In order to get
reasonable translation quality, the weights on these (and other) sources of
information need to be tuned. The tuning process is carried out by a script
called \texttt{cow.sh}, which runs \texttt{canoe} several times. \texttt{cow.sh}
requires a configuration file for \texttt{canoe}, called \texttt{canoe.ini} by
default.

You can do all the steps below by typing \texttt{make cow} in your
\texttt{toy.experiment} directory, but we'll break it down in several steps
here.

\subsubsection{The Decoder Configuration File: canoe.ini}

In this framework, we start with a template configuration file,
\texttt{models/decode/ canoe.ini.template}:
\begin{verbatim}
   > cd ../..          # only needed if you were still in models/tm
   > cd models/decode
   > head -5 canoe.ini.template
   [ttable-multi-prob]
     <CPTS>
   [lmodel-file]
     <LMS>
   [weight-f] <ENABLE_FTM>
\end{verbatim}

The framework will automatically insert the phrase tables and language models
generated in the previous steps into the \texttt{canoe.ini} file when you
make it.  These are the template parameters it will replace:
\begin{itemize}
\item \texttt{<SL>}:   source language code, e.g. ``fr'';
\item \texttt{<TL>}:   target language code, e.g., ``en'';
\item \texttt{<CPTS>}: all conditional phrase tables trained above, and found
                       in the \texttt{models/tm} directory; and
\item \texttt{<LMS>}:  all language models trained trained above, and found in
                       the \texttt{models/lm} directory);
\item \texttt{<ENABLE\_FTM>}: the right number of forward translation
model weights.  This special token is required to enable the use of forward
translation model weights by \texttt{canoe} and their tuning by
\texttt{cow.sh}.  Unlike other weights, which are used by default as soon as
the corresponding model is present, forward weights are disabled by default.
In this framework, if you don't want to use forward weights, comment out or
delete the \texttt{[weight-f]} line in \texttt{canoe.ini.template}.
\end{itemize}

So we make the \texttt{canoe.ini} file:
\begin{verbatim}
   > make canoe.ini
   ln -sf ../../models models
   cat canoe.ini.template \
      | sed -e 's/<SL>/fr/g' \
            -e 's/<TL>/en/g' \
            -e 's/<CPTS>/models\/tm\/cpt.ibm2-rf-zn-kn3.train.fr2en.gz [...] \
            -e 's/<LMS>/models\/lm\/train_en-kn-5g.binlm.gz/g' \
      > canoe.ini
   configtool check canoe.ini
   ok
\end{verbatim}
The first command above creates a soft link to the models directory---this way,
all models appear to be relative to the current directly, which makes it easier
to find them.  We'll create such symlinks everywhere we need access to the
models.  The next command replaces the template parameters by the appropriate
values.  The final command, \texttt{configtool check canoe.ini}, confirms that the
\texttt{canoe.ini} file produced is error-free---it check that all parameters
are compatible and that all models can be found.

Here is the result:
\begin{verbatim}
   > cat canoe.ini
   [ttable-multi-prob]
     models/tm/cpt.hmm2-rf-zn-kn3.train.fr2en.gz
     models/tm/cpt.ibm2-rf-zn-kn3.train.fr2en.gz
   [lmodel-file]
     models/lm/train_en-kn-5g.binlm.gz
   [weight-f] 1:1:1:1:1:1
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [distortion-model] WordDisplacement
   [segmentation-model] none
   [cube-pruning]
   [bypass-marked]
\end{verbatim}

When using this framework, do not modify \texttt{canoe.ini}
directly.\footnote{If you're working in a different setting, you might set your
decoding parameters directly in your canoe.ini.}  To change the training
parameters for language models or phrase tables, modify the parameters or
Makefiles appropriately in \texttt{models/LM} and \texttt{models/TM} and
regenerate those models.  Be careful, though: the \texttt{canoe.ini} file will
include all models generated in the same framework, so you should work in
different copies of the framework if you want to experiment with different
training parameters for those models.

To change the other decoding parameters, modify \texttt{canoe.ini.template}.
For example, you can add additional language models (trained outside the
framework) by adding them in the \texttt{[lmodel-file]} section, separated by
whitespace or a newline from \texttt{<CPTS>}.

The other decoding parameters control the search strategy.  They have been set
to reasonable defaults, but may need to be adjusted depending on the
speed/quality trade-offs you're willing to make or other models you wish to
use.

\subsubsection{Running COW (Canoe Optimize Weights)}

Besides the configuration file, the other main arguments required by
\texttt{cow.sh} are a directory in which to store temporary files, a source
file and one or more correct (ie human) translations of the source
file.\footnote{This example uses only one translation, but when multiple
alternative translations are available, it can be advantageous to use them.}
Here we call the temporary directory \texttt{foos}, and use the \texttt{dev1}
files for weight tuning:
\begin{verbatim}
   > make all
   mkdir -p foos
   cow.sh -v -parallel:"-n 1" -maxiter 15 -nbest-list-size 100 \
      -filt -nofloor -workdir foos -f canoe.ini
      ../../corpora/dev1_fr.rule ../../corpora/dev1_en.lc \
      >& log.canoe.ini.cow
   rm -f -r foos multi.probs.*.FILT.gz canoe.ini.FILT canoe.ini.FILT.cow
\end{verbatim}
Because \texttt{cow.sh} takes a while to run (a good two to three hours in this
example)\footnote{Running time for \texttt{cow.sh} can be reduced by executing it
in parallel. Do \texttt{cow.sh -h} to see documentation on this option; modify
\texttt{PARALLELISM\_LEVEL\_TUNE\_DECODE} in \texttt{Makefile.params} to adjust
cow parallelism in this framework.} and writes large amounts of logging
information, it is usually a good idea to redirect all of its output to a log
file, as shown here; and also to run it in the background (not done here).
Progress is most easily monitored by looking at the file
\texttt{rescore-results},\footnote{The scripts \texttt{best-rr-val} and
\texttt{all-best-rr-vals} can be handy if you're monitoring multiple
experiments.} which shows the translation quality (measured by BLEU score---see
section~\ref{Testing} for a description), as well as the current weights, for
each iteration. Note that the BLEU score does not necessarily increase
monotonically, as can be seen in several places in the \texttt{rescore-results}
file obtained with this example:
\begin{footnotesize}
\begin{verbatim}
> cat rescore-results
BLEU score: 0.141236   -d 1 -w 0 -lm 1 -tm 1:1:1:1:1:1 -ftm 1:1:1:1:1:1
BLEU score: 0.000000   -d 0.145 -w -0.971 -lm 0.600 -tm -0.48:-0.18:-0.28:0.19:-1:0.09 -ftm 0.17:-0.22:-0.06:-0.28:-0.09:-0.07
BLEU score: 0.065014   -d 1 -w -0.066 -lm 0.040 -tm -0.05:-0.01:0.03:0.00:-0.01:0.00 -ftm 0.06:0.00:-0.03:-0.00:0.00:-0.00
BLEU score: 0.122222   -d 2.74e-05 -w -1 -lm 0.296 -tm -0.09:-0.01:-0.10:-0.02:0.00:0.02 -ftm 0.24:0.19:0.00:0.12:0.12:0.04
BLEU score: 0.173738   -d 0.219 -w -1 -lm 0.572 -tm 0.10:0.00:-0.00:0.03:0.00:-0.01 -ftm 0.02:0.28:-0.00:-0.00:-0.05:0.05
BLEU score: 0.177699   -d 0.362 -w 0.193 -lm 1 -tm 0.53:-0.00:-0.09:-0.08:0.00:-0.02 -ftm -0.05:0.57:-0.18:-0.04:0.03:0.08
BLEU score: 0.060503   -d 0.021 -w -0.078 -lm 1 -tm 0.48:-0.01:0.00:-0.00:-0.00:0.00 -ftm 0.00:-0.03:0.01:-0.00:-0.01:0.00
BLEU score: 0.186415   -d 0.141 -w -0.603 -lm 1 -tm 0.38:0.00:0.01:-0.00:-0.00:-0.00 -ftm 0.04:0.38:-0.02:0.01:-0.05:0.01
BLEU score: 0.189153   -d 0.159 -w 0.049 -lm 1 -tm 0.44:-0.01:0.00:0.00:-0.00:0.00 -ftm 0.23:0.09:-0.31:0.02:0.00:-0.02
BLEU score: 0.197739   -d 0.172 -w 0.120 -lm 1 -tm 0.43:-0.00:0.00:0.00:-0.00:-0.00 -ftm 0.22:0.15:-0.33:0.02:0.00:-0.01
BLEU score: 0.200836   -d 0.224 -w 0.118 -lm 1 -tm 0.46:-0.00:0.00:0.00:-0.00:-5.60e-05 -ftm 0.23:0.16:-0.33:0.02:0.00:-0.01
BLEU score: 0.172499   -d 0.117 -w -0.289 -lm 0.261 -tm 0.12:0.00:-0.00:0.00:-0.04:0.03 -ftm 1:0.26:-0.54:0.05:0.02:-0.02
BLEU score: 0.192449   -d 0.199 -w -0.387 -lm 1 -tm 0.17:0.00:0.00:-0.04:-0.01:0.00 -ftm 0.93:0.25:-0.63:0.06:0.02:-0.02
BLEU score: 0.193263   -d 0.205 -w -0.396 -lm 1 -tm 0.18:0.02:-0.00:-0.04:-0.00:-0.00 -ftm 0.99:0.22:-0.68:0.07:0.01:-0.01
BLEU score: 0.173176   -d 0.048 -w 0.003 -lm 1 -tm 0.62:-0.00:-0.21:0.07:-0.00:0.00 -ftm 0.00:0.30:0.00:-0.03:0.00:-0.00
\end{verbatim}
\end{footnotesize}
When the last iteration still shows an improvement (which is not the case here),
it is sometimes a sign that we should have allowed \texttt{cow.sh} to run more
iterations, by setting the \texttt{-maxiter} option to a higher value.  Here,
it looks like we achieved convergence after ten iterations.  You will notice
that there are six weights after \texttt{-tm}: three for the backward
probability estimates in the phrase table created using IBM2 models, and three
for the phrase table created using HMM models.  For the same reason, there are
six weights after \texttt{-ftm}.

While \texttt{cow.sh} is working, it is saving a lot of information in your
temporary work directory, which are automatically cleaned up afterwards in this
framework if no errors were encountered.  The large temporary files are seldom
of any use afterwards, except for troubleshooting, so there is no point in
keeping them.  If you would like to inspect them, possibly to see the n-best
lists or other files used by \texttt{cow.sh}, you can remove the \texttt{rm}
command executed after \texttt{cow.sh} in \texttt{Makefile}.  A lot of
information is also logged to \texttt{log.canoe.ini.cow}.  This file usually
contains too much information, though, but the script \texttt{cowpie.sh} can be
used to produce a summary of some of the information it contains.  Type
\texttt{cowpie.sh -h} for an interpretation of the output.

The final output from \texttt{cow.sh} is written to the file
\texttt{canoe.ini.cow} (assuming the original configuration file was called
\texttt{canoe.ini}). This duplicates the contents of \texttt{canoe.ini}, but
adds the optimal weights learned on the development corpus:
\begin{verbatim}
   > cat canoe.ini.cow
[ttable-multi-prob] 
   models/tm/cpt.ibm2-rf-zn-kn3.train.fr2en.gz
   models/tm/cpt.hmm2-rf-zn-kn3.train.fr2en.gz
[lmodel-file] 
   models/lm/train_en-kn-5g.binlm.gz
[weight-d] 0.224038288
[weight-w] 0.1183493733
[weight-l] 1
[weight-t] 0.465875:-0.006479:0.004425:0.004474:-0.000445:-5.607e-05
[weight-f] 0.233136:0.162551:-0.337420:0.023109:0.000215:-0.019836
[ttable-limit] 30
[ttable-threshold] 0
[stack] 20000
[beam-threshold] 0.001
[distortion-limit] 7
[distortion-model] 
   WordDisplacement
[segmentation-model] none
[bypass-marked]
[cube-pruning]
\end{verbatim}
where the \texttt{weight-} parameters pertain to, respectively, the distortion
model, the word-length penalty, the language model, the translation models
(backward scores), and the forward scores of the translation models. The
language model usually obtains the highest weight, as in this case.\footnote{A
note of warning about the LM weight: following Doug Paul's ``DARPA'' LM format,
all LM formats we know use base-10 log probs (including our own binary LM
format), but canoe interprets them as natural logs: throughout PORTAGEshared,
logs are natural by default, not base 10.  This known bug has minimal impact;
correcting it simply requires multiplying the desired -weight-l values by
log(10), which cow, rat and rescore\_train do implicitly.  We chose not to fix
it to avoid having to adjust all previously tuned sets of weights.}

\subsection{Training a Rescoring Model} \label{RAT}

The final training step is to create a model for rescoring n-best
lists. Rescoring means having \texttt{canoe} generate a list of $n$ (typically
1000) translation hypotheses for each source sentence, then choosing the best
translations from among these. The advantage of this procedure is that the
choice can be made on the basis of information that is too expensive for
\texttt{canoe} to use during search. This step usually gives a modest
improvement over the results obtained using \texttt{canoe} alone. Sometimes,
however, it gives no significant improvement while being fairly slow, so if
translation speed is an issue, you will probably want to skip rescoring.  In
this framework, rescoring is automatically bypassed unless
\texttt{DO\_RESCORING} is set to 1 in \texttt{Makefile.params}.

Training a rescoring model involves generating n-best lists, then calculating
the values of selected \emph{features} for each hypothesis in each list. A
feature is any real-valued function that is intended to capture the relation
between a source sentence and a translation hypothesis. A rescoring model
consists of a vector of feature weights set so as to optimize translation
performance when a weighted combination of feature values is used to reorder
the n-best lists.

You can do all the steps below by typing \texttt{make rat} in your
\texttt{toy.experiment} directory, but we'll break it down in several steps
here.

\subsubsection{The Input Rescoring Model}

Training is carried out by the \texttt{rat.sh} script. This takes as input a
rescoring model that specifies which features to use, and it returns optimal
weights for these features.

Before you proceed, you need to manually edit \texttt{rescore-model.template}
and change all occurrences of \texttt{HMM1} to \texttt{HMM2}, since we
generated the second HMM model variant for word alignment models (see
section~\ref{HMM}).

The default model created by this framework contains a small set of useful
features:
\begin{verbatim}
   > cd ../..          # only needed if you were still in models/decode
   > cd models/rescore
   > make rescore-model.ini
   ln -sf ../../models models
   set -o pipefail; configtool rescore-model:ffvals models/decode/canoe.ini.cow \
      | cut -f 1 -d ' ' > rescore-model.ini
   cat rescore-model.template \
      | sed -e "s#IBM1FWD#models/tm/ibm1.train.en_given_fr.gz#" \
      [...]
      >> rescore-model.ini
   > cat rescore-model.ini
   FileFF:ffvals,1
   FileFF:ffvals,2
   FileFF:ffvals,3
   FileFF:ffvals,4
   FileFF:ffvals,5
   FileFF:ffvals,6
   FileFF:ffvals,7
   FileFF:ffvals,8
   FileFF:ffvals,9
   FileFF:ffvals,10
   FileFF:ffvals,11
   FileFF:ffvals,12
   FileFF:ffvals,13
   FileFF:ffvals,14
   FileFF:ffvals,15
   # NB: this omits some features that are slow to compute. Use rescore_train -H
   # for a complete list.
   LengthFF
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz
   HMMTgtGivenSrc:models/tm/hmm2.train.en_given_fr.gz
   HMMSrcGivenTgt:models/tm/hmm2.train.fr_given_en.gz
   HMMVitTgtGivenSrc:models/tm/hmm2.train.en_given_fr.gz
   HMMVitSrcGivenTgt:models/tm/hmm2.train.fr_given_en.gz
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   nbestWordPostSrc:1#<ffval-wts>#<pfx>
   nbestWordPostTrg:1#<ffval-wts>#<pfx>
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx>
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx>
   nbestNgramPost:2#1#<ffval-wts>#<pfx>
   nbestSentLenPost:1#<ffval-wts>#<pfx>
   ParMismatch
   QuotMismatch:ef
   #CacheLM:<src>.id
   RatioFF
\end{verbatim}
There are two kinds of features included in our rescoring model:
\begin{itemize}
\item Those that look like \texttt{FileFF:ffvals,}$i$ tell \texttt{rat.sh} to
use the $i$\/th feature generated by the \texttt{canoe} decoder itself. It is
standard practice to use all decoder features when rescoring, as is done
automatically by the framework via the \texttt{configtool} command executed
above.
\item The other features, following the format \emph{FeatureName:Args}, tell
\texttt{rat.sh} to generate values for the feature \emph{FeatureName} using
arguments \emph{Args}.  For example,
\texttt{IBM2TgtGivenSrc:model/tm/ibm2.train.en\_given\_fr.gz} says to calculate
the feature \texttt{IBM2TgtGivenSrc} using the IBM model
\texttt{model/tm/ibm2.train.en\_given\_fr.gz}, which we trained earlier. To
see a list of all available features, type \texttt{rescore\_train -H}.
\end{itemize}

Lines starting with \texttt{\#} are comments and are ignored by the software.

\subsubsection{Running RAT (Rescore And Translate)}

Apart from the rescoring model, {\tt rat.sh} needs a source file and one or
more alternative translations for it (same as {\tt cow.sh}). These may be the
same files used for {\tt cow.sh}, but it is generally better to use different
ones, so here we use {\tt dev2}:\footnote{As with \texttt{cow.sh}, you can
speed this up using parallelism, in this case via the -n option to
\texttt{rat.sh}, given before the \texttt{train} token (type \texttt{rat.sh -h}
for details), controlled in this framework via the variable
\texttt{PARALLELISM\_LEVEL\_TUNE\_RESCORE} in \texttt{Makefile.params}.}
\begin{verbatim}
   > make train
   cat models/decode/canoe.ini.cow > canoe.ini.cow.dev2
   configtool check canoe.ini.cow.dev2
   ok
   Tuning the rescoring model.
   rat.sh -lb -n 1 train -v -K 1000 -o rescore-model \
      -msrc ../../corpora/dev2_fr.rule \
      -f canoe.ini.cow.dev2 rescore-model.ini \
      ../../corpora/dev2_fr.lc ../../corpora/dev2_en.lc \
      >& log.rescore-model
\end{verbatim}

The output from \texttt{rat.sh} is written to the file \texttt{rescore-model}:
\begin{verbatim}
   > cat rescore-model
   FileFF:ffvals,1 0.001726045622
   FileFF:ffvals,2 -0.0259773694
   FileFF:ffvals,3 0.16128923
   FileFF:ffvals,4 -0.03746621311
   FileFF:ffvals,5 -0.005731543992
   FileFF:ffvals,6 0.002639640123
   FileFF:ffvals,7 0.002885707887
   FileFF:ffvals,8 -0.0003304783895
   FileFF:ffvals,9 0.001620606054
   FileFF:ffvals,10 0.05822454765
   FileFF:ffvals,11 -0.0004374884011
   FileFF:ffvals,12 0.26722911
   FileFF:ffvals,13 0.004129589535
   FileFF:ffvals,14 -0.003426758107
   FileFF:ffvals,15 9.72241105e-05
   # NB: this omits some features that are slow to compute. Use rescore_train -H 0
   # for a complete list. 0
   LengthFF -0.01664732769
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz 0.009676812217
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.1746531278
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz 0.003220736515
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz 0.00194059615
   HMMTgtGivenSrc:models/tm/hmm2.train.en_given_fr.gz -0.003876003204
   HMMSrcGivenTgt:models/tm/hmm2.train.fr_given_en.gz 0.005696662236
   HMMVitTgtGivenSrc:models/tm/hmm2.train.en_given_fr.gz -0.003361413721
   HMMVitSrcGivenTgt:models/tm/hmm2.train.fr_given_en.gz 0.0005181525485
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz -0.001264916151
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.004323456902
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz 0.3903803825
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz -1
   nbestWordPostSrc:1#<ffval-wts>#<pfx> -0.002481580013
   nbestWordPostTrg:1#<ffval-wts>#<pfx> -0.01662635989
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx> 0.1024738029
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx> 0.06133669615
   nbestNgramPost:2#1#<ffval-wts>#<pfx> 0.004936590791
   nbestSentLenPost:1#<ffval-wts>#<pfx> -0.05364471301
   ParMismatch 0.001803340274
   QuotMismatch:ef 0.001308475621
   #CacheLM:<src>.id 0
   RatioFF 0.04762249812
\end{verbatim}
This is identical to {\tt rescore-model.ini}, except that each feature is now
assigned a weight. Other by-products created by {\tt rat.sh} are found in the
working directory \texttt{workdir-dev2\_fr.lc-1000best} and include the nbest
lists \texttt{1000best.gz}, and the corresponding decoder features
\texttt{ffvals.gz} and additional features \texttt{ff.*}. All of these files
are compressed to save space.

\section{Translating and Testing} \label{TranslatingTesting}

\subsection{Translating} \label{Translating}

Once training is complete, the system can be used to translate.
\begin{verbatim}
   > cd ../..          # only needed if you were still in models/rescore
   > cd translate
\end{verbatim}

Some of the steps below will be performed if you do \texttt{make translate} in
your \texttt{toy.experiment} directory, but the final output you need depends
on what you're doing, and might not be produced by default.

\subsubsection{Decoding Only}

As mentioned earlier, there are two options for translating. The simplest is to
decode using the configuration file produced by \texttt{cow.sh}.  Earlier in
this toy experiment, we set \texttt{DO\_RESCORING} in \texttt{Makefile.params},
which we need to override manually here to demontrate translation by decoding
only:
\begin{verbatim}
   > make DO_RESCORING= translate
   ln -sf ../models models
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.out
   canoe-parallel.sh -lb -n 1 canoe -f canoe.ini.cow.test \
      < ../corpora/test_fr.rule > test.out 2> log.test.out
\end{verbatim}
This produces the output file \texttt{test.out}, containing one line for each
source line in \texttt{test\_fr.rule}.

\subsubsection{Decoding and Rescoring}

If you executed the command in the Decoding Only section above, please run
\texttt{make clean} before proceeding.

The other option for translating is to generate nbest lists and rescore them
using the model generated in section~\ref{RAT}. To do this, we use the {\tt
rat.sh} script again, but this time in translation mode:
\begin{verbatim}
   > make translate
   ln -sf ../models models
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.rat
   rat.sh -lb -n 1 trans -v -K 1000 \
      -msrc ../corpora/test_fr.rule -f canoe.ini.cow.test \
      models/rescore/rescore-model ../corpora/test_fr.lc \
      >& log.test.rat \
      && mv test_fr.rule.rat test.rat
   cp workdir-test_fr.rule-1000best/1best test.out
\end{verbatim}
This produces both \texttt{test.out}, the output of the decoder, and
\texttt{test.rat}, the best translation according to the rescoring model.

\subsubsection{Truecasing and Detokenizing} \label{Truecasing}

If you inspect the output files, you will notice that they contain only
lowercase, tokenized text. Two postprocessing step is required to restore normal
case and normally spaced text: truecasing and detokenizing.

Truecasing is done by the \texttt{truecase.pl} script using the model trained
previously:
\begin{verbatim}
   > make tc
   TrueCasing test.out.tc
   truecase.pl \
                   -uppercaseBOS \
                   -text=test.out \
                   -lm=models/tc/train_en-kn-3g.binlm.gz \
                   -map=models/tc/train_en.map \
                   > test.out.tc 2> log.test.out.tc
   TrueCasing test.rat.tc
   truecase.pl \
                   -uppercaseBOS \
                   -text=test.rat \
                   -lm=models/tc/train_en-kn-3g.binlm.gz \
                   -map=models/tc/train_en.map \
                   > test.rat.tc 2> log.test.rat.tc
\end{verbatim}

Detokenizing is done by the script \texttt{udetokenize.pl}, which has
hand-coded rules to detokenize French or English, encoded in utf-8.  Other
languages are not supported at this point, but may already be handled partly
correctly by this script.  As for other encodings, latin1 and cp-1252 are
supported by \texttt{detokenize.pl}.

\begin{verbatim}
   > make detok
   Detokenizing test.out
   udetokenize.pl -lang=en test.out test.out.detok
   Detokenizing test.out.tc
   udetokenize.pl -lang=en test.out.tc test.out.tc.detok
   Detokenizing test.rat.tc
   udetokenize.pl -lang=en test.rat.tc test.rat.tc.detok
   Detokenizing test.rat
   udetokenize.pl -lang=en test.rat test.rat.detok
\end{verbatim}

\subsection{Testing} \label{Testing}

Translation quality can be evaluated using BLEU score, which is a measure of
how well the translation matches an alternative translation that is known to be
correct. It is based on the number of short word sequences that the two
translations have in common, and varies between 0 for no matches to 1 for a
perfect match. BLEU is calculated by the program {\tt bleumain}:
\begin{verbatim}
   > make bleu
   Calculating BLEU for test.out.bleu
   bleumain test.out ../corpora/test_en.lc > test.out.bleu
   Calculating BLEU for test.rat.bleu
   bleumain test.rat ../corpora/test_en.lc > test.rat.bleu
   grep BLEU *.bleu
   test.out.bleu:BLEU score: 0.171809
   test.rat.bleu:BLEU score: 0.184020
\end{verbatim}
The output from {\tt bleumain} contains match statistics of various orders,
followed by the global BLEU score, as shown above.

The result above can also be obtained by typing \texttt{make eval}, or just
\texttt{make}, in your \texttt{toy.experiment} directory.

Here, we only calculate the BLEU scores on the lowercase output, using the
lowercase reference, but you can get BLEU scores for the truecased output by
giving the truecase translation and reference(s) to \texttt{bleumain}.

These results indicate that the translation produced by rescoring is slightly
better than the one produced by plain decoding. Given the small size of the
test set, it seems unlikely that the difference is statistically
significant. To test this hypothesis, we can use {\tt bleucompare}, which does
a comparison using pairwise bootstrap resampling:
\begin{verbatim}
   > bleucompare test.rat test.out REFS ../corpora/test_en.lc 
   Comparing using BLEU
   test.rat got max BLEU score in 97.8% of samples
   test.out got max BLEU score in 2.2% of samples
\end{verbatim}
This indicates that the difference is in fact significant at the 98\% level.
(Note that you may get completely different results here, since the training
corpus used is much too small to produce reliable results.)

\section{Final Note}
Because of differences in rounding, optimization, random number generation,
etc.\ between systems, results, especially numerical ones, may vary on your
platform and are shown in this document only as a reference for the user.

\end{document}
