\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper, top=1in, bottom=1in, left=1.25in, right=1.25in]{geometry}
\usepackage{isolatin1}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{url}
\usepackage{rotating}

% Need straight quotes in verbatim text.
\usepackage{upquote}
\usepackage{textcomp}
\newcommand\upquote[1]{\textquotesingle#1\textquotesingle}

% \code formats an inline code snippet without line breaking; it treats the
% underscore as a normal character. Use \url to format a code snippet with
% automatic line breaking, but then underscores are not rendered as characters
% on which copy/paste works.
% \code breaks for text containing underscores when used inside a footnote;
% for \code calls inside footnotes, use \us{} to specify an underscore.
% Use \upquote{quote-text} for straight single quotes around text within a \code
% call.
\def\code{\begingroup\catcode`\_=12 \codex}
\newcommand{\codex}[1]{\texttt{#1}\endgroup}
\chardef\us=`\_

\newcommand{\phs}{\tild{s}}   % source phrase
\newcommand{\pht}{\tild{t}}   % target phrase
% Official typesetting of PORTAGEshared, now called Portage 1.x
\newcommand{\PS}{Portage 1.4\xspace}
\newcommand{\tip}{\textbf{Useful Tip \large{\ding{43}} }}
\newcommand{\margintip}{\marginpar[{\textbf{Tip \large{\ding{43}}}}]{\textbf{\reflectbox{\large{\ding{43}}} Tip}}}
\newcommand{\tipsummary}{\noindent\textbf{Tip summary \large{\ding{43}} }}
\newcommand{\tipend}{\textbf{ \reflectbox{\large{\ding{43}}}}}

\usepackage{ifpdf}
\ifpdf
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\else
\fi

\title{\PS Tutorial: \\
       A toy experiment using the \\
       experimental framework}
\date{April, 2010}
\author{Eric Joanis}

\begin{document}

\vfill

\maketitle

\vfill

\begin{center}
An adaption of George Foster's \emph{Running Portage: A Toy Example} \\
to Samuel Larkin's experimental framework,\\
updated to reflect recommended usage of \PS.
\end{center}

\vfill
\vfill

\begin{center}
{~} \\ \footnotesize
   Technologies langagi{\`e}res interactives /
      Interactive Language Technologies \\
   Institut de technologie de l'information /
      Institute for Information Technology \\
   Conseil national de recherches Canada /
      National Research Council Canada \\
   Copyright \copyright\ 2008--2010, Sa Majest{\'e} la Reine du Chef du Canada
   \\ Copyright \copyright\ 2008--2010, Her Majesty the Queen in Right of Canada
\end{center}

\vfill

\newpage

\vfill

\tableofcontents

\vfill

\newpage


\section{Introduction}

This document describes how to run an experiment from end to end using the \PS
experimental framework. It is intended as a tutorial on using \PS, as well as a
starting point for further experiments.  Although the framework automates most
of the steps described below, we go through them one by one here in order to
better explain how to use the \PS software suite.

\PS can be viewed as a set of programs for turning a bilingual corpus into a
translation system. Here this process is illustrated with a small ``toy''
example of French to English translation, using text from the Hansard corpus.
The training corpus is too small for good translation, but large enough to give
the flavour of a more realistic setup. Running time is several hours.

\subsection{Making Sure \PS is Installed}

To begin, you must build or obtain the \PS executables and ensure that they are
in your path, typically by sourcing the \code{SETUP.bash} file as
customized for your environment during installation of \PS.\footnote{There is
  also a \code{SETUP.tcsh} for users of that shell. The examples in
  this document assume the use of bash.} You should also
set your environment variable \code{\$PORTAGE} to the directory where \PS is
installed, which is also done by \code{SETUP.bash}.  Follow the
instructions in \code{INSTALL} before you proceed with this document.

To make sure \PS is installed properly, try \code{canoe -h}, \code{cow.sh
-h}, \code{tokenize.pl -h} and \code{ce.pl -h}.  If you get usage
information for each of these programs, you should be ready to proceed.  If one
of them gives you an error message, then some part of your installation is
incomplete.  See the section \emph{Verifying your installation} in
\code{INSTALL} for troubleshooting suggestions.

\subsection{Running the Toy Experiment}

Once \PS is installed, you should make a complete copy of the framework
directory hierarchy, because it is designed to work in place, creating the
models within the hierarchy itself.  The philosophy of the framework is that
each experiment is done in a separate copy, where you might do various
customizations depending on what each experiment is intended to test.

For example:
\begin{small}
\begin{verbatim}
   > cd $PORTAGE
   > cp -pr framework toy.experiment
   > cd toy.experiment
\end{verbatim}
\end{small}
All commands provided in the rest of this document assume they are being run in
the \code{toy.experiment} directory or in a subdirectory thereof, which will
be specified relative to \code{toy.experiment}. Similary, whenever we quote a 
\code{cd} command, it will be assumed to be executed from this 
\code{toy.experiment} directory, not from the location of the previous
commands.

As you work through the example, the commands that you should
type\footnote{You can copy and paste commands from this PDF document onto
the command line in your interactive shell if you wish.} are preceded by a
prompt, \code{>}, and the system's response is not, though system output is not usually fully
reproduced here, for brevity's sake. When it is, results (especially numbers)
may vary a little from the ones shown, due to platform differences. Note that
many results presented here are truncated in precision for presentation purposes.

Many of the commands are expressed as \code{make} targets. This has the
advantage of requiring less typing, while still allowing you to see the actual
commands executed by the system because they are always echoed by \code{make}.
(You could also type them directly.) \code{make} also lets you skip
sections in this document (except for the first one, since it is done
manually). For example, if you are not interested in any steps before decoder
weight optimization, you can go to section~\ref{COW} and type \code{make
tune} to begin at that point. \code{make} will automatically run all the commands
required from previous sections before doing the step you specifically
requested. Here are some other useful make commands:
\begin{itemize}
\item \code{make all}: run all remaining steps at any point.
\item \code{make clean}: clean up and return the directory to its initial state
\item \code{make -j} \emph{target} or \code{make -j N} \emph{target}: build
      \emph{target} by running commands in parallel whenever possible (up to
      N ways parallel if N is specified). This lets you take advantage of
      multi-core capabilities of your machine, but use with caution: many
      commands in the framework are already internally parallelized, as
      discussed in section~\ref{FrameworkParams}.
\item \code{make help} displays some help and the main targets available in
      the makefile.
\item \code{make summary} displays resource used by the framework: time and
      memory used, as well as disk space for the runtime models (most
      informative once training has been completed; discussed further in
      section~\ref{FrameworkParams}).
\end{itemize}

\subsection{Overview of the Process}

Here is an overview of the \PS process, as described in the following
sections (section numbers are in brackets):
\begin{enumerate}
\item Corpus preparation (\ref{CorpusPreparation}): includes tokenization,
      alignment, lowercasing (\ref{Lowercasing}), and splitting
      (\ref{Splitting}).
\item Model training (\ref{Training}): includes language model (\ref{LM}),
      translation model (\ref{TM}), lexicalized distortion mode (\ref{LDM}),
      truecasing model (\ref{TC}), decoder weight optimization (\ref{COW}),
      rescoring model training (\ref{RAT}), and confidence estimation model
      training (\ref{CE}).
\item Translating and testing (\ref{TranslatingTesting}): includes
      decoding (\ref{Decoding}), rescoring (\ref{RATTrans}), confidence
      estimation(\ref{CETrans}), truecasing (\ref{Truecasing}) and testing
      (\ref{Testing}).
\end{enumerate}
These steps are fairly standard, but of course there are many variants on the
sample process illustrated here. For more information, see the \PS
user manual (found in \url{PORTAGEshared/doc/user-manual.html} on the CD).
For detailed information about a particular program, run the program with the
\code{-h} flag (or see \url{PORTAGEshared/doc/usage.html} on the CD).

\section{Corpus Preparation} \label{CorpusPreparation}

Corpus preparation involves converting raw document pairs into tokenized,
sentence-aligned files in the format expected by Portage.  We provide a
tokenizer, a sentence aligner and sample corpus pre-processing scripts with
\PS, which can help you with these steps.  We also provide a tool to extract
text from a translation memory in TMX format.  For details, see section
\emph{Text Processing} in the user manual.

We don't actually perform any of the steps mentioned above in this toy example
because they are highly dependent on your actual data.  You should plan to
invest some time in preprocessing your data well if you want to obtain good
results with \PS.

\subsection{Splitting the Corpus} \label{Splitting}

The tokenized, sentence-aligned corpus must be split into separate portions in
order to run experiments. Distinct, non-overlapping sub-corpora are required
for model training (see section~\ref{Training}), for tuning decoder weights
(section~\ref{COW}) and rescoring weights (section~\ref{RAT}), for confidence
estimation (if you use it; section~\ref{CE}), and for testing
(section~\ref{Testing}).\footnote{In this example, we use separate corpora for
tuning decoder and rescoring weights, but this is not necessary.  However,
confidence estimation must absolutely have its own separate tuning set.}
Typically, the tuning and testing corpora are around 1000 segments each.  If
the corpus is chronological, then it is a good idea to choose these corpora
from the most recent material, which is likely to resemble future text more
closely.

Since proper splitting of a corpus must take into account its structure
and nature, these steps are not handled by the experimental framework. For the
toy experiment, we provide small data sets that can be found here:
\url{PORTAGEshared/test-suite/unit-testing/framework-toy}. These sets
are very small, to minimize running time, so the resulting translations are of
poor quality. In this toy experiment, the tuning and testing corpora contain
just 100 segments each.

To drop these sets into the framework, simply copy them (or make symbolic
links) into your copy's corpora directory:
\begin{small}
\begin{verbatim}
   > cp $PORTAGE/test-suite/unit-testing/framework-toy/*.al \
        corpora/
   > wc corpora/*.al
       100    2115   11589 corpora/dev1_en.al
       100    2330   13680 corpora/dev1_fr.al
       100    2095   11462 corpora/dev2_en.al
       100    2443   13905 corpora/dev2_fr.al
       100    2383   13061 corpora/devCE_en.al
       100    2840   16209 corpora/devCE_fr.al
       100    2379   13292 corpora/test_en.al
       100    2616   15622 corpora/test_fr.al
      8892  182186  981455 corpora/train_en.al
      8892  208400 1178176 corpora/train_fr.al
     18584  409787 2268451 total
\end{verbatim}
\end{small}

In your own experiments, the files you need to copy into \code{corpora}
should be plain text in original truecase (if you're using truecasing),
tokenized, sentence-split and aligned, just like the ones we provide here.

\subsection{Setting Framework Parameters} \label{FrameworkParams}

Now you need to edit \code{Makefile.params} to set some global parameters:
\begin{itemize}
\item swap the values of \code{SRC_LANG} and
\code{TGT_LANG}, to select translation from French to English, rather than
the other way around, which is the default;
\item set \code{TRAIN_LM} and \code{TRAIN_TM} to
\code{train} instead of \code{lm-train} and \code{tm-train} since we
won't be using a separate corpora to train the translation and language models;
\item set \code{TUNE_CE} to \code{devCE} (also, remember to uncomment the
line by removing the \code{\#} at the start);
\item set \code{TEST_SET} to \code{test} instead of the default
  \code{test1 test2} because we have only one test set;
\item select a language modeling option (see below for more info about this
  choice): 
\begin{itemize}
\item to use SRILM, make sure the \code{LM_TOOLKIT} variable is set to
\code{SRI};
\item to use MITLM, set the \code{LM_TOOLKIT} variable to \code{MIT};
\item to use IRSLTM, set the \code{LM_TOOLKIT} variable to \code{IRST} and
set the \code{IRSTLM} variable to the location where IRSTLM is installed;
\item to use none of the above (for this toy example only), any valid value
for \code{LM_TOOLKIT} will do.
\end{itemize}
\end{itemize}
In this toy experiment, we will use the default value for all other parameters.

While in \code{Makefile.params}, you should read through all the variables in
the \emph{User definable variables} section of the file.  This is where most of
the configurable behaviours are set, such as whether to do rescoring and/or
truecasing, what LM toolkit you are using (if any), levels of parallelism, etc. 

We recommend you use SRILM (\url{http://www.speech.sri.com/projects/srilm/}) as
your language modeling toolkit, if your licensing requirements permit it; if
not, we recommend MITLM (\url{http://code.google.com/p/mitlm/}), another
excellent LM toolkit, which allows commercial use.  If you have no toolkit at
the moment, we provide trained language models that you can drop into the
framework to finish this toy experiment while skipping language model training.
To use IRSTLM, in addition to the edits described above, you need to type
\code{make help} and cut and paste the two export commands, unless you've
already done the equivalent steps while installing IRSTLM:
\begin{small}
\begin{verbatim}
   > make help | grep -B1 irstlm
   please run the following in order for this framework to run properly:
   export PATH=$PORTAGE/pkgs/irstlm/bin:$PATH
   export IRSTLM=$PORTAGE/pkgs/irstlm
   > export PATH=$PORTAGE/pkgs/irstlm/bin:$PATH
   > export IRSTLM=$PORTAGE/pkgs/irstlm
\end{verbatim}
\end{small}

Another set of parameters you might want to adjust are the various
\code{PARALLELISM_LEVEL_*} variables.  \PS is written in such a way as to
take advantage of multi-processor computers and/or multi-node computing
clusters, doing tasks in parellel where possible.  If you're not running on a
cluster, the number of CPUs on your machine is a good choice for all five
variables (this is the default).  If you are running on a cluster, the
framework uses \code{qsub} to submit jobs, via the \code{run-parallel.sh}
and \code{psub} scripts, and you can set the five variables according to
resources available to you.

When running this framework, you might notice that most commands are preceded
by control variables \code{RP_PSUB_OPTS=...} or \code{_LOCAL=1}.  These
strings only have an impact when running on a cluster, and are written in such
a way that they are ignored otherwise.  When working on a cluster, commands
preceded by \code{_LOCAL=1} are inexpensive ones that get
run directly instead of being submitted to the queueing system, while
\code{RP_PSUB_OPTS=...} is used to specify additional options to
\code{psub}, which is used in \PS to encapsulate the invocation of
\code{qsub}.  If your cluster has specific usage rules or if you require
additional parameters to \code{qsub}, you might want to customize
\code{psub} itself or add options as required in this framework.

\tip\margintip Many commands run in this framework will also be preceded by
\code{time-mem}. This utility script measures the time taken by a command, just
like the standard \code{time} utility, as well as memory usage. At any time
while things are running, or afterwards, you can type \code{make summary} to
get a summary of resources used by all components of the framework. While things
are running, you may get some error messages, but these are safe to ignore: the
report should still reflect the situation so far.  The output of \code{make
time-mem} can be very useful to determine which steps are taking the most time
and/or the most memory.  They can help you determine if you have enough
computing resources to process your corpora.  They can also help you determine
the cost of various choices you can make in Portage.  You can also type
\code{make summary} to get the \code{time-mem} information as well as the
space on disk of the models needed at runtime, such as would be deployed on a
translation server.\tipend

For the sake of brevity, when we quote executed commands in this document, we
only show the command itself, leaving out \code{time-mem} and the control
variables mentioned above.

\subsection{Lowercasing and Adding Escapes} \label{Lowercasing}

To reduce data sparseness, we convert all files to lowercase.  We keep the
lowercase and truecase versions separately, because we'll use the truecase
version to train a truecasing model.
\begin{small}
\begin{verbatim}
   > cd corpora
   > make lc
   cat dev1_fr.al | lc-utf8.pl > dev1_fr.lc
   [...]
\end{verbatim}
\end{small}

The decoder, \code{canoe}, treats \code{<}, \code{>} and \verb*X\X as
special characters in order to support markup for special translation rules.
We won't use any markup in this example, but we still need to escape any 
occurrences of the three special characters (in the source language only, since
this is only required for the input to \code{canoe}).

\begin{small}
\begin{verbatim}
   > make rule
   canoe-escapes.pl -add dev1_fr.lc > dev1_fr.rule
   canoe-escapes.pl -add dev2_fr.lc > dev2_fr.rule
   canoe-escapes.pl -add devCE_fr.lc > devCE_fr.rule
   canoe-escapes.pl -add test_fr.lc > test_fr.rule
\end{verbatim}
\end{small}


\section{Training} \label{Training}

This step creates various models and parameter files that are required for
translation. There are five steps in training: creating a language model,
creating a truecasing model, creating translation models, optimizing decoder
weights, and creating a rescoring model.  When using confidence estimation, the
CE model also has to be trained.

\subsection{Creating a Language Model} \label{LM}

Portage does not come with programs for language model training. However, it
accepts models in the widely-used ``ARPA'' format which is produced by
toolkits such as SRILM or IRSTLM. The following sections describe the training
process for SRILM, IRSTLM, and for using the pre-trained models.\footnote{In
this toy example, we use only the target language part of the parallel
training corpus to train the language model, but this is not the recommended practice.
If you have access to larger corpora of monolingual text in your target
language, you can use them to train additional language models.  The framework
does not support training multiple language models, but you can train them
externally and add them to the \code{canoe.ini.template} file (see
section~\ref{COW}).} Commands are assumed to be issued from the
\code{toy.experiment} directory (so run \code{cd ..} from the corpora
directory if you have not already done so).

\subsubsection{SRILM}

If you are using the SRILM toolkit, here is the command used to produce this
model:
\begin{small}
\begin{verbatim}
   > make lm
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format train_en-kn-5g.lm.gz
   ngram-count -interpolate -kndiscount -order 5 \
      -text ../../corpora/train_en.lc.gz -lm train_en-kn-5g.lm.gz \
      >& log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
\end{small}
The first command executed, \code{ngram-count}, produces the language model
itself in ``ARPA'' format.  The second command, \code{arpalm2binlm},
converts it into the Portage binary language model format for fast loading.


\subsubsection{MITLM}

If you are using the MITLM toolkit, here is the command used to produce the
model needed for this example, \code{train_en-kn-5g.binlm.gz}:
\begin{small}
\begin{verbatim}
   > make lm
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format train_en-kn-5g.lm.gz
   zcat ../../corpora/train_en.lc.gz \
      | estimate-ngram -order 5 -smoothing ModKN -text /dev/stdin \
        -write-lm train_en-kn-5g.lm.gz 2> log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
\end{small}
The first command executed, \code{estimate-ngram}, produces the language
model itself in ``ARPA'' format.  The second command, \code{arpalm2binlm},
converts it into the Portage binary language model format for fast loading.


\subsubsection{IRSTLM}

If you are using the IRSTLM toolkit, here is the command used to produce the
model needed for this example, \code{train_en-kn-5g.binlm.gz}:
\begin{small}
\begin{verbatim}
   > make lm
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Marking up ../../corpora/train_en.lc.gz
   zcat -f ../../corpora/train_en.lc.gz \
      | add-start-end.sh \
      | gzip -c > train_en.marked.gz
   Creating IRSTLM train_en-kn-5g.ilm.gz
   build-lm.sh -p -t stat.$$ -n 5 -k 4 -s kneser-ney \
      -i "gunzip -c train_en.marked.gz" -o train_en-kn-5g.ilm.gz \
      >& log.train_en-kn-5g.ilm
   Creating ARPA text format train_en-kn-5g.lm.gz
   compile-lm --text yes train_en-kn-5g.ilm.gz /dev/stdout \
        2> log.train_en-kn-5g.lm \
        | egrep -v '^Saving in txt format to' \
        | lm_sort.pl \
        | gzip > train_en-kn-5g.lm.gz
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
\end{small}
The first three commands executed, \code{add-start-end.sh},
\code{build-lm.sh} and \code{compile-lm}, construct the language model
in ``ARPA'' format using IRSTLM software. (Note: On a cluster,
\code{build-lm.sh} is replaced by \code{build-lm-qsub.sh}.) The script
\code{lm_sort.pl} reorders the contents of the LM to maximize the compression
ratio \code{gzip} can achieve. The sort step is unecessary with a small
language model, but can significantly reduce final file sizes with very large
language models.\footnote{We didn't do this sorting step with SRILM,
since its output already compresses well.} The last command,
\code{arpalm2binlm}, converts the language model into the Portage binary
language model format for fast loading.


\subsubsection{Using the Pre-trained Model}

If you have not installed any LM toolkit, we provide a pre-trained
language model, \code{train_en-\linebreak[0]kn-5g.lm.gz} in
\url{PORTAGEshared/test-suite/unit-testing/framework-toy/lm}. You can
copy that file into \code{models/lm} and then type \code{make lm} to
proceed with the rest of this example, although you won't be able to train on
new text without a language modeling toolkit.

\subsection{Creating a Truecasing Model} \label{TC}

Decoding is done in lowercase to reduce data sparseness, but at the end of the
translation process, you will probably want to restore proper mixed case to
your output.  We call this step truecasing.  To train a truecasing model, you
need both a lowercased and the original ``truecase'' version of the training
corpus in the target language.

The truecasing model consists of two different models: a ``casemap'', which
maps each lower case words to its possible truecase variants, and a standard
language model trained on the truecase corpus.

\begin{small}
\begin{verbatim}
   > make tc
   make -C models tc
   make -C tc all
   compile_truecase_map ../../corpora/train_en.tc.gz ../../corpora/train_en.lc.gz \
      > train_en.map 2> log.train_en.map
   [commands to train the truecase LM]
\end{verbatim}
\end{small}
The first command, \code{compile_truecase_map}, compiles the casemap
by reading the truecase and lowercased versions of the corpus simultaneously.
The remaining commands will depend on your choice of LM toolkit, training a
language model as in section~\ref{LM} above, but using the truecase corpus,
\code{train_en.tc.gz}, instead of the lowercased version.

If you have not installed any LM toolkit, we provide the file
\code{train_en-kn-3g.lm.gz} in
\url{PORTAGEshared/test-suite/unit-testing/framework-toy/tc}.  You can
copy that file into \code{models/tc} and and type \code{make tc} to proceed
with the rest of this example.

\subsection{Creating a Translation Model} \label{TM}

Creating a translation model involves two main steps: 1) training IBM and HMM word
alignment models in both directions, then 2) using them to extract phrase pairs
from the corpus.  We illustrate the use of two separate phrase tables: one
from IBM2 word alignments, and one from HMM word alignments (we recommended
doing the same for your experiments as well).  You can do all this by typing
\code{make tm} in your \code{toy.experiment} directory, but we will break
it down into several steps here.

\subsubsection{Creating a Translation Model Using IBM2 Alignments} \label{IBM2}

\subsubsection*{Training IBM2 Models}

First we train IBM2 word alignment models, which requires training IBM1 models
as a prerequisite.  We do this for both directions.
\begin{small}
\begin{verbatim}
   > cd models/tm
   > make ibm2_model
   cat.sh -n 4 -pn 4 -v -n1 5 -n2 0 -bin ibm1.train.en_given_fr.gz 
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 5 -n2 0 -bin ibm1.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.fr_given_en
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -bin -i ibm1.train.en_given_fr.gz ibm2.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -bin -i ibm1.train.fr_given_en.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.fr_given_en
\end{verbatim}
\end{small}
The \code{train_ibm} program could have been used directly, but we use
\code{cat.sh} instead, which runs \code{train_ibm} in parallel. Here
the options \code{-n 4 -pn 4} mean 4-ways parallel, because we are quoting
commands run on a quad core, and the default is the use the number of CPUs
available, as controlled by the \code{PARALLELISM_LEVEL_TM} variable in
\code{Makefile.params}.

The commands above write log files containing information about training.  For
example, the IBM log files show convergence and pruning statistics for each
iteration:
\begin{small}
\begin{verbatim}
   > grep -h ppx log.ibm[12]*en_given_fr
   parallel iter (IBM1): prev ppx = 356.599, size = 1461411 word pairs.
   parallel iter (IBM1): prev ppx = 109.263, size = 1350718 word pairs.
   parallel iter (IBM1): prev ppx = 66.443, size = 1182917 word pairs.
   parallel iter (IBM1): prev ppx = 52.8037, size = 1016137 word pairs.
   parallel iter (IBM1): prev ppx = 47.908, size = 874686 word pairs.
   parallel iter (IBM2): prev ppx = 45.755, size = 874415 word pairs.
   parallel iter (IBM2): prev ppx = 29.4005, size = 874073 word pairs.
   parallel iter (IBM2): prev ppx = 23.4925, size = 849903 word pairs.
   parallel iter (IBM2): prev ppx = 20.8623, size = 741721 word pairs.
   parallel iter (IBM2): prev ppx = 19.5686, size = 617726 word pairs.
\end{verbatim}
\end{small}

The log files also contain time and memory resource information, which you can
summarize at any level just as you can globally:
\begin{footnotesize}
\begin{verbatim}
   > make time-mem
   find -type f -name log.\* | sort | xargs time-mem -T -m tm
      log.ibm1.train.en_given_fr:TIME-MEM  WALL TIME: 1m3s   CPU TIME: 28s   VSZ: 0.071G  RSS: 0.010G
      log.ibm1.train.fr_given_en:TIME-MEM  WALL TIME: 1m0s   CPU TIME: 28s   VSZ: 0.072G  RSS: 0.009G
      log.ibm2.train.en_given_fr:TIME-MEM  WALL TIME: 57s    CPU TIME: 24s   VSZ: 0.071G  RSS: 0.011G
      log.ibm2.train.fr_given_en:TIME-MEM  WALL TIME: 58s    CPU TIME: 24s   VSZ: 0.084G  RSS: 0.012G
   tm:TIME-MEM                             WALL TIME: 3m58s  CPU TIME: 1m44s VSZ: 0.084G  RSS: 0.012G
\end{verbatim}
\end{footnotesize}

The IBM models are written to files \code{ibm[12].*} and contain word
translation/alignment probabilities.

\subsubsection*{Training Joint-Count Phrase Tables}

Now we extract the joint-count phrase table from the same parallel corpus,
using IBM2 word alignment models in both directions together.
\begin{small}
\begin{verbatim}
   > make ibm2_jpt
   gen-jpt-parallel.sh -n 4 -nw 4 -w 1 -o jpt.ibm2.train.fr-en.gz \
      GPT -v -m 8 -ibm 2 -1 fr -2 en \
      ibm2.train.en_given_fr.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      2> log.jpt.ibm2.train.fr-en
\end{verbatim}
\end{small}
The joint-count phrase table contains the co-occurrence frequency for
each phrase pair observed in the parallel corpus.

\subsubsection*{Training Conditional Phrase Tables}

Finally we compute the conditional phrase table, using two different
smoo\-thers to calculate the probabilities.\footnote{In general, we often obtain
better translation quality when using phrase probabilities estimated from
relative frequencies (``RFSmoother'') along with probability estimates
calculated using Zens-Ney lexical smoothing (``ZNSmoother''). Other smoothing
options are also available. We also used to include Kneser-Ney smoothing by
default, but in recent work it has not helped much, so we don't anymore.}
Although saved together in the same phrase table, they are separate probability
models, whose relative weights will be tuned during decoder weight optimization
(see section~\ref{COW}).
\begin{small}
\begin{verbatim}
   > make ibm2_cpt
   joint2cond_phrase_tables -reduce-mem -sort -prune1 100 -v -i -z \
      -ibm 2 -1 fr -2 en -s RFSmoother -s ZNSmoother \
      -multipr fwd -o cpt.ibm2-rf-zn.train \
      -ibm_l2_given_l1 ibm2.train.en_given_fr.gz \
      -ibm_l1_given_l2 ibm2.train.fr_given_en.gz \
      jpt.ibm2.train.fr-en.gz \
      >& log.cpt.ibm2-rf-zn-kn3.train.fr2en
\end{verbatim}
\end{small}
The phrase table \code{cpt.ibm2-rf-zn.train.fr2en.gz}, together with its HMM
counterpart (see below), is the main source of information for French to
English translation. Each of its lines is of the form:
\begin{verbatim}
   fr ||| en ||| p1(fr|en) p2(fr|en) p1(en|fr) p2(en|fr)
\end{verbatim}
where \code{fr} is a French source phrase, \code{en} is an English target
phrase, \code{p(fr|en)} is the probability that \code{fr} is the
translation of \code{en} (the ``backward'' probability), and
\code{p(en|fr)} is the probability that \code{en} is the translation of
\code{fr} (the ``forward'' probability). In our example here, \code{p1} is
the relative-frequency estimate of this probability, while \code{p2} is the
Zens-Ney lexical smoothing based estimate, because those are the smoothers we
chose. There can be any number of backward and forward probability estimates,
reflecting the number of smoothers you use.
%
Here are two sample lines from this file:
\begin{small}
\begin{verbatim}
   > zgrep '| proposed regulations |' cpt.ibm2-rf-zn.train.fr2en.gz 
   projets de règlement ||| proposed regulations ||| 0.67 1.2e-6 1 0.013
   règlements proposés ||| proposed regulations ||| 0.33 0.013 1 0.39
\end{verbatim}
\end{small}
Note that this example is carefully chosen; because of the small size of the
training corpus, many other entries in the phrase table are not such good
translations.

\subsubsection{Creating a Translation Model Using HMM Alignments} \label{HMM}

Now we repeat the same steps using HMM word alignment models, using the
``hmm3'' variant, which is the default.  This variant corresponds to the HMM
parameter settings we recommend using as a starting point.
\begin{small}
\begin{verbatim}
   > make hmm3_cpt
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -hmm -end-dist -anchor \
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \
      -i ibm1.train.en_given_fr.gz hmm3.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.hmm3.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -hmm -end-dist -anchor \
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \
      -i ibm1.train.fr_given_en.gz hmm3.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.hmm3.train.fr_given_en
   gen-jpt-parallel.sh -n 4 -nw 4 -w 1 -o jpt.hmm3.train.fr-en.gz \
      GPT -v -m 8 -hmm -1 fr -2 en \
      hmm3.train.en_given_fr.gz hmm3.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.jpt.hmm3.train.fr-en
   joint2cond_phrase_tables -reduce-mem -sort -prune1 100 -v -i -z \
      -hmm -1 fr -2 en -s RFSmoother -s ZNSmoother \
      -multipr fwd -o cpt.hmm3-rf-zn.train  \
      -ibm_l2_given_l1  hmm3.train.en_given_fr.gz \
      -ibm_l1_given_l2  hmm3.train.fr_given_en.gz \
      jpt.hmm3.train.fr-en.gz \
      >& log.cpt.hmm3-rf-zn.train.fr2en
\end{verbatim}
\end{small}

There are many variants of the HMM models.  In this framework, we've included
recipes to produce three of them: one with lexically conditioned jump
parameters, following He (WMT-2006), invoked via the \code{hmm1_*} targets;
one similar to the baseline described in Liang et al (ACL 2006), invoked via
the \code{hmm2_*} targets; and one tuned in-house on French-English
material, invoked via the \code{hmm3_*} targets.  Many other variants are
available, as documented by \code{train_ibm -h}. To select one or more of
them, you'll need to modify \url{models/tm/Makefile} and
\url{models/tm/Makefile.toolkit}. It's probably easier to modify the
commands associated with the \code{hmm1_model}, \code{hmm2_model} or 
\code{hmm3_model} targets than to create new targets.

To modify the default models generated, change the \code{PT_TYPES} variable
in \url{toy.experiment/Makefile.params} to, for instance, \code{ibm2_cpt
hmm1_cpt}: this variable enumerates which models you want to generate when you
type \code{make all}. All models you generate will be used in subsequent
steps, so generate only the ones you intend to use. (You should work in
different copies of the framework if you want to experiment with different
variants.)

\subsection{Creating a Lexicalized Distortion Model} \label{LDM}

Lexicalized Distortion Models (LDMs) are not used by default in this
framework.  If you want to enable their creation and use, you should set
\code{USE_LDM} to \code{1} in \code{Makefile.params} (by removing the initial
\code{\#} to uncomment the appropriate line).

The LDM is created in two steps: 1) counting occurrences of the different types
of distortion instances over the training corpus, using the program
\code{dmcount}, and 2) estimating scores from these counts, using the program
\code{dmestm}.  If you trained more than one phrase table, as we did above,
the first step is repeated for each phrase table.
\begin{small}
\begin{verbatim}
   > cd models/ldm
   > make all
   parallelize.pl -nolocal -psub -1 -n 30 -np 30 -w 100000 \
      -s ../../corpora/train_fr.lc.gz -s ../../corpora/train_en.lc.gz \
      'dmcount -v -m 8 \
       ../tm/ibm2.train.en_given_fr.gz ../tm/ibm2.train.fr_given_en.gz \
       ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
       > ldm.counts.ibm2.gz' \
      >& log.ldm.counts.ibm2
   parallelize.pl -nolocal -psub -1 -n 30 -np 30 -w 100000 \
      -s ../../corpora/train_fr.lc.gz -s ../../corpora/train_en.lc.gz \
      'dmcount -v -m 8 \
       ../tm/hmm3.train.en_given_fr.gz ../tm/hmm3.train.fr_given_en.gz \
       ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
       > ldm.counts.hmm3.gz' \
      >& log.ldm.counts.hmm3
   zcat -f ldm.counts.ibm2.gz ldm.counts.hmm3.gz \
      | time-mem dmestm -s -g ldm.hmm3+ibm2.fr2en.bkoff \
        -wtu 5 -wtg 5 -wt1 5 -wt2 5 2> log.ldm.hmm3+ibm2.fr2en \
      | gzip > ldm.hmm3+ibm2.fr2en.gz
\end{verbatim}
\end{small}

Estimating lexicalized distortion parameters (step 2, \code{dmestm}) might
require a lot of memory, sometimes two to three times as much as training
phrase tables.  As mentioned before, you can use \code{make time-mem} to see
your resource usage after the process has completed to place your resource
allocation.  We generally observe a small gain in translation quality (as
measured by BLEU) when using LDMs, although you will need to determine,
depending on your situation, whether this gain is worth the added memory and
time requirements.

\tip\margintip The commands for step 1 illustrate the use of one of our generic utilities:
\code{parallelize.pl}.  This script can be used to parallelize the execution
of any command where each input line is processed independently.  It takes care
of splitting the input(s) into chunks, running the chunks in parallel, and
concatenating (or otherwise merging) the outputs.  In this example the output
from all instances of \code{dmcount} is simply concatenated together, which
works correctly because \code{dmestm} tallies counts when it sees repeated
phrase pairs.  Run \code{parallelize.pl -h} for more details.\tipend


\subsection{Optimizing Decoder Weights} \label{COW}

The language and translation models are the main sources of information used
for translation, which is performed by the \code{canoe} decoder.  Here we
also trained an LDM as an additional source of information.  In order to get
reasonable translation quality, the weights on these (and other) sources of
information need to be tuned. The tuning process is carried out by a script
called \code{cow.sh}, which runs \code{canoe} several times.
\code{cow.sh} takes in an initial \code{canoe.ini} configuration file and
produces an optimized configuration file, \code{canoe.ini.cow}.

You can run all the steps below by typing \code{make decode} or \code{make cow}
in your main \code{toy.experiment} directory, but we'll break it down into
several steps here.

\subsubsection{The Decoder Configuration File: \code{canoe.ini}}

In this framework, we start with a template configuration file,
\url{models/decode/canoe.ini.template}:
\begin{small}
\begin{verbatim}
   > cd models/decode
   > cat canoe.ini.template
   [ttable-multi-prob]
     <CPTS>
   [lmodel-file]
     <LMS>
   [weight-f] <ENABLE_FTM>
   ...
\end{verbatim}
\end{small}

The framework will automatically insert the phrase tables and language models
generated in the previous steps into the \code{canoe.ini} file when you
make it.  These are the template parameters it will replace:
\begin{itemize}
\item \code{<SL>}:   source language code, e.g. ``fr'';
\item \code{<TL>}:   target language code, e.g., ``en'';
\item \code{<CPTS>}: all conditional phrase tables trained above and found
                       in the \code{models/tm} directory; and
\item \code{<LMS>}:  all language models trained trained above and found in
                       the \code{models/lm} directory);
\item \code{<ENABLE\_FTM>}: the right number of forward translation
model weights.  This special token is required to enable the use of forward
translation model weights by \code{canoe} and their tuning by
\code{cow.sh}.  Unlike other weights, which are used by default as soon as
the corresponding model is present, forward weights are disabled by default.
In this framework, if you don't want to use forward weights, comment out or
delete the \code{[weight-f]} line in \code{canoe.ini.template}.
\item If \code{USE_LDM} is set in \code{Makefile.params}, special
processing is also done to insert the necessary LDM parameters into the
\code{canoe.ini}.
\end{itemize}

So we make the \code{canoe.ini} file:
\begin{small}
\begin{verbatim}
   > make canoe.ini
   cat canoe.ini.template \
      | sed -e 's/<SL>/fr/g' \
            -e 's/<TL>/en/g' \
            -e 's#<CPTS>#models/tm/cpt.hmm3-rf-zn.train.fr2en.gz models/tm/[...]' \
            -e 's#<LMS>#models/lm/train_en-kn-5g.binlm.gz#g' \
            -e 's/^\(\[\(weight-f\|ftm\)\]\)/##mid##\1/' \
           > tmp.canoe.ini
   WT=$(perl -e 'print 1; print ":1" \
        foreach (2..`configtool nt tmp.canoe.ini`)'); \
   cat tmp.canoe.ini \
      | sed -e "s/^##mid##//" -e "s/<ENABLE_FTM>/$WT/" \
      | configtool -p "args:-dist-phrase-swap -dist-limit-ext \
           -lex-dist-model-file models/ldm/ldm.hmm3+ibm2.fr2en.gz -distorti[...]" \
           > canoe.ini
   rm tmp.canoe.ini
   configtool check canoe.ini
   ok
\end{verbatim}
\end{small}
This command silently creates a soft link to the \code{models} directory---this
way, all models appear to be relative to the current directly, which makes it easier
to find them.  We create such symlinks everywhere we need access to the
models.  The \code{sed} commands replace the template parameters by the
appropriate values.  The first \code{configtool} command inserts the lexicalized
distortion parameters if necessary.  The final command, \code{configtool
check canoe.ini}, confirms that the \code{canoe.ini} file produced is
error-free---it checks that all parameters are compatible and that all models
can be found.

Here is the result:
\begin{small}
\begin{verbatim}
   > cat canoe.ini
   [ttable-multi-prob] 
      models/tm/cpt.hmm3-rf-zn.train.fr2en.gz
      models/tm/cpt.ibm2-rf-zn.train.fr2en.gz
   [lex-dist-model-file] 
      models/ldm/ldm.hmm3+ibm2.fr2en.gz
   [lmodel-file] 
      models/lm/train_en-kn-5g.binlm.gz
   [weight-f] 1:1:1:1
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [dist-limit-ext]
   [dist-phrase-swap]
   [distortion-model] 
      WordDisplacement
      back-lex#m
      back-lex#s
      back-lex#d
      fwd-lex#m
      fwd-lex#s
      fwd-lex#d
   [segmentation-model] none
   [bypass-marked]
   [cube-pruning]
\end{verbatim}
\end{small}

When using this framework, do not modify \code{canoe.ini} directly.  To
change the training parameters for language models or phrase tables, modify the
parameters or Makefiles appropriately in \code{models/lm} and
\code{models/tm} and regenerate those models. Be careful, though: the
\code{canoe.ini} file will include all models generated in the same
framework, so you should work in different copies of the framework if you want
to experiment with different training parameters for those models. 
\tip\margintip Soft links can save you a lot of duplicated work if you have
related experiments.\tipend

\tip\margintip Any time a \code{canoe.ini} configuration file is manipulated,
always run \code{configtool check} on the resulting \code{canoe.ini} file to
verify its contents.\tipend

To change the other decoding parameters, modify \code{canoe.ini.template}.
For example, you can add additional language models (trained outside the
framework) by adding them in the \code{[lmodel-file]} section, separated by
whitespace or a newline from \code{<LMS>}.

The other decoding parameters control the search strategy.  They have been set
to reasonable defaults, but may need adjustment depending on the
speed/quality trade-offs you're willing to make or other models you wish to
use.

\subsubsection{Running COW (Canoe Optimize Weights)}

Besides the configuration file, the other main arguments required by
\code{cow.sh} are a directory in which to store temporary files, a source
file and one or more correct (i.e., human) translations of the source
file.\footnote{This example uses only one translation, but when multiple
alternative translations are available, it is advantageous to use them.}
Here we call the temporary directory \code{foos}, and we use the \code{dev1}
files for weight tuning:
\begin{small}
\begin{verbatim}
   > make all
   mkdir -p foos
   cow.sh -v -parallel:"-n 4" -maxiter 15 -nbest-list-size 100 \
      -filt -nofloor -workdir foos -f canoe.ini
      ../../corpora/dev1_fr.rule ../../corpora/dev1_en.lc \
      >& log.canoe.ini.cow
   rm -f -r foos *.FILT.gz *.FILT.bkoff canoe.ini.FILT canoe.ini.FILT.cow
\end{verbatim}
\end{small}
Because \code{cow.sh} takes a while to run (between 30 minutes and three
hours in this example, depending on your system)\footnote{Running time for
\code{cow.sh} can be reduced by executing it in parallel. Type 
\code{cow.sh -h} to see documentation on this option; modify
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}DECODE} in \code{Makefile.params} to
adjust cow parallelism in this framework.} and writes large amounts of logging
information, it is usually a good idea to redirect all of its output to a log
file, as is done here; and also to run it in the background (not done here).
Progress is most easily monitored by looking at the file
\code{rescore-results},\footnote{The scripts \code{best-rr-val} and
\code{all-best-rr-vals} can be handy if you are monitoring multiple
experiments.} which shows the translation quality (measured by BLEU score---see
section~\ref{Testing} for a description), as well as the current weights, for
each iteration. Note that the BLEU score does not necessarily increase
monotonically, as can be seen in several places in the \code{rescore-results}
file obtained with this example:
% cat rescore-results | perl -pe 's/(\.\d\d)\d+(:| \S|$)/\1\2/g; s/   -d/  -d/' | cut -c1-92
\begin{footnotesize}
\begin{verbatim}
> cat rescore-results
BLEU score: 0.165623  -d 1:1:1:1:1:1:1 -w 0 -lm 1 -tm 1:1:1:1 -ftm 1:1:1:1
BLEU score: 0.012741  -d 0.03:0.05:-1:-0.01:-0.00:-0.00:-0.01 -w -0.11 -lm 0.38 -tm 0.01:0.0
BLEU score: 0.105025  -d 0.75:0.31:-0.23:-1:-0.04:-0.24:-0.07 -w -0.35 -lm 0.76 -tm 0.02:-0.
BLEU score: 0.042423  -d 0.56:0.24:-0.00:0.56:0.85:0.21:-0.11 -w 0.03 -lm 1 -tm -0.16:0.23:-
BLEU score: 0.183071  -d 1:0.36:-0.55:-0.00:0.22:0.35:-0.69 -w -0.17 -lm 0.81 -tm 0.11:0.04:
BLEU score: 0.201546  -d 0.82:0.84:-0.03:0.17:1:0.06:-0.25 -w -0.42 -lm 0.75 -tm 0.09:0.06:-
BLEU score: 0.186689  -d 0.52:1:-0.07:-0.05:0.01:0.05:-0.00 -w -0.34 -lm 0.61 -tm 0.01:0.06:
BLEU score: 0.162512  -d 0.28:-0.84:-0.09:0.28:0.06:-0.19:-0.08 -w -0.39 -lm 1 -tm -0.00:0.0
BLEU score: 0.185599  -d 0.70:0.43:-0.04:-0.16:1:-0.06:-0.02 -w -0.07 -lm 0.40 -tm -0.00:-0.
BLEU score: 0.190144  -d 0.93:-1:-0.01:0.64:0.73:0.04:-0.52 -w 0.02 -lm 0.99 -tm 0.49:0.13:0
BLEU score: 0.184154  -d 0.25:0.56:0.04:-0.03:0.08:0.02:-0.02 -w -0.35 -lm 1 -tm 0.13:0.00:0
BLEU score: 0.177097  -d 0.55:1:0.01:-0.36:-0.04:0.20:-0.24 -w -0.53 -lm 0.53 -tm 0.12:0.03:
BLEU score: 0.201575  -d 0.89:1:-0.20:-0.11:0.30:0.32:-0.17 -w -0.42 -lm 0.55 -tm 0.13:0.02:
BLEU score: 0.202520  -d 0.77:0.54:-0.11:-0.11:-0.02:-0.00:-0.01 -w -0.22 -lm 1 -tm 0.11:0.0
BLEU score: 0.198976  -d 1:0.41:-0.30:0.00:0.05:0.79:-0.62 -w -0.19 -lm 0.69 -tm 0.08:0.00:-
\end{verbatim}
\end{footnotesize}
When the last iteration still shows an improvement (which is almost the case
here), it is sometimes a sign that we should have allowed \code{cow.sh} to
run more iterations, by setting the \code{-maxiter} option to a higher value.
Here, it looks like we achieved convergence at the 14th iterations; it might
have been worth doing a few more.  You will notice that there are four weights
after \code{-tm} (truncated in the output shown above): two for the backward
probability estimates in the phrase table created using IBM2 models, and two
for the phrase table created using HMM models.  For the same reason, there are
four weights after \code{-ftm}.  There are seven weights after \code{-d}
because we used the standard WordDisplacement model (``distortion penalty''),
as well as the six-feature lexicalized distortion model.

While \code{cow.sh} is working, it saves $n$-best lists and other intermediate
files to a temporary work directory called \code{foos}. If you wish to
prevent these (large) files from being automatically cleaned up on completion,
remove the \code{rm} command executed after \code{cow.sh} in
\code{Makefile}. In addition, a lot of information is also logged to
\code{log.canoe.ini.cow}; this can be summarized using the command
\code{cowpie.py log.canoe.ini.cow} (type \code{cowpie.py -h} for details).

The final output from \code{cow.sh} is written to the file
\code{canoe.ini.cow} (assuming the original configuration file was called
\code{canoe.ini}). This duplicates the contents of \code{canoe.ini}, but
adds the weights tuned on the development corpus:
% cat canoe.ini.cow | perl -pe 's/(\.\d\d\d\d\d\d)\d+/\1/g; s/(\.\d\d\d\d\d\d)\d+/\1/g if /weight-d/; s/^/   /'
\begin{small}
\begin{verbatim}
   > cat canoe.ini.cow
   [ttable-multi-prob] 
      models/tm/cpt.hmm3-rf-zn.train.fr2en.gz
      models/tm/cpt.ibm2-rf-zn.train.fr2en.gz
   [lex-dist-model-file] 
      models/ldm/ldm.hmm3+ibm2.fr2en.gz
   [lmodel-file] 
      models/lm/train_en-kn-5g.binlm.gz
   [weight-d] 0.779813:0.544810:-0.116676:-0.110313:-0.025498:-0.006713:-0.016213
   [weight-w] -0.227432
   [weight-l] 1
   [weight-t] 0.115227:0.004537:0.093380:0.004068
   [weight-f] 0.020464:0.003510:0.032827:0.066267
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [dist-limit-ext]
   [dist-phrase-swap]
   [distortion-model] 
      WordDisplacement
      back-lex#m
      back-lex#s
      back-lex#d
      fwd-lex#m
      fwd-lex#s
      fwd-lex#d
   [segmentation-model] none
   [bypass-marked]
   [cube-pruning]
\end{verbatim}
\end{small}
The \code{weight-} parameters above pertain to, the distortion
model, the word-length penalty, the language model, the backward scores of the
translation models, and the forward scores of the translation models,
respectively. The language model often obtains the highest weight, as is the
case here.\footnote{A note of warning about the LM weight: following Doug Paul's
``ARPA'' LM format, all LM formats we know use base-10 log probs (including our
own binary LM and TPLM formats), but canoe interprets them as natural logs:
throughout \PS, logs are natural by default, not base 10.  This known bug has
minimal impact; correcting it simply requires multiplying the desired
\code{weight-l} values by log(10), which \code{cow.sh}, \code{rat.sh} and
\code{rescore\_train} do implicitly. We chose not to fix it to avoid having to
adjust all previously tuned sets of weights.}

\subsection{Training a Rescoring Model} \label{RAT}

The next training step is to create a model for rescoring $n$-best
lists. Rescoring means having \code{canoe} generate a list of $n$ (typically
1000) translation hypotheses for each source sentence, then choosing the best
translations from among these. The advantage of this procedure is that the
choice can be made on the basis of information that is too expensive for
\code{canoe} to use during search. This step usually gives a modest
improvement over the results obtained using \code{canoe} alone. Sometimes,
however, it gives no significant improvement while being fairly slow, so if
translation speed is an issue, you might want to skip rescoring; experiment
with your own data to determine the best choice.  In this framework, rescoring
is automatically bypassed unless \code{DO_RESCORING} is set to 1 in
\code{Makefile.params}.

Training a rescoring model involves generating $n$-best lists, then calculating
the values of selected \emph{features} for each hypothesis in each list. A
feature is any real-valued function that is intended to capture the relation
between a source sentence and a translation hypothesis. A rescoring model
consists of a vector of feature weights set so as to optimize translation
performance when a weighted combination of feature values is used to reorder
the $n$-best lists.

You can run all the steps below by typing \code{make rescore} or
\code{make rat} in your main \code{toy.experiment} directory, but we'll break
it down as usual.

\subsubsection{The Input Rescoring Model}

Training is carried out by the \code{rat.sh} script. This takes as input a
rescoring model that specifies which features to use, and it returns optimal
weights for these features.

The default model created by this framework contains a small set of useful
features:
\begin{small}
\begin{verbatim}
   > cd models/rescore
   > make rescore-model.ini
   configtool rescore-model:ffvals models/decode/canoe.ini.cow \
      | cut -f 1 -d ' ' > rescore-model.ini
   cat rescore-model.template \
      | sed -e "s#IBM\\(.\\)FWD#models/tm/ibm\\1.train.en_given_fr.gz#" \
            -e "s#IBM\\(.\\)BKW#models/tm/ibm\\1.train.fr_given_en.gz#" \
            -e "s#HMM\\(.\\)FWD#models/tm/hmm\\1.train.en_given_fr.gz#" \
            -e "s#HMM\\(.\\)BKW#models/tm/hmm\\1.train.fr_given_en.gz#" \
      >> rescore-model.ini
   > cat rescore-model.ini
   FileFF:ffvals,1
   FileFF:ffvals,2
   FileFF:ffvals,3
   FileFF:ffvals,4
   FileFF:ffvals,5
   FileFF:ffvals,6
   FileFF:ffvals,7
   FileFF:ffvals,8
   FileFF:ffvals,9
   FileFF:ffvals,10
   FileFF:ffvals,11
   FileFF:ffvals,12
   FileFF:ffvals,13
   FileFF:ffvals,14
   FileFF:ffvals,15
   FileFF:ffvals,16
   FileFF:ffvals,17
   # NB: this omits some features that are slow to compute.
   # Use rescore_train -H for a complete list.
   LengthFF
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz
   HMMTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz
   HMMSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz
   HMMVitTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz
   HMMVitSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   nbestWordPostSrc:1#<ffval-wts>#<pfx>
   nbestWordPostTrg:1#<ffval-wts>#<pfx>
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx>
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx>
   nbestNgramPost:2#1#<ffval-wts>#<pfx>
   nbestSentLenPost:1#<ffval-wts>#<pfx>
   ParMismatch
   QuotMismatch:ef
   #CacheLM:<src>.id
   RatioFF
\end{verbatim}
\end{small}
There are two kinds of features included in our rescoring model:
\begin{itemize}
\item Those that look like \code{FileFF:ffvals,}$i$ tell \code{rat.sh} to
use the $i$\/th feature generated by the \code{canoe} decoder itself. It is
standard practice to use all decoder features when rescoring, as is done
automatically by the framework via the \code{configtool} command executed
above.
\item The other features, following the format \emph{FeatureName:Args}, tell
\code{rat.sh} to generate values for the feature \emph{FeatureName} using
arguments \emph{Args}.  For example, the string
\url{IBM2TgtGivenSrc:model/tm/ibm2.train.en_given_fr.gz} says to calculate
the feature \code{IBM2TgtGivenSrc} using the IBM model
\code{model/tm/ibm2.train.en_given_fr.gz}, which we trained earlier. To
see a list of all available features, type \code{rescore_train -H}.  In this
framework, you should set your features in \code{rescore-model.template}.
The special tokens all in upper case (\code{IBM1FWD}, etc) you see in the
provided template are replaced by the actual models you trained earlier.
\end{itemize}

Lines starting with \code{\#} are comments and are ignored by the software.

\subsubsection{Running RAT (Rescore And Translate)}

Apart from the rescoring model, \code{rat.sh} needs a source file and one or
more reference translations for it (same as \code{cow.sh}). These may be the
same files used for \code{cow.sh}, but it is sometimes better to use different
ones, so here we use \code{dev2}:\footnote{As with \code{cow.sh}, you can
speed this up using parallelism, in this case via the -n option to
\code{rat.sh}, given before the \code{train} token (type \code{rat.sh -h}
for details). In this framework, parallelism is controlled via the
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}RESCORE} variable in
\code{Makefile.params}.}
\begin{small}
\begin{verbatim}
   > make all
   cat models/decode/canoe.ini.cow > canoe.ini.cow.dev2
   configtool check canoe.ini.cow.dev2
   ok
   Tuning the rescoring model.
   rat.sh -lb -n 4 train -v -K 1000 -o rescore-model \
      -msrc ../../corpora/dev2_fr.rule -f canoe.ini.cow.dev2 rescore-model.ini \
      ../../corpora/dev2_fr.lc ../../corpora/dev2_en.lc >& log.rescore-model
\end{verbatim}
\end{small}

The output from \code{rat.sh} is written to the file \code{rescore-model}:
\begin{small}
\begin{verbatim}
   > cat rescore-model
   FileFF:ffvals,1 0.146268636
   FileFF:ffvals,2 0.0670145303
   FileFF:ffvals,3 -0.02778978646
   FileFF:ffvals,4 0.00745564606
   FileFF:ffvals,5 0.09763943404
   FileFF:ffvals,6 0.007505016867
   FileFF:ffvals,7 -0.0189950224
   FileFF:ffvals,8 -0.001560098957
   FileFF:ffvals,9 0.4941229522
   FileFF:ffvals,10 -0.005742284469
   FileFF:ffvals,11 0.001069668098
   FileFF:ffvals,12 0.00296442234
   FileFF:ffvals,13 0.001270559616
   FileFF:ffvals,14 0.2126453966
   FileFF:ffvals,15 0.05735969171
   FileFF:ffvals,16 0.02654521167
   FileFF:ffvals,17 0.0006618340849
   # NB: this omits some features that are slow to compute. 0
   # Use rescore_train -H for a complete list. 0
   LengthFF 8.133889787e-05
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz -0.000415542132
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.01087670028
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz 0.00176046392
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz 0.00724581303
   HMMTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz -0.00221155909
   HMMSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz -0.00353578873
   HMMVitTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz -0.00428659326
   HMMVitSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz 0.00148704838
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz 0.0112034809
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz -0.0190998213
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz -1
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.433482093
   nbestWordPostSrc:1#<ffval-wts>#<pfx> -0.004983890336
   nbestWordPostTrg:1#<ffval-wts>#<pfx> 0.001077609486
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx> -0.0001558708027
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx> 0.0002006395371
   nbestNgramPost:2#1#<ffval-wts>#<pfx> 0.02301278338
   nbestSentLenPost:1#<ffval-wts>#<pfx> -0.1470126659
   ParMismatch 6.123407275e-06
   QuotMismatch:ef -5.882521691e-06
   #CacheLM:<src>.id 0
   RatioFF -0.8815089464
\end{verbatim}
\end{small}
This is a copy of \code{rescore-model.ini} with a weight assigned to each
feature. Other by-products created by \code{rat.sh} are found in the directory
\code{workdir-dev2_fr.lc-1000best} and include the $n$-best lists
\code{1000best.gz}, and the corresponding decoder features \code{ffvals.gz}
and additional features \code{ff.*}. All of these files are compressed to
save space. These files are automatically deleted unless there is a problem.
Remove the \code{rm} command executed after \code{rat.sh} in Makefile if
you want to preserve them.

\subsection{Training a Confidence Estimation Model} \label{CE}

If you are going to use confidence estimation (CE), the final step is to train a
model to estimate the confidence of the system on the decoder's output. This
model uses features about the source text, the target text, translation memory
information if available, and so on, to come up with a confidence score between
0 and 1. As we mentioned before, it is critical that the data used to tune the
CE model be completely unseen data. It can't have been part of your training
data or the data you used to tune the decoder weights or the rescoring model.
Otherwise the confidence estimates produced will not be useful.

At this stage we have to return to one previous step. We did not train all
the models we need for CE: we need an LM for the source language. Edit
\code{Makefile.params} in the main \code{toy.experiment} directory and set 
\code{DO_CE} to \code{1} (by uncommenting the approriate line). If you run
\code{make confidence} in the main directory, all necessary steps will be done 
automatically.  As usual, we'll break it down a bit.
\begin{small}
\begin{verbatim}
   > cd models
   > make lm.fr
   make -C lm all LM_LANG=fr
   [...]
\end{verbatim}
\end{small}
This command will build the source-side LM, using the toolkit you selected
earlier.

Now we can work on the CE model itself. Note that our CE module does not
currently work with rescoring, only with decoding. We build on the decoder
model tuned in section~\ref{COW}.  

Just as for decoding and rescoring, CE works with a model described in a text
file.  In this framework we provide a template model that does not use a
translation memory.  If you have access to the results of looking up your
source text in a translation memory, it is worth incorporating them into your
CE model.  In the template provided here, all the translation memory related
features are commented out.  See \url{doc/README.confidence} for more
details.

Now we build the input CE model file:
\begin{small}
\begin{verbatim}
   > cd models/confidence
   > make ce-notm.ini
   ln -fs models/decode/canoe.ini.cow canoe.ini.cow
   configtool check canoe.ini.cow
   ok
   cat ce-notm.template \
      | sed -e "s#IBM\\(.\\)FWD#models/tm/ibm\\1.train.en_given_fr.gz#" \
            -e "s#IBM\\(.\\)BKW#models/tm/ibm\\1.train.fr_given_en.gz#" \
            -e "s#HMM\\(.\\)FWD#models/tm/hmm\\1.train.en_given_fr.gz#" \
            -e "s#HMM\\(.\\)BKW#models/tm/hmm\\1.train.fr_given_en.gz#" \
            -e "s#LM_SRC#models\/lm\/train_fr-kn-5g.binlm.gz#" \
            -e "s#LM_TGT#models\/lm\/train_en-kn-5g.binlm.gz#" \
      > ce-notm.ini
\end{verbatim}
\end{small}

The result is a \code{ce-notm.ini} file with the LMs and IBM models filled in,
which we now feed to \code{ce_translate.pl -train}.  The output will be a
trained CE model.

\begin{small}
\begin{verbatim}
   > make all
   ce_translate.pl -n=4 -train -src=fr -tgt=en -notok -nolc \
      -k=5 -desc=ce-notm.ini canoe.ini.cow ce_model \
      ../../corpora/devCE_fr.lc ../../corpora/devCE_en.lc \
      >& log.ce_model.cem
\end{verbatim}
\end{small}

The trained model is saved to the file \code{ce_model.cem}.  This file is
actually a gzipped-tarred archive, so you could examine it using the
command \code{tar -xOzf ce_model.cem | less}.\footnote{Note that the command
\code{tar -xOzf} contains the capital letter \code{O} (oh), not the number
\code{0} (zero).} However, the contents don't have much intuitive meaning, so
this is only useful for curiosity's sake.

\section{Translating and Testing} \label{TranslatingTesting}

\subsection{Translating} \label{Translating}

Once training is complete, the system can be used to translate new text or our
test corpus.

Some of the steps below will be performed if you run \code{make translate} in
your main \code{toy.experiment} directory, but the final output you need
depends on what you are doing and might not be produced by default. The actual output
produced depends on the various \code{DO_*} variables in
\code{Makefile.params}.

\subsubsection{Decoding Only} \label{Decoding}

As mentioned before, there are three options for translating. The simplest is
to decode using the configuration file produced by \code{cow.sh} and stop.
Earlier in this toy experiment, we set \code{DO_RESCORING} in
\code{Makefile.params}, which we need to change now to
demonstrate translation by decoding only.  Edit \code{Makefile.params} and
comment out the line that says \code{DO_RESCORING = 1}.\footnote{You can
accomplish the same result by adding \code{DO\us{}RESCORING=} to the make
command line, i.e., type \code{make translate DO\us{}RESCORING=} instead of
just \code{make translate}. This syntax can always be used to override a variable
definition on a given call to \code{make}.}
\begin{small}
\begin{verbatim}
   > cd translate
   > # edit ../Makefile.params and comment out DO_RESCORING = 1
   > make translate
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.out
   canoe-parallel.sh -lb -n 4 canoe -f canoe.ini.cow.test \
      < ../corpora/test_fr.rule > test.out 2> log.test.out
\end{verbatim}
\end{small}
This produces the output file \code{test.out}, containing one line for each
source line in \url{test.rule}.

\subsubsection{Decoding plus Rescoring} \label{RATTrans}

If you executed the commands in the \emph{Decoding Only} section above, please
run \code{make clean} before proceeding.  Also, edit \code{Makefile.params}
again and set (or uncomment the line) \code{DO_RESCORING = 1}.
\begin{small}
\begin{verbatim}
   > cd translate
   > # edit ../Makefile.params and set DO_RESCORING = 1
   > make clean
\end{verbatim}
\end{small}

The second option for translating is to generate $n$-best lists and rescore
them using the model generated in section~\ref{RAT}. To do this, we first run
the decoder model tuned in section~\ref{COW}, and then the rescoring model
tuned in section~\ref{RAT}.  The \code{rat.sh} script, used in translation
mode, performs both steps for us:
\begin{small}
\begin{verbatim}
   > make translate
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.rat
   rat.sh -lb -n 4 trans -v -K 1000 -msrc ../corpora/test_fr.rule \
      -f canoe.ini.cow.test models/rescore/rescore-model \
      ../corpora/test_fr.lc >& log.test.rat \
      && mv test_fr.rule.rat test.rat
   cp workdir-test_fr.rule-1000best/1best test.out
\end{verbatim}
\end{small}
This produces both \code{test.out}, the output of the decoder, and
\code{test.rat}, the best translation according to the rescoring model.  The
file \code{test.out} should be identical to the one produced in
section~\ref{Decoding}, although they might differ in minor ways because of
rounding differences.

\subsubsection{Decoding plus Confidence Estimation} \label{CETrans}

The third option for translating is to run the decoder and then estimate the
confidence of the system on the output of the decoder.  This option is not
currently compatible with rescoring: the confidence estimate is for the
one-best output of the decoder.  We use the decoder model tuned in
section~\ref{COW}, and then the CE model trained in section~\ref{CE}.  The
program \code{ce_translate.pl} handles both of these steps:
\begin{small}
\begin{verbatim}
   > cd translate
   > make confidence
   ln -fs models/confidence/ce_model.cem
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   ce_translate.pl -n=4 -notok -nolc -src=fr -tgt=en -test=../corpora/test_en.lc \
      -out=test.ce canoe.ini.cow.test ce_model.cem ../corpora/test_fr.lc \
      2> log.test.ce
\end{verbatim}
\end{small}
The main output file, \code{test.ce}, is different from the \code{test.out}
file we saw in the two previous steps in that 1) a confidence estimate is added
at the beginning of each line, and 2) the output has already been detokenized.
However, the translations are actually the same as in \code{test.out}, with
some postprocessing performed on them.

\subsubsection{Truecasing and Detokenizing} \label{Truecasing}

If you inspect the \code{test.out} and code{test.rat} output files, you will
notice that they contain lowercase, tokenized text. Two postprocessing step are 
required to restore normal case and normally spaced text: truecasing and
detokenizing.

Truecasing is done by the \code{truecase.pl} script using the model trained
previously:
\begin{small}
\begin{verbatim}
   > make tc
   TrueCasing test.out.tc
   truecase.pl --ucBOSEncoding=UTF-8 -text=test.out \
      -lm=models/tc/train_en-kn-3g.binlm.gz -map=models/tc/train_en.map \
      > test.out.tc 2> log.test.out.tc
   TrueCasing test.rat.tc
   truecase.pl --ucBOSEncoding=UTF-8 -text=test.rat \
      -lm=models/tc/train_en-kn-3g.binlm.gz -map=models/tc/train_en.map \
      > test.rat.tc 2> log.test.rat.tc
\end{verbatim}
\end{small}
Note the \code{--ucBOSEncoding=UTF-8} option: it tells the truecaser to
upper-case the beginning of each sentence (BOS), assuming the output is encoded
in UTF-8.  This program works with other encodings, as do most \PS programs,
but the framework assumes UTF-8 in its text preprocessing (lowercasing,
tokenization) and postprocessing (truecasing, detokenization).

Detokenizing is done by the script \code{udetokenize.pl}, which has
hand-coded rules to detokenize French or English text encoded in UTF-8. Other
languages are not supported at this point but may already be handled partly
correctly by this script. As for other encodings, latin1 and cp-1252 are
supported by \code{detokenize.pl}.

\begin{small}
\begin{verbatim}
   > make detok
   Detokenizing test.out
   udetokenize.pl -lang=en test.out test.out.detok
   Detokenizing test.out.tc
   udetokenize.pl -lang=en test.out.tc test.out.tc.detok
   Detokenizing test.rat.tc
   udetokenize.pl -lang=en test.rat.tc test.rat.tc.detok
   Detokenizing test.rat
   udetokenize.pl -lang=en test.rat test.rat.detok
\end{verbatim}
\end{small}

Detokenizing is not performed here on the \code{test.ce} file because it was
already done by \code{ce_translate.pl}.  Truecasing can optionally be performed by
\code{ce_translate.pl}, using the \code{-tc*} options, but this is
not integrated in the framework yet (type \code{ce_translate.pl -h} for
details.
% TODO fix the framework so that it does handle truecasing.

\subsection{Testing} \label{Testing}

Translation quality can be evaluated automatically using the BLEU metric, which
is a measure of how well the translation matches one or more reference
translations that are known to be correct (normally human translations). It is
based on the number of short word sequences that the translation has in common
with the references, and varies between 0 for no matches to 1 for a perfect
match. BLEU is calculated by the program \code{bleumain}:
\begin{small}
\begin{verbatim}
   > make bleu
   Calculating BLEU for test.out.bleu
   bleumain -c test.out ../corpora/test_en.lc > test.out.bleu
   Calculating BLEU for test.rat.bleu
   bleumain -c test.rat ../corpora/test_en.lc > test.rat.bleu
   grep Human *.bleu
   test.out.bleu:Human readable value: 18.11 +/- 3.17
   test.rat.bleu:Human readable value: 18.33 +/- 3.29
\end{verbatim}
\end{small}
The human readable value is the BLEU score multiplied by 100 rounded to 2
decimals. The full output from \code{bleumain}, saved in \code{*.bleu},
contains match statistics of various orders, followed by the global BLEU score
with a 95\% confidence interval, as shown above.

The result above can also be obtained by typing \code{make eval}, or
\code{make all}, in your \code{toy.experiment} directory.

Here, we calculate only the BLEU scores on the lowercase output, using the
lowercase reference, but you can get BLEU scores for the truecased output by
giving the truecase translation and reference(s) to \code{bleumain}.

These results indicate that the translation produced by rescoring is slightly
better than the one produced by plain decoding. Given the small size of the
test set, it seems unlikely that the difference is statistically
significant. To test this hypothesis, we can use \code{bleucompare}, which does
a comparison using pairwise bootstrap resampling:
\begin{small}
\begin{verbatim}
   > bleucompare test.rat test.out REFS ../corpora/test_en.lc 
   Comparing using BLEU
   test.rat got max BLEU score in 64.3% of samples
   test.out got max BLEU score in 35.7% of samples
\end{verbatim}
\end{small}
This indicates that the difference is not significant.  (Note that you may get
completely different results here, since the training corpus used is much too
small to produce reliable results.)

\section{PortageLive}

Now that we have a fully trained and tested system, you might want to deploy it
on a PortageLive translation server.  

The PortageLive models are optimized in various ways for a run-time
environment.  They use our tightly packed model file format, accessed
via memory-mapped IO; the various phrase tables are collapsed into a single
table with the weights pre-applied; filtering of the phrase table, normally
done on the fly, is pre-performed; and so on.  The result is a system that will
run faster and with less memory on the run-time server than within the
framework.

There is a \code{make} target in the framework to facilitate this process:
\code{make portageLive}.  This target, run directly from the top framework
directory (\code{toy.expermiment}), will create a directory structure with
symbolic links to all the model files needed on a run-time translation  server. 
You can then copy this structure to your run-time server by using a recursive
copy command that dereferences symbolic links.

\begin{small}
\begin{verbatim}
   > make portageLive
   [...]
   [lots of stuff happens]
   [...]
   from the root of the framework, you now have all that is required
   for portageLive.
   rsync -Larz models/portageLive/* <RHOST>:/<DEST_DIR_RHOST>
   scp -r models/portageLive/* <RHOST>:/<DEST_DIR_RHOST>
   cp -Lr models/portageLive/* /<DEST_DIR>
\end{verbatim}
\end{small}

The three sample commands printed at the end of the execution show you how to
perform a dereferencing copy of the structure created.

You can use \code{du -hL} to find out the size that structure will have once
fully expanded:
\begin{small}
\begin{verbatim}
   > du -hL models/portageLive/
   12M     models/portageLive/models/ldm/ldm.hmm3+ibm2.fr2en.tpldm
   12M     models/portageLive/models/ldm
   11M     models/portageLive/models/tm/cpt.fr2en.tppt
   22M     models/portageLive/models/tm
   652K    models/portageLive/models/tc/tc-map.en.tppt
   1.2M    models/portageLive/models/tc/tc-lm.en.tplm
   1.8M    models/portageLive/models/tc
   1.3M    models/portageLive/models/lm/train_fr-kn-5g.tplm
   1.3M    models/portageLive/models/lm/train_en-kn-5g.tplm
   2.6M    models/portageLive/models/lm
   39M     models/portageLive/models
   39M     models/portageLive/
\end{verbatim}
\end{small}


\section{Resource Summary}

Now that the whole experiment has run, we can get a global summary of resource
usage from all the parts of the framework, as shown in Figure~\ref{FigTimeMem}.

\begin{sidewaysfigure}
\caption{Time and memory resource summary}
\label{FigTimeMem}.
\begin{footnotesize}
\begin{verbatim}
> make time-mem
      log.ldm.counts.hmm3:ldm:TIME-MEM              WALL TIME: 32s        CPU TIME: 30s         VSZ: 0.075G     RSS: 0.008G
      log.ldm.counts.ibm2:ldm:TIME-MEM              WALL TIME: 10s        CPU TIME: 8s          VSZ: 0.075G     RSS: 0.007G
      log.ldm.hmm3+ibm2.fr2en:ldm:TIME-MEM          WALL TIME: 16s        CPU TIME: 12s         VSZ: 0.059G     RSS: 0.008G
   ldm:TIME-MEM                                     WALL TIME: 58s        CPU TIME: 50s         VSZ: 0.075G     RSS: 0.008G
      log.train_en-kn-5g.binlm:lm:TIME-MEM          WALL TIME: 3s         CPU TIME: 2s          VSZ: 0.059G     RSS: 0.006G
      log.train_en-kn-5g.lm:lm:TIME-MEM             WALL TIME: 2s         CPU TIME: 1s          VSZ: 0.065G     RSS: 0.012G
      log.train_fr-kn-5g.binlm:lm:TIME-MEM          WALL TIME: 4s         CPU TIME: 3s          VSZ: 0.064G     RSS: 0.009G
      log.train_fr-kn-5g.lm:lm:TIME-MEM             WALL TIME: 2s         CPU TIME: 1s          VSZ: 0.065G     RSS: 0.013G
   lm:TIME-MEM                                      WALL TIME: 11s        CPU TIME: 7s          VSZ: 0.065G     RSS: 0.013G
      log.cpt.hmm3-rf-zn.train.fr2en:tm:TIME-MEM    WALL TIME: 22s        CPU TIME: 22s         VSZ: 0.070G     RSS: 0.009G
      log.cpt.ibm2-rf-zn.train.fr2en:tm:TIME-MEM    WALL TIME: 8s         CPU TIME: 6s          VSZ: 0.070G     RSS: 0.009G
      log.hmm3.train.en_given_fr:tm:TIME-MEM        WALL TIME: 1m36s      CPU TIME: 6m37s       VSZ: 0.071G     RSS: 0.009G
      log.hmm3.train.fr_given_en:tm:TIME-MEM        WALL TIME: 1m21s      CPU TIME: 6m3s        VSZ: 0.071G     RSS: 0.010G
      log.ibm1.train.en_given_fr:tm:TIME-MEM        WALL TIME: 34s        CPU TIME: 1m8s        VSZ: 0.075G     RSS: 0.009G
      log.ibm1.train.fr_given_en:tm:TIME-MEM        WALL TIME: 38s        CPU TIME: 1m8s        VSZ: 0.075G     RSS: 0.009G
      log.ibm2.train.en_given_fr:tm:TIME-MEM        WALL TIME: 42s        CPU TIME: 57s         VSZ: 0.071G     RSS: 0.016G
      log.ibm2.train.fr_given_en:tm:TIME-MEM        WALL TIME: 1m1s       CPU TIME: 56s         VSZ: 0.071G     RSS: 0.009G
      log.jpt.hmm3.train.fr-en:tm:TIME-MEM          WALL TIME: 24s        CPU TIME: 35s         VSZ: 0.157G     RSS: 0.011G
      log.jpt.ibm2.train.fr-en:tm:TIME-MEM          WALL TIME: 22s        CPU TIME: 12s         VSZ: 0.157G     RSS: 0.011G
   tm:TIME-MEM                                      WALL TIME: 7m8s       CPU TIME: 18m4s       VSZ: 0.157G     RSS: 0.016G
      log.train_en-kn-3g.binlm:tc:TIME-MEM          WALL TIME: 2s         CPU TIME: 1s          VSZ: 0.068G     RSS: 0.008G
      log.train_en-kn-3g.lm:tc:TIME-MEM             WALL TIME: 13s        CPU TIME: 0s          VSZ: 0.065G     RSS: 0.012G
      log.train_en.map:tc:TIME-MEM                  WALL TIME: 1s         CPU TIME: 0s          VSZ: 0.046G     RSS: 0.007G
   tc:TIME-MEM                                      WALL TIME: 16s        CPU TIME: 1s          VSZ: 0.068G     RSS: 0.012G
      log.canoe.ini.cow:decode:TIME-MEM             WALL TIME: 25m20s     CPU TIME: 1h6m20s     VSZ: 0.525G     RSS: 0.459G
   decode:TIME-MEM                                  WALL TIME: 25m20s     CPU TIME: 1h6m20s     VSZ: 0.525G     RSS: 0.459G
      log.rescore-model:rescore:TIME-MEM            WALL TIME: 4m23s      CPU TIME: 15m25s      VSZ: 0.105G     RSS: 0.022G
   rescore:TIME-MEM                                 WALL TIME: 4m23s      CPU TIME: 15m25s      VSZ: 0.105G     RSS: 0.022G
      log.ce_model.cem:confidence:TIME-MEM          WALL TIME: 26s        CPU TIME: 2m14s       VSZ: 0.094G     RSS: 0.010G
   confidence:TIME-MEM                              WALL TIME: 26s        CPU TIME: 2m14s       VSZ: 0.094G     RSS: 0.010G
      log.test.ce:translate:TIME-MEM                WALL TIME: 29s        CPU TIME: 2m29s       VSZ: 0.094G     RSS: 0.010G
      log.test.rat:translate:TIME-MEM               WALL TIME: 4m31s      CPU TIME: 16m5s       VSZ: 0.091G     RSS: 0.019G
      log.test.rat.tc:translate:TIME-MEM            WALL TIME: 1s         CPU TIME: 0s          VSZ: 0.049G     RSS: 0.009G
   translate:TIME-MEM                               WALL TIME: 5m1s       CPU TIME: 18m34s      VSZ: 0.094G     RSS: 0.019G
TIME-MEM                                            WALL TIME: 43m43s     CPU TIME: 2h1m35s     VSZ: 0.525G     RSS: 0.459G
\end{verbatim}
\end{footnotesize}
\end{sidewaysfigure}

The output of \code{make time-mem} tells us how much RAM (``RSS'') and
virtual memory (``VSZ'') was used by each step of the process. It also tells us
the total amount of CPU time and the actual elapsed time (``wall time'').  When
running in parallel, the total CPU time will be higher than the wall time, as is
the case here.

In each summary line (the ones outdented, those for each component, and the
global summary at the end), the wall and CPU times are simply the sum of the
lines it summarizes, whereas the two memory figures are the maximum values from
the lines that it summarizes.

The memory figures might be a bit misleading: they reflect the maximum
RAM/virtual memory needed by any one component within a run.  In the case of a
process that is parallelized, say, 10 ways, the figure reflects the most memory
any one of the 10 parallel processes used, not the total used at any given
time.  When using this output to plan resources, you may need to multiply the
memory figures by the parallelism level you chose for each step.

The numbers here are not very interesting because we ran on such a small
system, but they'll be a lot more informative when you run on real data.

The second type of resource summary you can produce is the disk space occupied
by the models.  When you run \code{make summary}, you will first get the
\code{time-mem} report, then the following disk space report:
\begin{small}
\begin{verbatim}
   > make summary
   [output from time-mem plus:]
   13M     models/ldm
   804K    models/lm/train_en-kn-5g.binlm.gz
   944K    models/lm/train_en-kn-5g.lm.gz
   912K    models/lm/train_fr-kn-5g.binlm.gz
   1.1M    models/lm/train_fr-kn-5g.lm.gz
   4.7M    models/tm/ibm1.train.en_given_fr.gz
   4.6M    models/tm/ibm1.train.fr_given_en.gz
   3.7M    models/tm/ibm2.train.en_given_fr.gz
   148K    models/tm/ibm2.train.en_given_fr.pos.gz
   3.8M    models/tm/ibm2.train.fr_given_en.gz
   152K    models/tm/ibm2.train.fr_given_en.pos.gz
   4.0K    models/tm/hmm3.train.en_given_fr.dist.gz
   2.0M    models/tm/hmm3.train.en_given_fr.gz
   4.0K    models/tm/hmm3.train.fr_given_en.dist.gz
   2.1M    models/tm/hmm3.train.fr_given_en.gz
   3.5M    models/tm/jpt.hmm3.train.fr-en.gz
   1.3M    models/tm/jpt.ibm2.train.fr-en.gz
   11M     models/tm/cpt.hmm3-rf-zn.train.fr2en.gz
   3.2M    models/tm/cpt.ibm2-rf-zn.train.fr2en.gz
   1.7M    models/tc
   34M     translate
   91M     total
\end{verbatim}
\end{small}

This report differs from a simple \code{du -h} in that it looks specifically
for the model files.

\section{Final Note}
Because of differences in rounding, optimization, random number generation,
compilers, hardware, etc., results, especially numerical ones, are expected to
vary on different systems and are shown in this document only as an indication
of the type of output to expect, especially given the trivial size of the
corpus used.

\end{document}
