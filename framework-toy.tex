\documentclass[11pt]{article}
\usepackage{isolatin1}
\usepackage{xspace}

\newcommand{\phs}{\tild{s}}   % source phrase
\newcommand{\pht}{\tild{t}}   % target phrase
% Official typesetting of PORTAGEshared
\newcommand{\PS}{PORTAGE\-\emph{shared}\xspace}

\title{A toy experiment using the \PS \\
       experimental framework}
\date{}
\author{Eric Joanis}

\begin{document}
\maketitle

\begin{center}
An adaption of George Foster's \emph{Running Portage: A Toy Example} \\
to Samuel Larkin's experimental framework, updated to reflect recommended usage
of \PS.
\end{center}

\begin{center}
{~} \\ \tiny
   Technologies langagi{\`e}res interactives /
      Interactive Language Technologies \\
   Institut de technologie de l'information /
      Institute for Information Technology \\
   Conseil national de recherches Canada /
      National Research Council Canada \\
   Copyright \copyright 2008, Sa Majest{\'e} la Reine du Chef du Canada /
      Her Majesty in Right of Canada
\end{center}

\tableofcontents

\section{Introduction}

This document describes how to run a toy experiment using this experimental
framework from beginning to end.  It is intended as a tutorial in using \PS, as
well as a starting point for further experiments.  Although the framework
automates most of the steps described below, we go through them one by one here
in order to better explain how to use the \PS software suite.

\PS can be viewed as a set of programs for turning a bilingual corpus into a
translation system.  This document describes this process performed on a
trivially small data set.  The example is for French to English translation,
using text from the Hansard corpus.  It is too small for good translation, but
large enough to give the flavour of a more realistic setup. Running time is
about XXX.

\subsection{Running the toy experiment}

To begin, you must build or obtain the \PS executables and ensure that they are
in your path, typically by sourcing the \texttt{SETUP.bash/ .tcsh} file as
customized for your environment during installation of \PS.  You should also
set your environment variable \texttt{\$PORTAGE} to the directory where \PS is
installed, which is also done by \texttt{SETUP.bash/ .tcsh}.  Then you must make
a complete copy of this framework directory hierarchy, because it is designed
to work in place, creating the models within this hierarchy itself.  The
philosophy of this framework is that each experiment is done in a separate copy
of the framework.

For example:
\begin{verbatim}
   > cd \$PORTAGE
   > cp -pr framework toy.experiment
   > cd toy.experiment
\end{verbatim}
All commands provided in the rest of this document are assumed to run in
toy.experiment or in a subdirectory thereof, which will all be specified
relative to toy.experiment.

As you work through the example, the commands that you are supposed to type are
preceded by a prompt, \texttt{>}, and the system's response is not, though
system output is not always fully reproduced here, for brevity's sake.  When it
is, results (especially numbers) may vary a little from the ones shown, due to
platform differences.

Many of the commands are expressed as \texttt{make} targets. This has the
advantage of requiring less typing, while still allowing you to see the actual
commands executed by the system, which are always echoed by \texttt{make} (and
which of course you could also type directly). \texttt{make} also lets you skip
steps (except for the first one, since it is done manually).  For example, if
you are not interested in any steps before decoder weight optimization, you can
go to section~\ref{COW} and type \texttt{make tune} to begin at that point
(although you will have to wait until the system executes the necessary
commands to catch up). Here are some other useful make commands:
\begin{itemize}
\item \texttt{make all} runs all remaining steps at any point.
\item \texttt{make clean} cleans up and returns the directory to its initial state
\item \texttt{make -j} \emph{anything} builds the target \emph{anything} by running
      commands in parallel whenever possible. This can be much faster if you
      have a multiprocessor machine.
\item \texttt{make help} displays some help and the main targets available in the
      makefile.
\end{itemize}

\subsection{Overview of the Process}

Here is an overview of the \PS process, as described in the following
sections (section numbers are in brackets):
\begin{enumerate}
\item Corpus preparation (\ref{CorpusPreparation}): includes tokenization
      (\ref{Tokenization}), alignment (\ref{Alignment}), lowercasing
      (\ref{Lowercasing}), and splitting (\ref{Splitting}).
\item Model training (\ref{Training}): includes language model (\ref{LM}),
      translation model (\ref{TM}), truecasing model (\ref{TC}), decoder weight
      optimization (\ref{COW}), and rescoring model training (\ref{RAT}).
\item Translating and testing (\ref{TranslatingTesting}): includes
      translating (\ref{Translating}), truecasing (\ref{Truecasing}) and
      testing (\ref{Testing}).
\end{enumerate}
These steps are fairly standard, but of course there are many variants on the
sample process illustrated here. For more information, see the Portage HTML
user documentation. For detailed information on a particular program, run it
with the \texttt{-h} flag.

\section{Corpus Preparation} \label{CorpusPreparation}

Corpus preparation involves converting raw document pairs into tokenized,
sentence-aligned files in the format expected by Portage.  We provide a
tokenizer, a sentence aligner and sample corpus pre-processing scripts with
\PS, which can help you with these steps.  For details, see section \emph{Text
Processing} in the user manual (found in
\textt{PORTAGEshared/doc/user-manual.html} on the CD).

\subsection{Splitting the Corpus} \label{Splitting}

The tokenized, sentence-aligned corpus must be split into separate portions
in order to run experiments. Distinct, non-overlapping sub-corpora are required
for model training (see section~\ref{Training}), for optimizing decoder weights
(section~\ref{COW}), for training a rescoring model (section~\ref{RAT}), and
one  testing (section~\ref{Testing}). Typically, the latter three corpora are
around 1000 segments each.  If the corpus is chronological, then it is a good
idea to choose the test corpus from the most recent material, which is likely
to resemble future text more closely.

Since proper splitting of your corpus needs to take into account the structure
and nature of your data, we don't perform those steps in this toy experiment.
Instead, we've provided small sets that can be found here:
\texttt{PORTAGEshared/test-suite/unit-testing/framework-toy}.  These sets
are very small to reduce running time, so you should not be surprised when the
results of this toy experiment are of poor quality.

To drop these sets into the framework, simply copy them (or make symbolic
links) into your copy's corpora directory:
\begin{verbatim}
   > cp $PORTAGE/test-suite/unit-testing/framework-toy/*.al corpora/
   > wc corpora/*.al
       100    2115   11589 corpora/dev1_en.al
       100    2330   13680 corpora/dev1_fr.al
       100    2095   11462 corpora/dev2_en.al
       100    2443   13905 corpora/dev2_fr.al
       100    2379   13292 corpora/test_en.al
       100    2616   15622 corpora/test_fr.al
      8992  184569  994516 corpora/train_en.al
      8992  211240 1194385 corpora/train_fr.al
     18584  409787 2268451 total
\end{verbatim}

In your own experiments, the files you need to copy into \texttt{corpora}
should be in original truecase (if you're using truecasing), tokenized,
sentence-split and aligned, just like the ones we provide here.

\subsection{Setting framework parameters} \label{FrameworkParams}

Now you need to tell the framework what you've called your test set, since we
didn't use the default.  Edit \texttt{Makefile.params} and change the
\texttt{TEST\_SET} variable to say
\texttt{test} instead of \texttt{test1 test2}.
This variable lists the stem of all your test sets, and the framework
assumes you have two by default.
You will also need to set \texttt{TRAIN\_LM} to \texttt{train} instead of
\texttt{lm-train} since we won't be using a separate corpus to train language
models with.
After editing the file:
\begin{verbatim}
   > grep "export TEST_SET" Makefile.params
   export TEST_SET ?= test
   > grep "export TRAIN_LM" Makefile.params
   export TRAIN_LM ?= train
\end{verbatim}

While you're there, you should get familiar with all the variables in the
\emph{User definable variables} section of this file.  It is where most of the
configurable behaviours are set, such as whether to do rescoring and/or
truecasing, what LM toolkit you are using (if any), etc.  

In particular, the example is for translating from French to English, while the
default in the framework is the other direction.  To proceed as described
below, you should reverse the values of \texttt{SRC\_LANG} and
\texttt{TGT\_LANG}.
\begin{verbatim}
   > grep "_LANG " Makefile.params
export SRC_LANG ?= fr
export TGT_LANG ?= en
\end{verbatim}

In this toy experiment, we'll use the default value for most other parameters,
but you need to set the \texttt{IRSTLM} variable if you have installed IRSTLM
and wish to use it, or the \texttt{LM\_TOOLKIT\_SRI} to use SRILM (if your
licensing requirements permit it).  If you have neither, leave both variables
undefined; we provide trained language models that you can drop into the
framework to finish this toy experiment while skipping language model training.

Another set of parameters you might want to adjust are the various
\texttt{PARALLELISM\_LEVEL\_*} variables.  \PS is written in such a way as to
take advantage of multi-processor computers and/or multi-node computing
clusters, doing tasks in parellel where possible.  By default, the framework
uses \texttt{qsub} to submit jobs if you're working on a cluster, but minimizes
parallelism otherwise.  Either way, parallelism will be even more aggressive if
you add the \texttt{-j} option to each \texttt{make} command line (use with
caution!).

One more thing you will notice about this framework is that most commands run
by make are preceeded by \texttt{RP\_PSUB\_OPTS=...} or \texttt{\_LOCAL=1}.
These only have an impact when running on a cluster, and are written in such a
way that they are effectively ignored otherwise.  When working on a cluster,
commands preceded by \texttt{\_LOCAL=1}, which should be very short commands,
will not be run directly instead of being submitted to the queue, while
\texttt{RP\_PSUB\_OPTS=...} is used to specify additional options to
\texttt{psub}, which is used in \PS to encapsulate the invocation of
\texttt{qsub}.  If your cluster has specific usage rules or your require
additional parameters to \texttt{qsub}, you might want to customize
\texttt{psub} itself or add options as required in this framework.

\subsection{Lowercasing and adding escapes}

To reduce data sparseness, we convert all files to lowercase.  We keep the
lowercase and truecase versions separately, because we'll use the truecase
version to 
\begin{verbatim}
   > cd corpora
   > make lc
   cat dev1_fr.al | lc-utf8.pl > dev1_fr.lc
   [...]
\end{verbatim}

The decoder, canoe, treats \texttt{<}, \testtt{>} and \texttt{\\} as special
characters, in order to support markups for special translation rules.  We
won't use any markups for this example, but we still need to escape the three
special characters, in the source language only, since this is only for the
input to canoe.

\begin{verbatim}
   > make rule
   canoe-escapes.pl -add dev1_fr.lc > dev1_fr.rule
   canoe-escapes.pl -add dev2_fr.lc > dev2_fr.rule
   canoe-escapes.pl -add test_fr.lc > test_fr.rule
\end{verbatim}



\section{Training} \label{Training}

This step creates various models and parameter files that are required for
translation. There are five steps in training: creating a language model,
creating a truecasing model, creating translation models, optimizing decoder
weights, and creating a rescoring model.

\subsection{Creating a Language Model} \label{LM}

Portage does not come with programs for language model training. However,
it accepts models in the widely-used ``DARPA'' format which is produced by popular
toolkits such as CMU SLM (www.speech.cs.cmu.edu/ SLM\_info.html), SRILM
(www.speech.sri.com/projects/srilm), or IRSTLM (irstlm.sourceforge.net).

If you are using the IRSTLM toolkit, here is the command used to produce the
model needed for this example, \texttt{train\_en-kn-5g.binlm.gz}:
\begin{verbatim}
   > make lms
   make -C models/lm all
   zcat -f ../../corpora/train_en.lc.gz | add-start-end.sh | gzip -c \
      > train_en.marked.gz
   build-lm.sh -p -t stat.$$ -n 5 -k 1 -s kneser-ney \
      -i "gunzip -c train_en.marked.gz" -o train_en-kn-5g.ilm.gz \
      >& log.train_en-kn-5g.ilm
   compile-lm --text yes train_en-kn-5g.ilm.gz unsorted.train_en-kn-5g.lm \
      >& log.train_en-kn-5g.lm
   lm_sort_filter.sh -lm unsorted.train_en-kn-5g.lm train_en-kn-5g.lm.gz
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
The first three commands executed, \texttt{add-start-end.sh},
\texttt{build-lm.sh} and \texttt{compile-lm}, construct the language model in
``DARPA'' format using IRSTLM software.  The next command,
\texttt{lm\_sort\_filter.sh}, reorders the contents of the LM to maximize the
compression ratio \texttt{gzip} can achieve.  The sort step is unecessary with
a small language model, but can significantly reduce final file sizes with very
large language models.  The last command, \texttt{arpalm2binlm}, converts the
language model into the Portage binary language model format.  This binary
format contains the same information, but is much faster to load in \PS
software.

If you are using the SRILM toolkit, here is the command used to produce this
model:
\begin{verbatim}
   > make lms
   make -C models/lm all 
   ngram-count -interpolate -kndiscount -order 5 \
      -text ../../corpora/train_en.lc.gz -lm train_en-kn-5g.lm.gz \
      >& log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \
      >& log.train_en-kn-5g.binlm
\end{verbatim}
The first command executed, \texttt{ngram-count}, produces the language model
itself in ``DARPA'' format.  The second command, \texttt{arpalm2binlm},
converts it into the Portage binary language model format, like in the IRSTLM
example.  With SRILM, we don't use the optional sorting step, since its output
already compresses well.

With either toolkit, this example assumes that the target language is English;
for English to French translation, a similar command could be used to create a
French language model.

In this example, we only use the target language part of the parallel training
corpus to train the language model, but this is not the recommended practice.
If you have access to larger corpora of monolingual text in your target
language, you can use them to train additional language models.  The framework
does not automate training such additional language models, but you can add
them in manually before optimizing the decoder weights (see section~\ref{COW}).
XXX Make sure this forward ref is correct. XXX

\subsection{Creating a Truecasing Model} \label{TC}

Decoding is done in lowercase to reduce data sparseness, but at the end of the
translation process, you might want to restore proper mixed case to your
output.  We call this step truecasing.  To train a truecasing model, you need
both a lowercased and the original ``truecase'' version of the training corpus
in the target language.

The truecasing model consists of two different models: a ``casemap'', which
maps each lower case words to its possible truecase variants, and a standard
language model trained on the truecase corpus.

\begin{verbatim}
   > make tc
   make -C models/tc all
   compile_truecase_map ../../corpora/train_en.tc.gz \
      ../../corpora/train_en.lc.gz \
      > train_en.map
   [commands to train the truecase LM]
\end{verbatim}
The first command, \texttt{compile\_truecase\_map}, compiles the casemap using
by reading the truecase and lowercased version of the corpus simultaneously.
The next commands will depend on your choice of LM toolkit, training a language
model as in section~\ref{LM} above, but using the truecase corpus,
\texttt{train\_en.tc.gz}, instead of the lowercased version,
\texttt{train\_en.lc.gz}, as above.

\subsection{Creating a Translation Model} \label{TM}

Creating a translation model involves two main steps: training IBM and HMM word
alignment models in both directions, then using them to extract phrase pairs
from the corpus.  This step was once fairly simple, but not the recommended
practice is to train and use two sets of phrase tables: one from IBM2 word
alignments, and one from HMM word alignments.  You can do all this by typing
\texttt{make tms} in your \texttt{toy.experiment} directory, but we'll break it
down in several steps here.

\subsubsection{Creating a Translation Model Using IBM2 Alignments}

First we train IBM2 word alignment models, which require training IBM1 models
as a prerequisite.  We do this for both directions.
\begin{verbatim}
   > cd models/tm
   > make ibm2_model
   make -C models/tm all
   Generating ibm1.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -n1 5 -n2 0 ibm1.train.en_given_fr.gz 
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.en_given_fr
   Generating ibm2.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -m -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -i ibm1.train.en_given_fr.gz ibm2.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.en_given_fr
   Generating ibm1.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 5 -n2 0 ibm1.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm1.train.fr_given_en
   Generating ibm2.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \
      -i ibm1.train.fr_given_en.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.ibm2.train.fr_given_en
\end{verbatim}
The \texttt{train\_ibm} program could have been used directly, but we use
\texttt{cat.sh} instead, so that we can take advantage of parallelism.  Here
the options \texttt{-n 1 -pn 1} disable all parallelism, because we ran it on a
non-clustered computer, but the degree of parallelism used is controlled by the
\texttt{PARALLELISM\_LEVEL\_TM} variable in \texttt{Makefile.params}.

The commands above write log files containing information about training.  For
example, the IBM log files show convergence and pruning statistics for each
iteration:
\begin{verbatim}
   > grep -h ppx log.ibm[12]*en_given_fr
   iter 1 (IBM1): prev ppx = 478.842, size = 1488446 word pairs; took 7 second(s).
   iter 2 (IBM1): prev ppx = 125.745, size = 1368326 word pairs; took 6 second(s).
   iter 3 (IBM1): prev ppx = 79.431, size = 1170561 word pairs; took 6 second(s).
   iter 4 (IBM1): prev ppx = 65.4013, size = 996641 word pairs; took 5 second(s).
   iter 5 (IBM1): prev ppx = 60.3716, size = 853457 word pairs; took 5 second(s).
   parallel iter (IBM2): prev ppx = 58.1128, size = 853135 word pairs.
   parallel iter (IBM2): prev ppx = 41.3624, size = 852820 word pairs.
   parallel iter (IBM2): prev ppx = 34.4884, size = 838117 word pairs.
   parallel iter (IBM2): prev ppx = 31.1112, size = 754164 word pairs.
   parallel iter (IBM2): prev ppx = 29.3212, size = 647532 word pairs.
\end{verbatim}

The IBM models are written to files \texttt{ibm[12].*} and contain word
translation/alignment probabilities.

Now we extract the joint-count phrase table from the same parallel corpus,
using IBM2 word alignment models in both directions together.
\begin{verbatim}
   > make ibm2_jpt
   Generating jpt.ibm2.train.fr-en.gz
   gen-jpt-parallel.sh -n 1 -o jpt.ibm2.train.fr-en.gz GPT \
      -v -j -w 1 -m 8 -ibm 2 -1 fr -2 en \
      ibm2.train.en_given_fr.gz ibm2.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      2> log.jpt.ibm2.train.fr-en
\end{verbatim}
The joint-count phrase table contains the raw joint occurrence frequency for
each phrase pair observed in the parallel corpus.

Finally we compute the conditional phrase table, using three different
smoothers to calculate the probabilities.  In general, we obtain better
translation quality when using phrase probabilities estimated using relative
frequencies (``RFSmoother''), relative frequencies with Kneser-Ney smoothing
(``KNSmoother'') and the lexical smoothing proposed and Zens and Ney
(``ZNSmoother'').  Although output together in the same phrase table, they are
separate probability models, whose relative weights will be tuned during
decoder weight optimization (see section~\ref{COW}).
\begin{verbatim}
   > make ibm2_cpt
   Generating cpt.ibm2-rf-zn-kn3.train.fr2en.gz
   joint2cond_phrase_tables -sort -prune1 100 -ibm 2 -v -i -z \
      -1 fr -2 en -s RFSmoother -s ZNSmoother -s "KNSmoother 3" \
      -multipr fwd -o cpt.ibm2-rf-zn-kn3.train \
      -ibm_l2_given_l1  ibm2.train.en_given_fr.gz \
      -ibm_l1_given_l2  ibm2.train.fr_given_en.gz \
      jpt.ibm2.train.fr-en.gz \
      >& log.cpt.ibm2-rf-zn-kn3.train.fr2en
\end{verbatim}
The phrasetable \texttt{jpt.ibm2.train.fr-en.gz}, jointly with its HMM variant
(see below), is the main source of information for French to English
translation.  Each of its lines is of the form:
\begin{verbatim}
   fr ||| en ||| p(fr|en) p(en|fr)
\end{verbatim}
where \texttt{fr} is a French source phrase, \texttt{en} is an English target
phrase, \texttt{p(fr|en)} is the probability that \texttt{fr} is the
translation of \texttt{en}, and \texttt{p(en|fr)} is the probability that
\texttt{en} is the translation of \texttt{fr}.
Here are two sample lines from this file:
\begin{verbatim}
   > zgrep '| proposed regulations |' cpt.ibm2-rf-zn-kn3.train.fr2en.gz 
   projets de règlement ||| proposed regulations ||| 0.5 5.74665412e-07 0.0586759842 1 0.00752825038 0.117351968
   règlements proposés ||| proposed regulations ||| 0.5 0.0124936768 0.0586759842 1 0.383383272 0.117351968
\end{verbatim}
You'll notice there are in fact six numbers per line.  That's because we used
three smoothers: we have three estimates of \texttt{p(fr|en)} followed by three
estimates of \texttt{p(en|fr)}.  Also, this example is carefully chosen;
because of the small size of the training corpus, many other entries in the
phrase table are incorrect.

\subsubsection{Creating a Translation Model Using HMM Alignments}

Now we repeat the same steps using HMM word alignment models, using the
``Liang'' variant.
\begin{verbatim}
   > make liang_cpt
   Generating liang.train.en_given_fr.gz
   cat.sh -n 1 -pn 1 -v -n1 0 -n2 5 -mimic liang \
      -i ibm1.train.en_given_fr.gz liang.train.en_given_fr.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.liang.train.en_given_fr
   Generating liang.train.fr_given_en.gz
   cat.sh -n 1 -pn 1 -v -r -n1 0 -n2 5 -mimic liang \
      -i ibm1.train.fr_given_en.gz liang.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      >& log.liang.train.fr_given_en
   Generating jpt.liang.train.fr-en.gz
   gen-jpt-parallel.sh -n 1 -o jpt.liang.train.fr-en.gz \
      GPT -v -j -w 1 -m 8 -hmm -1 fr -2 en \
      liang.train.en_given_fr.gz liang.train.fr_given_en.gz \
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \
      2> log.jpt.liang.train.fr-en
   Generating cpt.liang-rf-zn-kn3.train.fr2en.gz
   joint2cond_phrase_tables -sort -prune1 100 -hmm -v -i -z \
      -1 fr -2 en -s RFSmoother  -s ZNSmoother  -s "KNSmoother 3" \
      -multipr fwd -o  cpt.liang-rf-zn-kn3.train  \
      -ibm_l2_given_l1  liang.train.en_given_fr.gz \
      -ibm_l1_given_l2  liang.train.fr_given_en.gz \
      jpt.liang.train.fr-en.gz \
      >& log.cpt.liang-rf-zn-kn3.train.fr2en
\end{verbatim}
There are many variants to the HMM models.  It is not clear which one is the
best, so we don't try to cover all possibilities in this framework.  You'll
need to modify \texttt{models/tm/Makefile} to select the variants you want.
The variant based on Liang et al's baseline (enabled by specifying
\texttt{-mimic liang} as above) works well.  The variant with lexically
conditioned jump parameters, following He (WMT-2006), is sometimes better
(enabled by specifying \texttt{-mimic he-lex}).

\subsection{Optimizing Decoder Weights} \label{COW}

The language and translation models are the main sources of information used
for translation, which is performed by the \texttt{canoe} decoder. In order to get
reasonable translation quality, the weights on these (and other) sources of
information need to be tuned. The tuning process is carried out by a script
called \texttt{cow.sh}, which runs \texttt{canoe} several times. \texttt{cow.sh}
requires a configuration file for \texttt{canoe}, called \texttt{canoe.ini} by
default.

\subsubsection{The Decoder Configuration File: canoe.ini}

In this framework, we start with a template configuration file,
\texttt{models/decode/ canoe.ini.template}:
\begin{verbatim}
   > cd ../..          # only needed if you were still in models/tm
   > cd models/decode
   > head -5 canoe.ini.template
   [ttable-multi-prob]
     <CPTS>
   [lmodel-file]
     <LMS>
   [weight-f] <FORWARD_WEIGHTS>
\end{verbatim}

The framework will automatically insert the phrase tables and language models
generated in the previous steps into the \texttt{canoe.ini} file when you
make it.  \texttt{make canoe.ini} will first create a soft link to the models
directory---this way, all models appear to be relative to the current directly,
which makes it easier to find them.  We'll create such symlinks everywhere we
need access to the models.  The command replaces the template parameters
\texttt{<SL>}, \texttt{<TL>}, \texttt{<TRAIN\_TM>} and, \texttt{<TRAIN\_LM>}
and \texttt{<CPTS>} by the appropriate values.  XXX document each of these
tags. XXX

The special token \texttt{<FORWARD\_WEIGHTS>} causes default forward weights inserter
into the \texttt{canoe.ini}, to be used by \texttt{canoe} and tuned by
\texttt{cow.sh}.  Unlike other weigths, which are used by default as soon as
the corresponding model is present, forward weights are disabled by default, so
the appropriate number of 1's must be inserted into the \texttt{canoe.ini} for
its \texttt{[weight-f]} parameter.  In this framework, if you don't want to use
forward weights, remove the \texttt{[weight-f]} line in
\texttt{canoe.ini.template}.

The final command, \texttt{configtool check canoe.ini}, confirms that the
\texttt{canoe.ini} file produced is error-free---it check that all parameters
are compatible and that all models can be found.
\begin{verbatim}
   > make canoe.ini
   ln -sf ../../models models
   cat canoe.ini.template \
      | sed -e 's/<SL>/fr/g' \
            -e 's/<TL>/en/g' \
            -e 's/<TRAIN_TM>/train/g' \
            -e 's/<TRAIN_LM>/train/g' \
            -e 's/<CPTS>/models\/tm\/cpt.ibm2-rf-zn-kn3.train.fr2en.gz [...] \
            -e 's/<LMS>/models\/lm\/train_en-kn-5g.binlm.gz/g' \
      > canoe.ini
   configtool check canoe.ini
   ok
   > cat canoe.ini
   [ttable-multi-prob]
     models/tm/cpt.ibm2-rf-zn-kn3.train.fr2en.gz
     models/tm/cpt.liang-rf-zn-kn3.train.fr2en.gz
   [lmodel-file]
     models/lm/train_en-kn-5g.binlm.gz
   [weight-f] 1:1:1:1:1:1
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [distortion-model] WordDisplacement
   [segmentation-model] none
   [cube-pruning]
   [bypass-marked]
\end{verbatim}
 
When using this framework, do not modify \texttt{canoe.ini}
directly.\footnote{If you're working in a different framework or manually,
you'll probably set your decoding parameters directly in your canoe.ini.}  To
change the training parameters for language models or phrase tables, modify the
parameters or Makefiles appropriately in \texttt{models/LM} and
\texttt{models/TM} and regenerate those models.  Be careful, though: the
\texttt{canoe.ini} file will include all models generated in the same
framework, so you should work in different copies of the framework if you want
to experiment with different training parameters.

To change the other decoding parameters, modify \texttt{canoe.ini.template} as
required.  For example, you can add additional language models (trained outside
the framework) by adding them in the \texttt{[lmodel-file]} section, separated
by whitespace or a newline from \texttt{<CPTS>}.

The other decoding parameters control the search strategy.  They have been set
to reasonable defaults, but may need to be adjusted depending on the
speed/quality trade-offs you're willing to make or other models you wish to
use.

\subsubsection{Running COW (Canoe Optimize Weights)}

Besides the configuration file, the other main arguments required by
\texttt{cow.sh} are a directory in which to store temporary files, a source
file and one or more correct (ie human) translations of the source
file.\footnote{This example uses only one translation, but when multiple
alternative translations are available, it can be advantageous to use them.}
Here we call the temporary directory \texttt{foos}, and use the \texttt{dev1}
files for weight tuning:
\begin{verbatim}
   > make all
   mkdir -p foos
   cow.sh -v -parallel:"-n 1" -maxiter 15 -nbest-list-size 100 \
      -filt -nofloor -workdir foos -f canoe.ini
      ../../corpora/dev1_fr.rule XXX ../../corpora/dev1_en.lc \
      >& log.canoe.ini.cow
   rm -f -r foos multi.probs.*.FILT.gz canoe.ini.FILT canoe.ini.FILT.cow
\end{verbatim}
Because \texttt{cow.sh} takes a while to run (a good two to three hours in this
example)\footnote{Running time for \texttt{cow.sh} can be reduced by executing it
in parallel. Do \texttt{cow.sh -h} to see documentation on this option; modify
\texttt{PARALLELISM\_LEVEL\_TUNE\_DECODE} in \texttt{Makefile.params} to adjust
cow parallelism in this framework.} and writes large amounts of logging
information, it is usually a good idea to redirect all of its output to a log
file, as shown here; and also to run it in the background (not done here, for
compatibility with the \texttt{make} procedure).  Progress can be monitored by
looking at the file \texttt{rescore-results}, which shows the translation quality
(measured by BLEU score---see section~\ref{Testing} for a description), as well
as the current weights, for each iteration. Note that the BLEU score does not
necessarily increase monotonically, as can be seen in several places in the
\texttt{rescore-results} file obtained with this example:
\begin{footnotesize}
\begin{verbatim}
> cat rescore-results
BLEU score: 0.136568   -d 1 -w 0 -lm 1 -tm 1:1:1:1:1:1 -ftm 1:1:1:1:1:1
BLEU score: 0.102302   -d 0.040 -w -1 -lm 0.626 -tm 0.02:0.16:-0.17:0.59:-0.11:-0.28 -ftm 0.29:0.19:-0.06:-0.32:0.71:-0.14
BLEU score: 0.022673   -d 0.062 -w -0.565 -lm 1 -tm 0.06:0.10:-0.00:-0.00:0.07:-0.00 -ftm 0.02:-0.09:-0.00:0.01:-0.00:0.02
BLEU score: 0.174290   -d 0.084 -w -0.020 -lm 1 -tm 0.13:0.08:-0.01:-0.03:0.08:-0.00 -ftm -0.01:0.00:-0.00:0.06:0.14:-0.12
BLEU score: 0.083275   -d 0.778 -w 0.415 -lm 1 -tm 0.35:-0.04:-0.01:-0.04:0.03:-0.01 -ftm 0.03:-0.03:-0.00:0.04:-0.01:0.00
BLEU score: 0.177278   -d 0.221 -w -0.012 -lm 1 -tm 0.15:0.11:-0.00:-0.00:0.01:0.00 -ftm 0.05:0.22:0.01:0.13:-0.01:-0.00
BLEU score: 0.189677   -d 0.354 -w -0.120 -lm 1 -tm 0.19:0.09:-0.01:-0.01:0.01:-0.17 -ftm 0.04:-0.00:-0.02:0.19:0.10:-0.03
BLEU score: 0.200537   -d 0.365 -w -0.304 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.00:-0.14 -ftm 0.03:0.05:-0.03:0.17:0.10:-0.03
BLEU score: 0.200639   -d 0.364 -w -0.301 -lm 1 -tm 0.15:0.12:-0.01:-0.01:0.01:-0.14 -ftm 0.03:0.05:-0.03:0.17:0.10:-0.03
BLEU score: 0.200826   -d 0.362 -w -0.299 -lm 1 -tm 0.15:0.12:-0.01:-0.01:0.00:-0.13 -ftm 0.03:0.06:-0.03:0.17:0.10:-0.03
BLEU score: 0.200826   -d 0.363 -w -0.295 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.01:-0.13 -ftm 0.03:0.06:-0.03:0.17:0.10:-0.03
BLEU score: 0.200826   -d 0.364 -w -0.296 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.01:-0.13 -ftm 0.03:0.06:-0.03:0.17:0.10:-0.03
BLEU score: 0.200826   -d 0.364 -w -0.296 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.01:-0.13 -ftm 0.03:0.06:-0.03:0.17:0.09:-0.03
BLEU score: 0.200826   -d 0.364 -w -0.294 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.01:-0.14 -ftm 0.03:0.06:-0.03:0.17:0.09:-0.03
BLEU score: 0.200826   -d 0.363 -w -0.294 -lm 1 -tm 0.15:0.11:-0.01:-0.01:0.01:-0.14 -ftm 0.03:0.06:-0.03:0.17:0.09:-0.03
\end{verbatim}
\end{footnotesize}
When the last iteration still shows an improvement (which is not the case here),
it is sometimes a sign that we should have allowed \texttt{cow.sh} to run more
iterations, by setting the \texttt{-maxiter} option to a higher value.  Here,
it looks like we achieved convergence after ten iterations.  You will notice
that there are six weights after \texttt{-tm}: three for the backward
probability estimates in the phrase table created using IBM2 models, and three
for the phrase table created using HMM models.  For the same reason, there are
six weights after \texttt{-ftm}.

While \texttt{cow.sh} is working, it is saving a lot of information in your
temporary work directory, which are automatically cleaned up afterwards in this
framework if no errors were encountered.  The large temporary files are seldom
of any use afterwards, except for troubleshooting, so there is no point in
keeping them.  If you would like to inspect them, possibly to see the n-best
lists or other files used by \texttt{cow.sh}, you can remove the \texttt{rm}
command executed after \texttt{cow.sh} in \texttt{Makefile}.

The output from \texttt{cow.sh} is written to the file \texttt{canoe.ini.cow}
(assuming the original configuration file was called \texttt{canoe.ini}). This
duplicates the contents of \texttt{canoe.ini}, but adds the optimal weights
learned on the development corpus:
\begin{verbatim}
   > cat canoe.ini.cow
[ttable-multi-prob] 
   models/tm/cpt.ibm2-rf-zn-kn3.train.fr2en.gz
   models/tm/cpt.liang-rf-zn-kn3.train.fr2en.gz
[lmodel-file] 
   models/lm/train_en-kn-5g.binlm.gz
[weight-d] 0.362472415
[weight-w] -0.2999703884
[weight-l] 1
[weight-t] 0.156554:0.120460:-0.012390:-0.018825:0.009940:-0.138515
[weight-f] 0.031522:0.061217:-0.032945:0.171901:0.101529:-0.034342
[ttable-limit] 30
[ttable-threshold] 0
[stack] 20000
[beam-threshold] 0.001
[distortion-limit] 7
[distortion-model] 
   WordDisplacement
[segmentation-model] none
[bypass-marked]
[cube-pruning]
\end{verbatim}
where the \texttt{weight-} parameters pertain to, respectively, the distortion
model, the word-length penalty, the language model, the translation models
(backward scores), and the forward scores of the translation models. The
language model usually obtains the highest weight, as in this case.\footnote{A
note of warning about the LM weight: following Doug Paul's ``DARPA'' LM format,
all LM formats we know use base-10 log probs (including our own binary LM
format), but canoe interprets them as natural logs: throughout PORTAGEshared,
logs are natural by default, not base 10.  This known bug has minimal impact;
correcting it simply requires multiplying the desired -weight-l values by
log(10), which cow, rat and rescore\_train do implicitly.  We chose not to fix
it to avoid having to adjust all previously tuned sets of weights.}

\subsection{Training a Rescoring Model} \label{RAT}

The final training step is to create a model for rescoring n-best
lists. Rescoring means having \texttt{canoe} generate a list of $n$ (typically
1000) translation hypotheses for each source sentence, then choosing the best
translations from among these. The advantage of this procedure is that the
choice can be made on the basis of information that is too expensive for
\texttt{canoe} to use during search. This step usually gives a modest
improvement over the results obtained using \texttt{canoe} alone. Sometimes,
however, it gives no significant improvement while being fairly slow, so if
translation speed is an issue, you will probably want to skip rescoring.  In
this framework, rescoring is automatically bypassed unless
\texttt{DO\_RESCORING} is set to 1 in \texttt{Makefile.params}.

Training a rescoring model involves generating n-best lists, then calculating
the values of selected \emph{features} for each hypothesis in each list. A
feature is any real-valued function that is intended to capture the relation
between a source sentence and a translation hypothesis. A rescoring model
consists of a vector of feature weights set so as to optimize translation
performance when a weighted combination of feature values is used to reorder
the n-best lists.

\subsubsection{The Input Rescoring Model}

Training is carried out by the \texttt{rat.sh} script. This takes as input a
rescoring model that specifies which features to use, and it returns optimal
weights for these features.

The default model created by this framework contains a small set of useful
features:
\begin{verbatim}
   > cd ../..          # only needed if you were still in models/tm
   > cd models/rescore
   > make rescore-model.ini
   ln -sf ../../models models
   configtool rescore-model:ffvals models/decode/canoe.ini.cow \
      | cut -f 1 -d ' ' > rescore-model.ini
   echo [...] >> rescore-model.ini
   [...]
   > cat rescore-model.ini
   FileFF:ffvals,1
   FileFF:ffvals,2
   FileFF:ffvals,3
   FileFF:ffvals,4
   FileFF:ffvals,5
   FileFF:ffvals,6
   FileFF:ffvals,7
   FileFF:ffvals,8
   FileFF:ffvals,9
   XXX
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   LengthFF
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx>
   nbestSentLenPost:1#<ffval-wts>#<pfx>
   nbestWordPostTrg:1#<ffval-wts>#<pfx>
   nbestNgramPost:1#1#<ffval-wts>#<pfx>
   nbestNgramPost:2#1#<ffval-wts>#<pfx>
\end{verbatim}
There are two kinds of features included in our rescoring model:
\begin{itemize}
\item Those that look like \texttt{FileFF:ffvals,}$i$ tell \texttt{rat.sh} to
use the $i$\/th feature generated by the \texttt{canoe} decoder itself. It is
standard practice to use all decoder features when rescoring, as is done
automatically by the framework via the \texttt{configtool} command executed
above.
\item The other features, following the format \emph{FeatureName:args}, tell
\texttt{rat.sh} to generate values for the feature \emph{FeatureName} using
arguments \emph{args}.  For example,
\texttt{IBM2TgtGivenSrc:model/tm/ibm2.train.en\_given\_fr.gz} says to calculate
the feature \texttt{IBM2TgtGivenSrc} using the IBM model
\texttt{model/tm/ibm2.train.en\_given\_fr.gz}.  which we trained earlier. To
see a list of all available features, type \texttt{rescore\_train -H}.
\end{itemize}

\subsubsection{Running RAT (Rescore And Translate)}

Apart from the rescoring model, {\tt rat.sh} needs a source file and one or
more alternative translations for it (same as {\tt cow.sh}). These may be the
same files used for {\tt cow.sh}, but it is generally better to use different
ones, so here we use {\tt dev2}:\footnote{As with \texttt{cow.sh}, you can
speed this up using parallelism, in this case via the -n option to
\texttt{rat.sh}, given before the \texttt{train} token (type \texttt{rat.sh -h}
for details), controlled in this framework via the variable
\texttt{PARALLELISM\_LEVEL\_TUNE\_RESCORE} in \texttt{Makefile.params}.}
\begin{verbatim}
   > make train
   cat models/decode/canoe.ini.cow > canoe.ini.cow.dev2
   configtool check canoe.ini.cow.dev2
   ok
   Tuning a rescoring model.
   rat.sh -lb -n 1 train -v -K 1000 -o rescore-model \
      -msrc XXX \
      -f canoe.ini.cow.dev2 rescore-model.ini \
      ../../corpora/dev2_fr.lc ../../corpora/dev2_en.lc \
      >& log.rescore-model
\end{verbatim}

The output from \texttt{rat.sh} is written to the file \texttt{rescore-model}:
\begin{verbatim}
   > cat rescore-model
   XXX
\end{verbatim}
This is identical to {\tt rescore-model.ini}, except that each feature is now
assigned a weight. Other by-products created by {\tt rat.sh} are found in the
working directory \texttt{workdir-dev2\_fr.lc-1000best} and include the nbest
lists \texttt{1000best.gz}, and the corresponding decoder features
\texttt{ffvals.gz} and additional features \texttt{ff.*}. All of these files
are compressed to save space.

\section{Translating and Testing} \label{TranslatingTesting}

\subsection{Translating} \label{Translating}

\subsection{Testing} \label{Testing}

\section{Final Note}
Because of differences in rounding, optimization, random number generation,
etc.\ between systems, results may vary on your platform and are shown in this
document only as a reference for the user.

\end{document}
