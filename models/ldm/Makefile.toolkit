# vim:noet:ts=3:nowrap
#
# @author Samuel Larkin
# @file Makefile.toolkit
# @brief Makefile targets and recipes to create Lexicalized Distortion Models.
#
# Traitement multilingue de textes / Multilingual Text Processing
# Tech. de l'information et des communications / Information and Communications Tech.
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2010, 2015, Sa Majeste la Reine du Chef du Canada
# Copyright 2010, 2015, Her Majesty in Right of Canada

LDM_DIR_PFX := $(dir $(lastword ${MAKEFILE_LIST}))

# We assume the top level Makefile.params has already been included.
include ${LDM_DIR_PFX}../Makefile.definition

# Maximum phrase length.
M ?= 8

# Define default weights.
WTU ?= 5
WTG ?= 5
WT1 ?= 5
WT2 ?= 5

# How many jobs to create when creating the counts.
PARALLELISM_LEVEL_LDM ?= 30
# How many workers to use to process those jobs.
PARALLELISM_LEVEL_LDM_WORKERS ?= ${PARALLELISM_LEVEL_LDM}
# How many cpus each workers should use.
COUNT_CPUS ?= 1

# We want a minimum of 100,000 lines per jobs that parallelize.pl creates.
MINIMUM_PARALLEL_CHUNK_SIZE ?= 100000

# Resource monitoring.
TIME_MEM ?= time-mem

# Number of cpus to use to create the final model.
ESTIM_CPUS ?= 4

# [Hierarchical] Lexicalized Distortion count file extension.
COUNTSX ?= .counts.gz

# [Hierarchical] Lexicalized Distortion count fileter command.
# cat => no filtering.
LDM_FILTER_CMD ?= cat

# Lexicalized Distortion Model file prefix.
LDM_MODEL_PFX ?= ldm

# Hierarchical Lexicalized Distortion Model file prefix.
HLDM_MODEL_PFX ?= hldm

# Define this variable if you don't want to merge your ldm counts in a fillup manner.
DONT_USE_FILLUP ?=


########################################
# FUNCTIONS.

# Adds the hierarchical lexicalized distortion model's option.
ADD_HLDM_OPTS = $(and $(findstring ${HLDM_MODEL_PFX}, $1), -hier)


########################################
# How to calculate the lexicalized counts.
# Note we are explicitely using the guard shell which will prevent sending this
# command to the cluster queue since parallelize.pl will take care of this for
# us.
# You must provide this target with:
# - a source corpus with the L1X suffix;
# - a target corpus with the L2X suffix;
# - a alignment file for each parallel corpora provided.
%${COUNTSX}: SHELL=${FRAMEWORK_SHELL}
%${COUNTSX}:
	@_LOCAL=1; [[ `expr $(words $+) % 3` -eq 0 ]] || ! echo "ERROR: You must provide triplets of corpora/alignment to create $@." >&2
	RP_PSUB_OPTS="-j 4 -${COUNT_CPUS} -N $@" \
	parallelize.pl \
		-rp "-j 4" \
		-n ${PARALLELISM_LEVEL_LDM} \
		-np ${PARALLELISM_LEVEL_LDM_WORKERS} \
		-w ${MINIMUM_PARALLEL_CHUNK_SIZE} \
		$(addprefix -s , \
			$(filter %${L1X}, $+) \
			$(filter %${L2X}, $+) \
			$(filter %align.gz, $+)) \
		-merge 'merge_multi_column_counts -' \
		'dmcount -ext -v -m ${M} $(call ADD_HLDM_OPTS, $@) \
			$(call interleave3, \
				$(filter %${L1X}, $+), \
				$(filter %${L2X}, $+), \
				$(filter %.align.gz, $+)) \
			| li-sort.sh > $@' \
	&> log.$(basename $@)


########################################
# How to create a lexicalized distortion models.
# You must provide this target:
# - one or more lexicalized distortion model counts.
sort_count_files = $(filter $1, $(foreach a,${POSSIBLE_WORD_ALIGNMENT_MODELS}, ${HLDM_MODEL_PFX}.$a${COUNTSX} ${LDM_MODEL_PFX}.$a${COUNTSX}))
%${L1_2_L2X}: SHELL=${FRAMEWORK_SHELL}
%${L1_2_L2X}:
	RP_PSUB_OPTS="-${ESTIM_CPUS}" \
	set -o pipefail; \
	merge_multi_column_counts $(if ${DONT_USE_FILLUP},,-fillup) - $(call sort_count_files, $+) \
		| ${LDM_FILTER_CMD} \
		| ${TIME_MEM} dmestm -s -g $(basename $@).bkoff -wtu ${WTU} -wtg ${WTG} -wt1 ${WT1} -wt2 ${WT2} \
	2> log.$(basename $@) \
	| gzip \
	> $@


########################################
# How to make a lexicalized distortion model a tightly packed version.
# You must provide this target:
# - a text version of the lexicalized distortion model.
%.tpldm: SHELL=${FRAMEWORK_SHELL}
%.tpldm:
	RP_PSUB_OPTS="-${ESTIM_CPUS}" \
	${TIME_MEM} textldm2tpldm.sh $< &> log.$@


################################################################################
#
# NOTE: In practice, tuning the lexicalized distortion model yields most of the
# time a similar set of weights thus we will skip tuning and use that set of
# weights which looks like 5, 5, 5, 5.
TUNE_RESULTS_DIR ?= tune.results

# What dev set to use for tuning.
TUNE_SET ?= ${TUNE_DECODE}
TUNE_SET ?= dev1

# Number of workers to use in parallel during tuning.
NUMBER_PARALLEL_WORKER ?= `wc -l < $<`

# Number of cpus per worker to use when tuning.
NUMBER_PARALLEL_CPU ?= 4

# How many samples out of 256 (4^4) we want to sample.
SAMPLE_SIZE ?= 64


########################################
# TUNE
#
# WARNING: Below is out-of-date, unmaintained code with lots of stuff hard-coded.
.PHONY: tune
tune: log.tune-dms
	@egrep -o 'ppx = [0-9\.]+' ${TUNE_RESULTS_DIR}/* \
	| sort -g -k 3,3n \
	| head -n 1

log.tune-dms: tune-dms.cmds
	run-parallel.sh -psub "-${NUMBER_PARALLEL_CPU}" $< ${NUMBER_PARALLEL_WORKER} &> $@

ldm.counts.tune.hmm1+ibm2.gz: ldm.counts.tune.hmm1.gz ldm.counts.tune.ibm2.gz
	zcat $+ | gzip > $@

ldm.counts.tune.%.gz: $(addprefix ${TUNE_SET},${L1} ${L2})
	dmcount -v -m ${M} $*.${TRAIN_TM}.${L2_GIVEN_L1X} $*.${TRAIN_TM}.${L1_GIVEN_L2X} $+ \
	| gzip > $@

tune-dms.cmds: ldm.counts.ibm2.gz ldm.counts.hmm1.gz ldm.counts.tune.hmm1+ibm2.gz
	mkdir --parents ${TUNE_RESULTS_DIR}
	for wtu in 5 10 15 20; do for wtg in 5 10 15 20; do for wt1 in 5 10 15 20; do for wt2 in 5 10 15 20; do \
		echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2 || "; \
		echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu $$wtu  -wtg $$wtg  -wt1 $$wt1  -wt2 $$wt2  > /dev/null ) &> ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2"; \
	done; done; done; done | shuf | head -${SAMPLE_SIZE} > $@
	echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20 || " >> $@
	echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu 20  -wtg 20  -wt1 20  -wt2 20  > /dev/null ) &> ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20" >> $@

