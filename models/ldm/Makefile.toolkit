# vim:noet:ts=3:nowrap
#
# @author Samuel Larkin
# @file Makefile.dm
# @brief Train Lexicalized Distortion Models.
#        How to make targets.
#
# Technologies langagieres interactives / Interactive Language Technologies
# Inst. de technologie de l'information / Institute for Information Technology
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2010, Sa Majeste la Reine du Chef du Canada
# Copyright 2010, Her Majesty in Right of Canada

ifndef SRC_LANG
$(error You must define SRC_LANG)
endif

ifndef TGT_LANG
$(error You must define TGT_LANG)
endif

# Maximum phrase length.
M ?= 8

# Define default weights.
WTU ?= 5
WTG ?= 5
WT1 ?= 5
WT2 ?= 5

# Define corpora extension.
SRCX  ?= _${SRC_LANG}${LANGX}
TGTX  ?= _${TGT_LANG}${LANGX}
SRCXZ ?= ${SRCX}${GZ}
TGTXZ ?= ${TGTX}${GZ}

# How many jobs to create when creating the counts.
PARALLELISM_LEVEL_LDM ?= 30
# How many workers to use to process those jobs.
PARALLELISM_LEVEL_LDM_WORKERS ?= ${PARALLELISM_LEVEL_LDM}
# How many cpus each workers should use.
COUNT_CPUS ?= 1

# IBM / HMM models extension
SRC_GIVEN_TGT  ?= ${SRC_LANG}_given_${TGT_LANG}
TGT_GIVEN_SRC  ?= ${TGT_LANG}_given_${SRC_LANG}
SRC_GIVEN_TGTX ?= ${SRC_GIVEN_TGT}.gz
TGT_GIVEN_SRCX ?= ${TGT_GIVEN_SRC}.gz

# conditional phrase table extension
SRC_2_TGT  ?= ${SRC_LANG}2${TGT_LANG}
TGT_2_SRC  ?= ${TGT_LANG}2${SRC_LANG}
SRC_2_TGTX ?= ${SRC_2_TGT}.gz
TGT_2_SRCX ?= ${TGT_2_SRC}.gz

IBM1_MODEL_PFX ?= ibm1
IBM4_MODEL_PFX ?= ibm4

# We want a minimum of 100,000 lines per jobs that parallelize.pl creates.
MINIMUM_PARALLEL_CHUNK_SIZE ?= 100000

# Where are the corpora?
CORPORA_DIR ?= ../../corpora

# Where are the word/phrase alignment models?
MODEL_DIRS ?= ../tm ../tm/mixtm

# Resource monitoring.
TIME_MEM ?= time-mem

# Number of cpu to use to create the final model.
ESTIM_CPU ?= 4

# [Hierarchical] Lexicalized Distortion count file extension.
COUNTSX ?= .counts.gz

# [Hierarchical] Lexicalized Distortion count fileter command.
# cat => no filtering.
LDM_FILTER_CMD ?= cat

# Lexicalized Distortion Model file prefix.
LDM_MODEL_PFX ?= ldm

# Hierarchical Lexicalized Distortion Model file prefix.
HLDM_MODEL_PFX ?= hldm

# Define this variable if you don't want to merge your ldm counts in a fillup manner.
DONT_USE_FILLUP ?=



########################################
# FUNCTIONS.
# Joins two parallel lists of words.
interleave = $(strip $(subst \, , $(join ${1}, $(addprefix \, $(join $2, $(addprefix \, $3))))))




# Where can I find the corpora.
vpath %${SRCX}  ${CORPORA_DIR}
vpath %${TGTX}  ${CORPORA_DIR}
vpath %${SRCXZ} ${CORPORA_DIR}
vpath %${TGTXZ} ${CORPORA_DIR}

# Where can we find the phrase models.
vpath %${TGT_GIVEN_SRCX} ${MODEL_DIRS}
vpath %${SRC_GIVEN_TGTX} ${MODEL_DIRS}
vpath %.align.gz  ${MODEL_DIRS}

# Where to find the filtered corpora for IBM4
vpath %/${SRC_LANG}.lc ${MODEL_DIRS}
vpath %/${TGT_LANG}.lc ${MODEL_DIRS}



# Adds the hierarchical lexicalized distortion model's option.
ADD_HLDM_OPTS = $(and $(findstring ${HLDM_MODEL_PFX}, $1), -hier)

# Fix the path of the alignment files, if necessary to pick them up from ../tm
FIX_PATHS = $(foreach f,$(filter $1,$2),$(if $(filter-out $1,$f),$f,$(if $(filter ../%,$f),$f,../tm/$f)))

########################################
# How to calculate the lexicalized counts.
# Note we are explicitely using the guard shell which will prevent sending this
# command to the cluster queue since parallelize.pl will take care of this for
# us.
# You must provide this target with:
# - a source corpus with the SRCXZ suffix;
# - a target corpus with the TGTXZ suffix;
# - a alignment file for each parallel corpora provided.
%${COUNTSX}: SHELL=${LOCAL_SHELL}
%${COUNTSX}:
	@_LOCAL=1; [[ `expr $(words $+) % 3` -eq 0 ]] || ! echo "ERROR: You must provide triplets of corpora/alignment to create $@." >&2
	parallelize.pl \
		-nolocal \
		-psub -${COUNT_CPUS} \
		-n ${PARALLELISM_LEVEL_LDM} \
		-np ${PARALLELISM_LEVEL_LDM_WORKERS} \
		-w ${MINIMUM_PARALLEL_CHUNK_SIZE} \
		$(addprefix -s , \
			$(call FIX_PATHS, %${SRCXZ}, $+) \
			$(call FIX_PATHS, %${TGTXZ}, $+) \
			$(call FIX_PATHS, %align.gz, $+)) \
		-merge 'merge_multi_column_counts -' \
		'dmcount -ext -v -m ${M} $(call ADD_HLDM_OPTS, $@) \
			$(call interleave, \
				$(call FIX_PATHS, %${SRCXZ}, $+), \
				$(call FIX_PATHS, %${TGTXZ}, $+), \
				$(call FIX_PATHS, %.align.gz, $+)) \
			| li-sort.sh > $@' \
	&> log.$(basename $@)

# This is a special rule to handle ibm4 since we need a filter corpora for IBM4.
%.lc:  SHELL=${LOCAL_SHELL}
%.lc:  
	${MAKE} -C ../tm $@

%.align.gz: SHELL=${LOCAL_SHELL}
%.align.gz:
	${MAKE} -C ../tm $@

${IBM1_MODEL_PFX}.%.gz: SHELL=${LOCAL_SHELL}
${IBM1_MODEL_PFX}.%.gz:
	${MAKE} -C ../tm $@



########################################
# How to create a lexicalized distortion models.
# You must provide this target:
# - one or more lexicalized distortion model counts.
%${SRC_2_TGTX}: SHELL=${FRAMEWORK_SHELL}
%${SRC_2_TGTX}:
	RP_PSUB_OPTS="-${ESTIM_CPU}" \
	set -o pipefail; \
	merge_multi_column_counts $(if ${DONT_USE_FILLUP},,-fillup) - $+ \
		| ${LDM_FILTER_CMD} \
		| ${TIME_MEM} dmestm -s -g $(basename $@).bkoff -wtu ${WTU} -wtg ${WTG} -wt1 ${WT1} -wt2 ${WT2} \
	2> log.$(basename $@) \
	| gzip \
	> $@



########################################
# How to make a lexicalized distortion model a tightly packed version.
# You must provide this target:
# - a text version of the lexicalized distortion model.
%.tpldm: SHELL=${FRAMEWORK_SHELL}
%.tpldm:
	RP_PSUB_OPTS="-${ESTIM_CPU}" \
	${TIME_MEM} textldm2tpldm.sh $< &> log.$@



################################################################################
# 
# NOTE: In practice, tuning the lexicalized distortion model yields most of the
# time a similar set of weights thus we will skip tuning and use that set of
# weights which looks like 5, 5, 5, 5.
TUNE_RESULTS_DIR ?= tune.results

# What dev set to use for tuning.
TUNE_SET ?= ${TUNE_DECODE}

# Number of workers to use in parallel during tuning.
NUMBER_PARALLEL_WORKER ?= `wc -l < $<`

# Number of cpus per worker to use when tuning.
NUMBER_PARALLEL_CPU ?= 4

# How many samples out of 256 (4^4) we want to sample.
SAMPLE_SIZE ?= 64


########################################
# TUNE
#
# WARNING: Below is out-of-date, unmaintained code with lots of stuff hard-coded.
.PHONY: tune
tune: log.tune-dms
	@egrep -o 'ppx = [0-9\.]+' ${TUNE_RESULTS_DIR}/* \
	| sort -g -k 3,3n \
	| head -n 1

log.tune-dms: tune-dms.cmds
	run-parallel.sh -psub "-${NUMBER_PARALLEL_CPU}" $< ${NUMBER_PARALLEL_WORKER} &> $@

ldm.counts.tune.hmm1+ibm2.gz: ldm.counts.tune.hmm1.gz ldm.counts.tune.ibm2.gz 
	zcat $+ | gzip > $@

ldm.counts.tune.%.gz: $(addprefix ${TUNE_SET},${SRCX} ${TGTX})
	dmcount -v -m ${M} $*.${TRAIN_TM}.${TGT_GIVEN_SRCX} $*.${TRAIN_TM}.${SRC_GIVEN_TGTX} $+ \
	| gzip > $@

tune-dms.cmds: ldm.counts.ibm2.gz ldm.counts.hmm1.gz ldm.counts.tune.hmm1+ibm2.gz
	mkdir -p ${TUNE_RESULTS_DIR}
	for wtu in 5 10 15 20; do for wtg in 5 10 15 20; do for wt1 in 5 10 15 20; do for wt2 in 5 10 15 20; do \
		echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2 || "; \
		echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu $$wtu  -wtg $$wtg  -wt1 $$wt1  -wt2 $$wt2  > /dev/null ) &> ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2"; \
	done; done; done; done | shuf | head -${SAMPLE_SIZE} > $@
	echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20 || " >> $@
	echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu 20  -wtg 20  -wt1 20  -wt2 20  > /dev/null ) &> ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20" >> $@

