# vim:noet:ts=3:nowrap
# $Id$
#
# @author Samuel Larkin
# @file Makefile.dm
# @brief Train Lexicalized Distortion Models.
#        How to make targets.
#
# Technologies langagieres interactives / Interactive Language Technologies
# Inst. de technologie de l'information / Institute for Information Technology
# Conseil national de recherches Canada / National Research Council Canada
# Copyright 2010, Sa Majeste la Reine du Chef du Canada
# Copyright 2010, Her Majesty in Right of Canada

ifndef SRC_LANG
$(error "You must define SRC_LANG")
endif

ifndef TGT_LANG
$(error "You must define TGT_LANG")
endif

# Define default weights.
WTU ?= 5
WTG ?= 5
WT1 ?= 5
WT2 ?= 5

# Define corpora extension.
SRCX  ?= _${SRC_LANG}${LANGX}
TGTX  ?= _${TGT_LANG}${LANGX}
SRCXZ ?= ${SRCX}${GZ}
TGTXZ ?= ${TGTX}${GZ}

# How many jobs to create when creating the counts.
COUNT_PARALLEL_LEVEL ?= 30
# How many workers to use to process those jobs.
COUNT_PARALLEL_WORKERS ?= ${COUNT_PARALLEL_LEVEL}
# How many cpus each workers should use.
COUNT_CPUS ?= 1

# IBM / HMM models extension
SRC_GIVEN_TGT  ?= ${SRC_LANG}_given_${TGT_LANG}
TGT_GIVEN_SRC  ?= ${TGT_LANG}_given_${SRC_LANG}
SRC_GIVEN_TGTX ?= ${SRC_GIVEN_TGT}.gz
TGT_GIVEN_SRCX ?= ${TGT_GIVEN_SRC}.gz

# conditional phrase table extension
SRC_2_TGT  ?= ${SRC_LANG}2${TGT_LANG}
TGT_2_SRC  ?= ${TGT_LANG}2${SRC_LANG}
SRC_2_TGTX ?= ${SRC_2_TGT}.gz
TGT_2_SRCX ?= ${TGT_2_SRC}.gz

# We want a minimum of 100,000 lines per jobs that parallelize.pl creates.
MINIMUM_PARALLEL_CHUNK_SIZE ?= 100000

# Where are the corpora?
CORPORA_DIR ?= ../../corpora

# Where are the word/phrase alignment models?
MODEL_DIR ?= ../tm

# Resource monitoring.
P_RES_MON ?= p-res-mon.sh -t

# Number of cpu to use to create the final model.
ESTIM_CPU ?= 4



# Where can I find the corpora.
vpath %${SRCX}  ${CORPORA_DIR}
vpath %${TGTX}  ${CORPORA_DIR}
vpath %${SRCXZ} ${CORPORA_DIR}
vpath %${TGTXZ} ${CORPORA_DIR}

# Where can we find the phrase models.
vpath %${TGT_GIVEN_SRCX} ${MODEL_DIR}
vpath %${SRC_GIVEN_TGTX} ${MODEL_DIR}



########################################
# How to calculate the lexicalized counts.
LDM_COUNTS_PFX ?= ldm.counts
# Note we are explicitely using the guard shell which will prevent sending this
# command to the cluster queue since parallelize.pl will take care of this for
# us.
# You must provide this target:
# - a source corpus with the SRCXZ suffix;
# - a target corpus with the TGTXZ suffix;
# - a forward word alignment model aka target given source model with the TGT_GIVEN_SRCX suffix;
# - a backward word alignment model aka source given target model with the SRC_GIVEN_TGTX suffix.
${LDM_COUNTS_PFX}%: SHELL=${GUARD_SHELL}
${LDM_COUNTS_PFX}%:
	parallelize.pl \
		-nolocal \
		-psub -${COUNT_CPUS} \
		-n ${COUNT_PARALLEL_LEVEL} \
		-np ${COUNT_PARALLEL_WORKERS} \
		-w ${MINIMUM_PARALLEL_CHUNK_SIZE} \
		-s $(filter %${SRCXZ}, $+) \
		-s $(filter %${TGTXZ}, $+) \
		'dmcount -v -m 8 $(filter %${TGT_GIVEN_SRCX}, $+) $(filter %${SRC_GIVEN_TGTX}, $+) $(filter %${SRCXZ}, $+) $(filter %${TGTXZ}, $+) > $@' \
	>& log.$(basename $@)



########################################
# How to create a lexicalized distortion models.
LDM_MODEL_PFX ?= ldm
# You must provide this target:
# - one or more lexicalized distortion model counts.
${LDM_MODEL_PFX}%${SRC_2_TGTX}: SHELL=${FRAMEWORK_SHELL}
${LDM_MODEL_PFX}%${SRC_2_TGTX}:
	RP_PSUB_OPTS="-${ESTIM_CPU}" \
	zcat -f $+ \
	| ${P_RES_MON} dmestm -s -g $(basename $@).bkoff -wtu ${WTU} -wtg ${WTG} -wt1 ${WT1} -wt2 ${WT2} \
	2> log.$(basename $@) \
	| gzip \
	> $@



########################################
# How to make a lexicalized distortion model a tightly packed version.
# You must provide this target:
# - a text version of the lexicalized distortion model.
%.tpldm: SHELL=${FRAMEWORK_SHELL}
%.tpldm:
	textldm2tpldm.sh $<



################################################################################
# 
# NOTE: In practice, tuning the lexicalized distortion model yields most of the
# time a similar set of weights thus we will skip tuning and use that set of
# weights which looks like 5, 5, 5, 5.
TUNE_RESULTS_DIR ?= tune.results

# What dev set to use for tuning.
TUNE_SET ?= ${TUNE_DECODE}

# Number of workers to use in parallel during tuning.
NUMBER_PARALLEL_WORKER ?= `wc -l < $<`

# Number of cpus per worker to use when tuning.
NUMBER_PARALLEL_CPU ?= 4

# How many samples out of 256 (4^4) we want to sample.
SAMPLE_SIZE ?= 64


########################################
# TUNE
.PHONY: tune
tune: log.tune-dms
	@egrep -o 'ppx = [0-9\.]+' ${TUNE_RESULTS_DIR}/* \
	| sort -g -k 3,3n \
	| head -n 1

log.tune-dms: tune-dms.cmds
	run-parallel.sh -psub "-${NUMBER_PARALLEL_CPU}" $< ${NUMBER_PARALLEL_WORKER} >& $@

ldm.counts.tune.hmm1+ibm2.gz: ldm.counts.tune.hmm1.gz ldm.counts.tune.ibm2.gz 
	zcat $+ | gzip > $@

ldm.counts.tune.%.gz: $(addprefix ${TUNE_SET},${SRCX} ${TGTX})
	dmcount -v -m 8 $*.${TRAIN_TM}.${TGT_GIVEN_SRCX} $*.${TRAIN_TM}.${SRC_GIVEN_TGTX} $+ \
	| gzip > $@

tune-dms.cmds: ldm.counts.ibm2.gz ldm.counts.hmm1.gz ldm.counts.tune.hmm1+ibm2.gz
	mkdir -p ${TUNE_RESULTS_DIR}
	for wtu in 5 10 15 20; do for wtg in 5 10 15 20; do for wt1 in 5 10 15 20; do for wt2 in 5 10 15 20; do \
		echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2 || "; \
		echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu $$wtu  -wtg $$wtg  -wt1 $$wt1  -wt2 $$wt2  > /dev/null ) >& ${TUNE_RESULTS_DIR}/res.tune.$$wtu.$$wtg.$$wt1.$$wt2"; \
	done; done; done; done | shuf | head -${SAMPLE_SIZE} > $@
	echo -n "test -f ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20 || " >> $@
	echo "(zcat -f $(wordlist 1,2,$+) | dmestm -eval $(word 3,$+) -wtu 20  -wtg 20  -wt1 20  -wt2 20  > /dev/null ) >& ${TUNE_RESULTS_DIR}/res.tune.20.20.20.20" >> $@

