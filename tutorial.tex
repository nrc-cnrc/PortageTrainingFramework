\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper, top=1in, bottom=1in, left=1.25in, right=1.25in]{geometry}
\usepackage{isolatin1}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{url}
\usepackage{rotating}

% Need straight quotes in verbatim text.
\usepackage{upquote}
\usepackage{textcomp}
\newcommand\upquote[1]{\textquotesingle#1\textquotesingle}
\usepackage{alltt}
% This is needed in order to get bold in tt.
\renewcommand{\ttdefault}{txtt}
\newcommand{\bs}{\textbackslash{}}

% \code formats an inline code snippet without line breaking; it treats the
% underscore as a normal character. Use \url to format a code snippet with
% automatic line breaking, but then underscores are not rendered as characters
% on which copy/paste works.
% \code breaks for text containing underscores when used inside a footnote;
% for \code calls inside footnotes, use \us{} to specify an underscore.
% Use \upquote{quote-text} for straight single quotes around text within a \code
% call.
\def\code{\begingroup\catcode`\_=12 \codex}
\newcommand{\codex}[1]{\texttt{#1}\endgroup}
\chardef\us=`\_

\newcommand{\phs}{\tild{s}}   % source phrase
\newcommand{\pht}{\tild{t}}   % target phrase
% Official typesetting of PORTAGEshared, now called Portage 1.x
\newcommand{\PS}{Portage 1.4\xspace}
\newcommand{\tip}{\textbf{Useful Tip \large{\ding{43}} }}
\newcommand{\margintip}{\marginpar[{\textbf{Tip \large{\ding{43}}}}]{\textbf{\reflectbox{\large{\ding{43}}} Tip}}}
\newcommand{\tipsummary}{\noindent\textbf{Tip summary \large{\ding{43}} }}
\newcommand{\tipend}{\textbf{ \reflectbox{\large{\ding{43}}}}}

\usepackage{ifpdf}
\ifpdf
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\else
\fi

\title{\PS Tutorial: \\
       A toy experiment using the \\
       experimental framework}
\date{September, 2011}
\author{Eric Joanis, Darlene Stewart, Samuel Larkin, George Foster}

\begin{document}

\vfill

\maketitle

\vfill

\begin{center}
An adaptation of George Foster's \emph{Running Portage: A Toy Example} \\
to Samuel Larkin's experimental framework,\\
%updated to reflect recommended usage of \PS.
updated to reflect recommended usage of Portage 1.4.3.
\end{center}

\vfill
\vfill

\begin{center}
{~} \\ \footnotesize
   Technologies langagi{\`e}res interactives /
      Interactive Language Technologies \\
   Institut de technologie de l'information /
      Institute for Information Technology \\
   Conseil national de recherches Canada /
      National Research Council Canada \\
   Copyright \copyright\ 2008--2011, Sa Majest{\'e} la Reine du Chef du Canada
   \\ Copyright \copyright\ 2008--2011, Her Majesty in Right of Canada
\end{center}

\vfill

\newpage

\vfill

\tableofcontents

\vfill

\newpage


\section{Introduction}

This document describes how to run an experiment from end to end using the \PS
experimental framework. It is intended as a tutorial on using \PS, as well as a
starting point for further experiments.  Although the framework automates most
of the steps described below, we go through them one by one here in order to
better explain how to use the \PS software suite.

\PS can be viewed as a set of programs for turning a bilingual corpus into a
translation system. Here this process is illustrated with a small ``toy''
example of French to English translation, using text from the Hansard corpus.
The training corpus is too small for good translation, but large enough to give
the flavour of a more realistic setup. Running time is several hours.

\subsection{Making Sure \PS is Installed}

To begin, you must build or obtain the \PS executables and ensure that they are
in your path, typically by sourcing the \code{SETUP.bash} file as
customized for your environment during installation of \PS.\footnote{There is
  also a \code{SETUP.tcsh} for users of that shell. The examples in
  this document assume the use of bash.} You should also
set your environment variable \code{\$PORTAGE} to the directory where \PS is
installed, which is also done by \code{SETUP.bash}.  Follow the
instructions in \code{INSTALL} before you proceed with this document.

To make sure \PS is installed properly, try \code{canoe -h}, \code{cow.sh
-h}, \code{tokenize.pl -h} and \code{ce.pl -h}.  If you get usage
information for each of these programs, you should be ready to proceed.  If one
of them gives you an error message, then some part of your installation is
incomplete.  See the section \emph{Verifying your installation} in
\code{INSTALL} for troubleshooting suggestions.

\subsection{Running the Toy Experiment}

Once \PS is installed, you should make a complete copy of the framework
directory hierarchy, because it is designed to work in place, creating the
models within the hierarchy itself.  The philosophy of the framework is that
each experiment is done in a separate copy, where you might do various
customizations depending on what each experiment is intended to test.

For example:
\begin{small}
\begin{alltt}
   > \textbf{cd $PORTAGE}
   > \textbf{cp -pr framework toy.experiment}
   > \textbf{cd toy.experiment}
\end{alltt}
\end{small}

All commands provided in the rest of this document assume they are being run in
the \code{toy.experiment} directory or in a subdirectory thereof, which will
be specified relative to \code{toy.experiment}. Similary, whenever we quote a
\code{cd} command, it will be assumed to be executed from this
\code{toy.experiment} directory, not from the location of the previous
commands.

As you work through the example, the commands that you should type\footnote{You
can copy and paste commands from this PDF document onto the command line in
your interactive shell if you wish.} are shown in bold and preceded by a
prompt, \code{>}, and the system's response is not, though system output is not
usually fully reproduced here, for brevity's sake. When it is, results
(especially numbers) may vary a little from the ones shown, due to platform
differences. Note that many results presented here are truncated in precision
for presentation purposes.

Many of the commands are expressed as \code{make} targets. This has the
advantage of requiring less typing, while still allowing you to see the actual
commands executed by the system because they are always echoed by \code{make}.
(You could also type them directly.) \code{make} also lets you skip
sections in this document (except for the first one, since it is done
manually). For example, if you are not interested in any steps before decoder
weight optimization, you can go directly to section~\ref{COW} and type
\code{make tune} to begin at that point. \code{make} will automatically run all
the commands required from previous sections before doing the step you
specifically requested. Here are some other useful make commands:
\begin{itemize}
\item \code{make all}: run all remaining steps at any point.
\item \code{make clean}: clean up and return the directory to its initial state
\item \code{make -j} \emph{target} or \code{make -j N} \emph{target}: build
      \emph{target} by running commands in parallel whenever possible (up to
      N ways parallel if N is specified). This lets you take advantage of
      multi-core capabilities of your machine, but use with caution: many
      commands in the framework are already internally parallelized, as
      discussed in section~\ref{FrameworkParams}.
\item \code{make help} display some help and the main targets available in
      the makefile.
\item \code{make summary} display the resources used by the framework: time and
      memory used, as well as disk space for the runtime models (most
      informative once training has been completed; discussed further in
      section~\ref{FrameworkParams}).
\end{itemize}

\subsection{Overview of the Process}

Here is an overview of the \PS process, as described in the following
sections (section numbers are in brackets):
\begin{enumerate}
\item Corpus preparation (\S\ref{CorpusPreparation}): includes tokenization,
      alignment, lowercasing (\S\ref{Lowercasing}), and splitting
      (\S\ref{Splitting}).
\item Model training (\S\ref{Training}): includes language model (\S\ref{LM}),
      translation model (\S\ref{TM}), lexicalized distortion mode (\S\ref{LDM}),
      truecasing model (\S\ref{TC}), decoder weight optimization (\S\ref{COW}),
      confidence estimation model training (\S\ref{CE}), and rescoring model
      training (\S\ref{RAT}).
\item Translating and testing (\S\ref{TranslatingTesting}): includes
      decoding (\S\ref{Decoding}), confidence estimation(\S\ref{CETrans}),
      rescoring (\S\ref{RATTrans}), truecasing (\S\ref{Truecasing}) and testing
      (\S\ref{Testing}).
\end{enumerate}
These steps are fairly standard, but of course there are many variants on the
sample process illustrated here. For more information, see the \PS
user manual (found in \url{$PORTAGE/doc/user-manual.html} on the CD).
For detailed information about a particular program, run the program with the
\code{-h} flag (or see \url{$PORTAGE/doc/usage.html} on the CD).

\section{Corpus Preparation} \label{CorpusPreparation}

Corpus preparation involves converting raw document pairs into tokenized,
sentence-aligned files in the format expected by Portage.  We provide a
tokenizer, a sentence aligner and sample corpus pre-processing scripts with
\PS, which can help you with these steps.  We also provide a tool to extract
text from a translation memory in TMX format.  For details, see section
\emph{Text Processing} in the user manual.

We don't actually perform any of the steps mentioned above in this toy example
because they are highly dependent on your actual data.  You should plan to
invest some time in preprocessing your data well if you want to obtain good
results with \PS.

\subsection{Splitting the Corpus} \label{Splitting}

The corpus, which we assume you have tokenized and sentence-aligned, as
discussed above, must be split into separate portions in order to run
experiments. Distinct, non-overlapping sub-corpora are required for model
training (see \S\ref{Training}), for tuning decoder weights
(\S\ref{COW}) and rescoring weights (\S\ref{RAT}), for confidence
estimation (if you use it; \S\ref{CE}), and for testing
(\S\ref{Testing}).\footnote{In this example, we use separate corpora for
tuning decoder and rescoring weights, but this is not necessary.  However,
confidence estimation must absolutely have its own separate tuning set.}
Typically, the tuning and testing corpora are around 1000 segments each.  If
the corpus is chronological, then it is a good idea to choose these corpora
from the most recent material, which is likely to resemble future text more
closely.

Since proper splitting of a corpus must take into account its structure
and nature, these steps are not handled by the experimental framework. For the
toy experiment, we provide small data sets that can be found here:
\url{$PORTAGE/test-suite/tutorial-data}. These sets
are very small, to minimize running time, so the resulting translations are of
poor quality. In this toy experiment, the tuning and testing corpora contain
just 100 segments each.

To drop these sets into the framework, simply copy them (or make symbolic
links) into your copy's corpora directory:
\begin{small}
\begin{alltt}
   > \textbf{cp $PORTAGE/test-suite/tutorial-data/*.al* corpora/}
   > \textbf{wc corpora/*.al}
      100   2114  11589 corpora/dev1_en.al
      100   2330  13680 corpora/dev1_fr.al
      100   2095  11462 corpora/dev2_en.al
      100   2443  13905 corpora/dev2_fr.al
      100   2383  13061 corpora/devCE_en.al
      100   2840  16209 corpora/devCE_fr.al
      100   2379  13292 corpora/test_en.al
      100   2616  15622 corpora/test_fr.al
      800  19201 108820 total
   > \textbf{zcat train_en.al.gz | wc}
      8892  182186  981455
   > \textbf{zcat train_fr.al.gz | wc}
      8892  208400 1178176
\end{alltt}
\end{small}

In your own experiments, the files you need to copy into \code{corpora}
should be plain text in original truecase (if you're using truecasing),
tokenized, sentence-split and aligned, just like the ones we provide here.

Although the framework does not support compressed dev and test files, the
training files can and should be compressed, as shown here.  Most programs and
modules in Portage transparently handle compressed files, compressing and
decompressing them on the fly as needed.

\subsection{Setting Framework Parameters} \label{FrameworkParams}

Now you need to edit \code{Makefile.params} to set some global parameters:
\begin{itemize}
\item swap the values of \code{SRC_LANG} and
\code{TGT_LANG}, to select translation from French to English, rather than
the other way around, which is the default;
\item set \code{TRAIN_LM} and \code{TRAIN_TM} to
\code{train} instead of \code{lm-train} and \code{tm-train} since we
won't be using a separate corpora to train the translation and language models;
\item set \code{TUNE_CE} to \code{devCE} (also, remember to uncomment the
line by removing the \code{\#} at the start);
\item set \code{TEST_SET} to \code{test} instead of the default
  \code{test1 test2} because we have only one test set;
\item select a language modeling option (see below for more info about this
  choice):
\begin{itemize}
\item to use SRILM, make sure the \code{LM_TOOLKIT} variable is set to
\code{SRI};
\item to use MITLM, set the \code{LM_TOOLKIT} variable to \code{MIT}.
\end{itemize}
%\item set \code{DO_RESCORING} to \code{1} by uncommenting the relevant line.
\end{itemize}
In this toy experiment, we will use the default value for all other parameters.

While in \code{Makefile.params}, you should read through all the variables in
the \emph{User definable variables} section of the file.  This is where most of
the configurable behaviours are set, such as whether to do rescoring and/or
truecasing, what LM toolkit you are using (if any), levels of parallelism, etc.

We recommend you use SRILM (\url{http://www.speech.sri.com/projects/srilm/}) as
your language modeling toolkit, if your licensing requirements permit it; if
not, we recommend MITLM (\url{http://code.google.com/p/mitlm/}), another
excellent LM toolkit, which allows commercial use.  Although not recommended,
IRSTLM remains another option.  See \code{Makefile.params} and type \code{make
help} for more details.

Another set of parameters you might want to adjust are the various
\code{PARALLELISM_LEVEL_*} variables.  \PS is written in such a way as to
take advantage of multi-processor computers and/or multi-node computing
clusters, doing tasks in parellel where possible.  If you're not running on a
cluster, the number of CPUs on your machine is a good choice for all five
variables (this is the default).  If you are running on a cluster, the
framework uses \code{qsub} to submit jobs, via the \code{run-parallel.sh}
and \code{psub} scripts, and you can set the five variables according to
resources available to you.

When running this framework, you might notice that most commands are preceded
by control variables \code{RP_PSUB_OPTS=...} or \code{_LOCAL=1}.  These
strings only have an impact when running on a cluster, and are written in such
a way that they are ignored otherwise.  When working on a cluster, commands
preceded by \code{_LOCAL=1} are inexpensive ones that get
run directly instead of being submitted to the queueing system, while
\code{RP_PSUB_OPTS=...} is used to specify additional options to
\code{psub}, which is used in \PS to encapsulate the invocation of
\code{qsub}.  If your cluster has specific usage rules or if you require
additional parameters to \code{qsub}, you might want to customize
\code{psub} itself or add options as required in this framework.

\tip\margintip Many commands run in this framework will also be preceded by
\code{time-mem}. This utility script measures the time taken by a command, just
like the standard \code{time} utility, as well as memory usage. At any time
while things are running, or afterwards, you can type \code{make summary} to
get a summary of resources used by all components of the framework. While things
are running, you may get some error messages, but these are safe to ignore: the
report should still reflect the situation so far.  The output of \code{make
time-mem} can be very useful to determine which steps are taking the most time
and/or the most memory.  They can help you determine if you have enough
computing resources to process your corpora.  They can also help you determine
the cost of various choices you can make in Portage.  You can also type
\code{make summary} to get the \code{time-mem} information as well as the
space on disk of the models needed at runtime, such as would be deployed on a
translation server.\tipend

For the sake of brevity, when we quote executed commands in this document, we
only show the command itself, leaving out \code{time-mem} and the control
variables mentioned above.

\subsection{Lowercasing and Adding Escapes} \label{Lowercasing}

To reduce data sparseness, we convert all files to lowercase.\footnote{We show
\code{utf8\_casemap} being used.  If you installed Portage without ICU, you
will see \code{lc-utf8.pl} instead.} We keep the lowercase and truecase
versions separately, because we'll use the truecase version to train a
truecasing model.
\begin{small}
\begin{alltt}
   > \textbf{cd corpora}
   > \textbf{make lc}
   cat dev1_fr.al | utf8_casemap -c l > dev1_fr.lc
   [...]
   zcat train_en.al.gz | utf8_casemap -c l | gzip > train_en.lc.gz
\end{alltt}
\end{small}

The decoder, \code{canoe}, treats \code{<}, \code{>} and \verb*X\X as
special characters in order to support markup for special translation rules.
We won't use any markup in this example, but we still need to escape any
occurrences of the three special characters (in the source language only, since
this is only required for the input to \code{canoe}).

\begin{small}
\begin{alltt}
   > \textbf{make rule}
   canoe-escapes.pl -add dev1_fr.lc > dev1_fr.rule
   canoe-escapes.pl -add dev2_fr.lc > dev2_fr.rule
   canoe-escapes.pl -add devCE_fr.lc > devCE_fr.rule
   canoe-escapes.pl -add test_fr.lc > test_fr.rule
\end{alltt}
\end{small}


\section{Training} \label{Training}

This step creates various models and parameter files that are required for
translation. There are six steps in training.  Three are mandatory: creating a
language model (\S\ref{LM}), creating a translation model (\S\ref{TM}), and
optimizing decoder weights (\S\ref{COW}).  Four are optional: creating a
truecasing model (\S\ref{TC}), creating a lexicalized distortion model
(\S\ref{LDM}), training a rescoring model (\S\ref{RAT}), and training a
confidence estimation (CE) model (\S\ref{CE}).

\subsection{Creating a Language Model} \label{LM}

Portage does not come with programs for language model training. However, it
accepts models in the widely-used ``ARPA'' format which is produced by
toolkits such as SRILM or MITLM. The following sections describe the training
process for SRILM and MITLM.\footnote{In this toy example, we use only the
target language part of the parallel training corpus to train the language
model, but this is not the recommended practice.  If you have access to larger
corpora of monolingual text in your target language, you can use them to train
additional language models.  The framework does not support training multiple
language models, but you can train them externally and add them to the
\code{canoe.ini.template} file (see \S\ref{COW}).} Commands are assumed to be
issued from the \code{toy.experiment} directory (so run \code{cd ..} from the
corpora directory if you have not already done so).

\subsubsection{SRILM}

If you are using the SRILM toolkit, here is the command used to produce this
model:
\begin{small}
\begin{alltt}
   > \textbf{make lm}
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format train_en-kn-5g.lm.gz
   ngram-count -interpolate -kndiscount -order 5 \bs
      -text ../../corpora/train_en.lc.gz -lm train_en-kn-5g.lm.gz \bs
      >& log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \bs
      >& log.train_en-kn-5g.binlm
\end{alltt}
\end{small}
The first command executed, \code{ngram-count}, produces the language model
itself in ``ARPA'' format.  The second command, \code{arpalm2binlm},
converts it into the Portage binary language model format for fast loading.


\subsubsection{MITLM}

If you are using the MITLM toolkit, here is the command used to produce the
model needed for this example, \code{train_en-kn-5g.binlm.gz}:
\begin{small}
\begin{alltt}
   > \textbf{make lm}
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format train_en-kn-5g.lm.gz
   zcat ../../corpora/train_en.lc.gz \bs
      | perl -ple 's/^{\bs}s*(.*?){\bs}s*$/$1/; s/{\bs}s+/ /g;' \bs
      | estimate-ngram -order 5 -smoothing ModKN -text /dev/stdin \bs
        -write-lm train_en-kn-5g.lm.gz 2> log.train_en-kn-5g.lm
   arpalm2binlm train_en-kn-5g.lm.gz train_en-kn-5g.binlm.gz \bs
      >& log.train_en-kn-5g.binlm
\end{alltt}
\end{small}
The first command executed, \code{estimate-ngram}, produces the language model
itself in ``ARPA'' format.  The \code{perl} command that precedes it removes
extraneous whitespace, because \code{estimate-ngram} is picky about its input.
The second command, \code{arpalm2binlm}, converts it into the Portage binary
language model format for fast loading.


\subsection{Creating a Truecasing Model} \label{TC}

Decoding is done in lowercase to reduce data sparseness, but at the end of the
translation process, you will probably want to restore proper mixed case to
your output.  We call this step truecasing.  To train a truecasing model, you
need both a lowercased version and the original ``truecase'' version of the
training corpus in the target language.

The basic truecasing model consists of two different models: a ``casemap'',
which maps each lower case words to its possible truecase variants, as observed
in the training corpus, and a standard language model trained on the truecase
corpus.

With version 1.4.3, we have introduced a new variant of truecasing which
carries casing information from the source sentence to the target sentence,
including the casing for the first word in the sentence, the casing of OOVs
(out-of-vocabulary words), and unusual casing for sequences of several words
(such as all caps).

Truecasing with source information is now the default.\footnote{If you want to
use basic truecasing, as in previous versions of Portage, comment out the line
\code{TC\_USE\_SRC\_MODELS=1} in the advanced configuration section of
\code{Makefile.params}.}  It requires three language models: source-side and
target-side case-normalizing\footnote{``Normalized case'' means that the first
character of the sentence is in lowercase unless the word inherently requires
the upper case.  E.g., ``his flat in is London.'' and ``London is on the
Thames.'' are in normalized case.  The acronym ``nc1'', used in some file
names, means ``normalized-cased first word''.} language models, as well as the
main truecasing LM.  The target-side case-normalizing LM is used only for
generating a normalized-case target-side training corpus; it is not used in
translation.  The main truecasing LM and the ``casemap'' are trained on the
normalized-case target-side training corpus.  Together, they are used to
determine the case for the output of the decoder.  Then additional case
information from the source sentence, including the first word, is transfered
to that truecased output of the decoder.

Running \code{make tc} at the root of the framework will build all necessary
files and models.  Breaking with the convention in rest of this document, we'll
interleave the commands executed by make and our comments about them, to
improve readability.

\begin{small}
\begin{alltt}
   > \textbf{make tc}
   make -C models tc
   make -C tc all
   zcat -f ../../corpora/train_en.tc.gz \bs
        | perl -ne 'use encoding "UTF-8"; s/^[^[:lower:]]+$/{\bs}n/; print $_ unless /^$/;' \bs
        | utokenize.pl -pretok -paraline -ss -p -lang=en 2> log.train_en.tokss \bs
        | gzip > train_en.tokss.gz
\end{alltt}
\end{small}

We first generate \code{train_en.tokss.gz}, a variant of the target-side
training corpus with all-caps sentences (which are misleading for training
truecasing models) removed and sentence splitting re-done.  Sentence splitting
is re-applied because corpus text from some sources such as TMX files may
contain lines with multiple sentences, and beginning-of-sentence detection is
important for case normalization.

\begin{small}
\begin{alltt}
   zcat -f train_en.tokss.gz | filter-nc1.py -enc UTF-8 \bs
        | reverse.pl | gzip > train_en.revtokss.gz
\end{alltt}
\end{small}

Second, we generate \code{train_en.revtokss.gz}, a permutation of the target-side
training corpus needed to train the target-side case-normalizing LM.

\begin{small}
\begin{alltt}
   make -f ../lm/Makefile CORPUS_EXT=.revtokss.gz CORPORA_DIR=../tc LM_DESC=.nc1 \bs
        train_en.nc1.binlm.gz
   [commands to train case-normalizing target-language LM train_en.nc1.binlm.gz]
\end{alltt}
\end{small}

Third, we internally re-invoke \code{make} to train a language model (as in
section~\ref{LM}) on this permuted corpus, producing \code{train_en.binlm.gz},
the target-side case-normalizing language model.

\begin{small}
\begin{alltt}
   normc1 -ignore 1 -extended -notitle -loc en_CA.UTF-8 train_en.nc1.binlm.gz \bs
        train_en.tokss.gz 2> log.train_en.nc1 \bs
        | perl -pe 's/(.)$/$1 /; s/(.){\bs}n/$1/' | gzip > train_en.nc1.gz
\end{alltt}
\end{small}

Next, \code{normc1} uses the case-normalizing LM to produce
\code{train_en.nc1.gz}, the normalized-case target-side training corpus.

\begin{small}
\begin{alltt}
   zcat -f train_en.nc1.gz |  utf8_casemap -c l \bs
        | compile_truecase_map train_en.nc1.gz - 2> log.train_en.map > train_en.map
\end{alltt}
\end{small}

Here we generate the casemap for the main target-side truecasing model,
\code{train_en.map}, using \code{compile_truecase_map}, which compiles the
casemap by processing the normalized-case and lowercase versions of the corpus
simultaneously.

\begin{small}
\begin{alltt}
   make -f ../lm/Makefile CORPUS_EXT=.nc1.gz CORPORA_DIR=../tc train_en-kn-3g.binlm.gz
   [commands to train truecasing LM train_en-kn-3g.binlm.gz]
\end{alltt}
\end{small}

Then, we invoke \code{make} again, this time to produce
\code{train_en-kn-3g.binlm.gz}, the main target-size truecasing LM, trained on
the normalized-case corpus.

\begin{small}
\begin{alltt}
   zcat -f ../../corpora/train_fr.al.gz \bs
        | utokenize.pl -pretok -paraline -ss -lang=fr 2> log.train_fr.tokss \bs
        | perl -pe 'use encoding "UTF-8"; s/^[^[:lower:]]+($|( : ))//;' \bs
        | gzip > train_fr.tokss.gz
   zcat -f train_fr.tokss.gz | filter-nc1.py -enc UTF-8 \bs
        | reverse.pl | gzip > train_fr.revtokss.gz
   make -f ../lm/Makefile CORPUS_EXT=.revtokss.gz CORPORA_DIR=../tc LM_DESC=.nc1 \bs
        LM_LANG=fr train_fr.nc1.binlm.gz
   [commands to train case-normalizing source-language LM train_fr.nc1.binlm.gz]
\end{alltt}
\end{small}

The final three commands do the same thing as the first three, this time to
produce \code{train_fr.nc1.binlm.gz}, the source-side case-normalizing language
model needed by the new truecasing workflow.

\subsection{Creating a Translation Model} \label{TM}

Creating a translation model involves two main steps: 1) training IBM and HMM word
alignment models in both directions, then 2) using them to extract phrase pairs
from the corpus.  We illustrate the use of two separate phrase tables: one
from IBM2 word alignments, and one from HMM word alignments, whose counts we
merge afterwards (we recommended doing the same for your experiments as well).
You can do all this by typing \code{make tm} in your \code{toy.experiment}
directory, but we will break it down into several steps here.

\subsubsection{Creating a Translation Model Using IBM2 Alignments} \label{IBM2}

\subsubsection*{Training IBM2 Models}

First we train IBM2 word alignment models, which requires training IBM1 models
as a prerequisite.  We do this for both directions.
\begin{small}
\begin{alltt}
   > \textbf{cd models/tm}
   > \textbf{make ibm2_model}
   cat.sh -n 4 -pn 4 -v -n1 5 -n2 0 -bin ibm1.train.en_given_fr.gz
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.ibm1.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 5 -n2 0 -bin ibm1.train.fr_given_en.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.ibm1.train.fr_given_en
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \bs
      -bin -i ibm1.train.en_given_fr.gz ibm2.train.en_given_fr.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.ibm2.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \bs
      -bin -i ibm1.train.fr_given_en.gz ibm2.train.fr_given_en.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.ibm2.train.fr_given_en
\end{alltt}
\end{small}
The \code{train_ibm} program could have been used directly, but we use
\code{cat.sh} instead, which runs \code{train_ibm} in parallel. Here the
options \code{-n 4 -pn 4} mean 4-ways parallel: the default is the use the
number of CPUs available, as controlled by the \code{PARALLELISM_LEVEL_TM}
variable in \code{Makefile.params}.

The commands above write log files containing information about training.  For
example, the IBM log files show convergence and pruning statistics for each
iteration:
\begin{small}
\begin{alltt}
   > \textbf{grep -h ppx log.ibm[12]*en_given_fr}
   Parallel iter (IBM1): prev ppx = 356.599, size = 1461411 word pairs.
   Parallel iter (IBM1): prev ppx = 109.263, size = 1350718 word pairs.
   Parallel iter (IBM1): prev ppx = 66.443, size = 1182917 word pairs.
   Parallel iter (IBM1): prev ppx = 52.8037, size = 1016137 word pairs.
   Parallel iter (IBM1): prev ppx = 47.908, size = 874686 word pairs.
   Parallel iter (IBM2): prev ppx = 45.755, size = 874415 word pairs.
   Parallel iter (IBM2): prev ppx = 29.4005, size = 874073 word pairs.
   Parallel iter (IBM2): prev ppx = 23.4925, size = 849903 word pairs.
   Parallel iter (IBM2): prev ppx = 20.8623, size = 741721 word pairs.
   Parallel iter (IBM2): prev ppx = 19.5686, size = 617726 word pairs.
\end{alltt}
\end{small}

The log files also contain time and memory resource information, which you can
summarize at any level just as you can globally:
\begin{footnotesize}
\begin{alltt}
   > \textbf{make time-mem}
   find -type f -name log.\* | sort | xargs time-mem-tally-pl -no-dir -m tm \bs
      | second-to-hms.pl | expand-auto.pl
      log.ibm1.train.en_given_fr:TIME-MEM  WALL TIME: 1m3s   CPU TIME: 28s   VSZ: 0.102G  RSS: 0.012G
      log.ibm1.train.fr_given_en:TIME-MEM  WALL TIME: 1m0s   CPU TIME: 28s   VSZ: 0.099G  RSS: 0.013G
      log.ibm2.train.en_given_fr:TIME-MEM  WALL TIME: 57s    CPU TIME: 24s   VSZ: 0.099G  RSS: 0.012G
      log.ibm2.train.fr_given_en:TIME-MEM  WALL TIME: 58s    CPU TIME: 24s   VSZ: 0.099G  RSS: 0.014G
   tm:TIME-MEM                             WALL TIME: 3m58s  CPU TIME: 1m44s VSZ: 0.102G  RSS: 0.014G
\end{alltt}
\end{footnotesize}

The IBM models are written to files \code{ibm[12].*} and contain word
translation/alignment probabilities.

\subsubsection*{Training Joint-Count Phrase Tables}

Now we extract the joint-count phrase table from the same parallel corpus,
using IBM2 word alignment models in both directions together.
\begin{small}
\begin{alltt}
   > \textbf{make ibm2_jpt}
   gen-jpt-parallel.sh -n 4 -nw 4 -w 1 -o jpt.ibm2.train.fr-en.gz \bs
      GPT -v -m 8 -ibm 2 -1 fr -2 en \bs
      ibm2.train.en_given_fr.gz ibm2.train.fr_given_en.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.jpt.ibm2.train.fr-en
\end{alltt}
\end{small}
The joint-count phrase table contains the co-occurrence frequency for
each phrase pair observed in the parallel corpus.

\subsubsection*{Training Conditional Phrase Tables}

We used to train conditional phrase tables separately from the IBM2 and the HMM
joint-count phrase tables, but now we merge them first, so we'll come back to
this step after doing the HMM models.

\subsubsection{Creating a Translation Model Using HMM Alignments} \label{HMM}

Now we repeat the same steps using HMM word alignment models, using the
``hmm3'' variant, which is the default.  This variant corresponds to the HMM
parameter settings we recommend using as a starting point.
\begin{small}
\begin{alltt}
   > \textbf{make hmm3_jpt}
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -hmm -end-dist -anchor \bs
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \bs
      -i ibm1.train.en_given_fr.gz hmm3.train.en_given_fr.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.hmm3.train.en_given_fr
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -hmm -end-dist -anchor \bs
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \bs
      -i ibm1.train.fr_given_en.gz hmm3.train.fr_given_en.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.hmm3.train.fr_given_en
   gen-jpt-parallel.sh -n 4 -nw 4 -w 1 -o jpt.hmm3.train.fr-en.gz \bs
      GPT -v -m 8 -hmm -1 fr -2 en \bs
      hmm3.train.en_given_fr.gz hmm3.train.fr_given_en.gz \bs
      ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
      >& log.jpt.hmm3.train.fr-en
\end{alltt}
\end{small}

There are many variants of the HMM models.  In this framework, we've included
recipes to produce three of them: one with lexically conditioned jump
parameters, following He (WMT-2006), invoked via the \code{hmm1_*} targets;
one similar to the baseline described in Liang et al (ACL 2006), invoked via
the \code{hmm2_*} targets; and one tuned in-house to maximize BLEU scores on
French-English material, invoked via the \code{hmm3_*} targets.  Many other
variants are available, as documented by \code{train_ibm -h}. To select one or
more of them, you'll need to modify \url{models/tm/Makefile} and
\url{models/tm/Makefile.toolkit}. It's probably easier to modify the commands
associated with the \code{hmm1_model}, \code{hmm2_model} or \code{hmm3_model}
targets than to create new targets.

To modify the default models generated, change the \code{PT_TYPES} variable
in \url{toy.experiment/Makefile.params} to, for instance, \code{ibm2_cpt
hmm1_cpt}: this variable enumerates which models you want to generate when you
type \code{make all}. All models you generate will be used in subsequent
steps, so generate only the ones you intend to use. (You should work in
different copies of the framework if you want to experiment with different
variants.)

\subsubsection{Creating a Conditional Phrase Table}

Now that we have joint-count phrase tables from our IBM2 and HMM3
word-alignment models, we merge their counts together in a single joint-count
phrase table.
\begin{small}
\begin{alltt}
   > \textbf{make jpt.merged.train.fr-en.gz}
   merge_counts jpt.merged.train.fr-en.gz \bs
      jpt.ibm2.train.fr-en.gz jpt.hmm3.train.fr-en.gz
\end{alltt}
\end{small}

Finally we compute the conditional phrase table, using two different
smoo\-thers to calculate the probabilities.\footnote{In general, we often obtain
better translation quality when using phrase probabilities estimated from
relative frequencies (``RFSmoother'') along with probability estimates
calculated using Zens-Ney lexical smoothing (``ZNSmoother''). Other smoothing
options are also available. Recent results have shown that using Kneser-Ney
smoothing with Zens-Ney smoothing, without the relative frequency estimate,
gives the best results in a wide range of cases.  It is probably worth trying
this combination with your data.  If you edit \code{models/tm/Makefile.params},
you will find an example that does just that: look for the string
\code{-kn3-zn} and uncomment the two lines there.}
Although stored together in the same phrase table, they are separate probability
models, whose relative weights will be tuned during decoder weight optimization
(see \S\ref{COW}).
\begin{small}
\begin{alltt}
   > \textbf{make merged_cpt}
   joint2cond_phrase_tables -prune1 100 -v -i -z -hmm -1 fr -2 en \bs
      -s RFSmoother -s ZNSmoother -multipr fwd -o cpt.merged.hmm3-rf-zn.train \bs
      -ibm_l2_given_l1 hmm3.train.en_given_fr.gz \bs
      -ibm_l1_given_l2 hmm3.train.fr_given_en.gz \bs
      -reduce-mem -no-sort jpt.merged.train.fr-en.gz \bs
      >& log.cpt.merged.hmm3-rf-zn-kn3.train.fr2en
\end{alltt}
\end{small}
The phrase table \code{cpt.merged.hmm3-rf-zn.train.fr2en.gz} is the main source
of information for French to English translation. Each of its lines is of the
form:
\begin{verbatim}
   fr ||| en ||| p1(fr|en) p2(fr|en) p1(en|fr) p2(en|fr)
\end{verbatim}
where \code{fr} is a French source phrase, \code{en} is an English target
phrase, \code{p(fr|en)} is the probability that \code{fr} is the
translation of \code{en} (the ``backward'' probability), and
\code{p(en|fr)} is the probability that \code{en} is the translation of
\code{fr} (the ``forward'' probability). In our example here, \code{p1} is
the relative-frequency estimate of this probability, while \code{p2} is the
Zens-Ney lexical smoothing based estimate: those are the smoothers we
recommend using. There can be any number of backward and forward probability
estimates, reflecting the number of smoothers you use.
%
Here are two sample lines from this file:
\begin{small}
\begin{alltt}
   > \textbf{zgrep '| proposed regulations |' cpt.merged.hmm3-rf-zn.train.fr2en.gz}
   projets de règlement ||| proposed regulations ||| 0.67 0.0016 1 0.026
   règlements proposés ||| proposed regulations ||| 0.33 0.018 1 0.42
\end{alltt}
\end{small}
Note that this example is carefully chosen; because of the small size of the
training corpus, many other entries in the phrase table are not such good
translations.

\subsection{Creating a Lexicalized Distortion Model} \label{LDM}

Lexicalized Distortion Models (LDMs) are not used by default in this
framework.  If you want to enable their creation and use, you should set
\code{USE_LDM} to \code{1} in \code{Makefile.params} (by removing the initial
\code{\#} to uncomment the appropriate line).

The LDM is created in two steps: 1) counting occurrences of the different types
of distortion instances over the training corpus, using the program
\code{dmcount}, and 2) estimating scores from these counts, using the program
\code{dmestm}.  If you trained more than one phrase table, as we did above,
the first step is repeated for each phrase table.
\begin{small}
\begin{alltt}
   > \textbf{cd models/ldm}
   > \textbf{make all}
   parallelize.pl -nolocal -psub -1 -n 4 -np 4 -w 100000 \bs
      -s '../../corpora/train_fr.lc.gz' -s '../../corpora/train_en.lc.gz' \bs
      -merge 'merge_multi_column_counts -' \bs
      'dmcount -v -m 8 \bs
       ../tm/ibm2.train.en_given_fr.gz ../tm/ibm2.train.fr_given_en.gz \bs
       ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
       > ldm.counts.ibm2.gz' \bs
      >& log.ldm.counts.ibm2
   parallelize.pl -nolocal -psub -1 -n 4 -np 4 -w 100000 \bs
      -s '../../corpora/train_fr.lc.gz' -s '../../corpora/train_en.lc.gz' \bs
      -merge 'merge_multi_column_counts -' \bs
      'dmcount -v -m 8 \bs
       ../tm/hmm3.train.en_given_fr.gz ../tm/hmm3.train.fr_given_en.gz \bs
       ../../corpora/train_fr.lc.gz ../../corpora/train_en.lc.gz \bs
       > ldm.counts.hmm3.gz' \bs
      >& log.ldm.counts.hmm3
   merge_multi_column_counts - ldm.counts.hmm3.gz ldm.counts.ibm2.gz \bs
      | time-mem dmestm -s -g ldm.hmm3+ibm2.fr2en.bkoff \bs
        -wtu 5 -wtg 5 -wt1 5 -wt2 5 2> log.ldm.hmm3+ibm2.fr2en \bs
      | gzip > ldm.hmm3+ibm2.fr2en.gz
\end{alltt}
\end{small}

Estimating lexicalized distortion parameters (step 2, \code{dmestm}) might
require a lot of memory, sometimes two to three times as much as training
phrase tables.  As mentioned before, you can use \code{make time-mem} to see
your resource usage after the process has completed to place your resource
allocation.  We generally observe a small gain in translation quality (as
measured by BLEU) when using LDMs, although you will need to determine,
depending on your situation, whether this gain is worth the added memory and
time requirements.

\tip\margintip The commands for step 1 illustrate the use of one of our generic utilities:
\code{parallelize.pl}.  This script can be used to parallelize the execution
of any command where each input line is processed independently.  It takes care
of splitting the input(s) into chunks, running the chunks in parallel, and
concatenating (or otherwise merging) the outputs.  In this example the output
from all instances of \code{dmcount} is simply concatenated together, which
works correctly because \code{dmestm} tallies counts when it sees repeated
phrase pairs.  Run \code{parallelize.pl -h} for more details.\tipend

The rest of this document assumes you did not train LDMs, so if you modified
your \code{Makefile.params} file to set \code{USE_LDM = 1}, you should comment
out that line before proceeding to the next section.

\subsection{Optimizing Decoder Weights} \label{COW}

The language and translation models are the main sources of information used
for translation, which is performed by the \code{canoe} decoder.  Optionally,
we might train an LDM as an additional source of information, but we're not
including it here.  In order to get reasonable translation quality, the weights
on these (and other) sources of information need to be tuned. The tuning
process is carried out by a script called \code{cow.sh}, which runs
\code{canoe} several times.  \code{cow.sh} takes in an initial \code{canoe.ini}
configuration file and produces an optimized configuration file,
\code{canoe.ini.cow}.

You can run all the steps below by typing \code{make decode} or \code{make cow}
in your main \code{toy.experiment} directory, but we'll break it down into
several steps here.

\subsubsection{The Decoder Configuration File: \code{canoe.ini}}

In this framework, we start with a template configuration file,
\url{models/decode/canoe.ini.template}:
\begin{small}
\begin{alltt}
   > \textbf{cd models/decode}
   > \textbf{cat canoe.ini.template}
   [ttable-multi-prob]
     <CPTS>
   [lmodel-file]
     <LMS>
   [use-ftm]
   ...
\end{alltt}
\end{small}

The framework will automatically insert the phrase tables and language models
generated in the previous steps into the \code{canoe.ini} file when you
make it.  These are the template parameters it will replace:
\begin{itemize}
\item \code{<SL>}:   source language code, e.g. ``fr'';
\item \code{<TL>}:   target language code, e.g., ``en'';
\item \code{<CPTS>}: all conditional phrase tables trained above and found
                       in the \code{models/tm} directory; and
\item \code{<LMS>}:  all language models trained trained above and found in
                       the \code{models/lm} directory);
\item If \code{USE_LDM} is set in \code{Makefile.params}, special
processing is also done to insert the necessary LDM parameters into the
\code{canoe.ini} file.
\end{itemize}

So we make the \code{canoe.ini} file:
\begin{small}
\begin{alltt}
   > \textbf{make canoe.ini}
   cat canoe.ini.template \bs
      | sed -e 's/<SL>/fr/g' \bs
            -e 's/<TL>/en/g' \bs
            -e 's#<CPTS>#models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.gz#g' \bs
            -e 's#<LMS>#models/lm/train_en-kn-5g.binlm.gz#g' \bs
            -e 's/^\bs(\bs[\bs(weight-f\bs|ftm\bs)\bs]\bs)/##mid##\bs1/' \bs
      | configtool -p "args:-distortion-model WordDisplacement " - \bs
      > canoe.ini
   configtool check canoe.ini
   ok
\end{alltt}
\end{small}
This command silently creates a soft link to the \code{models} directory---this
way, all models appear to be relative to the current directly, which makes it easier
to find them.  We create such symlinks everywhere we need access to the
models.  The \code{sed} commands replace the template parameters by the
appropriate values.  The first \code{configtool} command inserts the lexicalized
distortion parameters if necessary.  The final command, \code{configtool
check canoe.ini}, confirms that the \code{canoe.ini} file produced is
error-free---it checks that all parameters are compatible and that all models
can be found.

Here is the result:\footnote{If you set \code{USE\_LDM=1}, you would also see a
line pointing to the LDM file, \code{[lex-dist-model-file]
models/ldm/ldm.hmm3+ibm2.fr2en.gz}; the \code{[distortion-model]} block would
include the six LDM features, \code{back-lex\#m back-lex\#s back-lex\#d
fwd-lex\#m fwd-lex\#s fwd-lex\#d}; and the options \code{[dist-phrase-swap]} and
\code{[dist-limit-ext]} would be enabled.}
\begin{small}
\begin{alltt}
   > \textbf{cat canoe.ini}
   [ttable-multi-prob]
      models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.gz
   [lmodel-file]
      models/lm/train_en-kn-5g.binlm.gz
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [distortion-model]
      WordDisplacement
   [segmentation-model] none
   [bypass-marked]
   [cube-pruning]
   [use-ftm]
\end{alltt}
\end{small}

When using this framework, do not modify \code{canoe.ini} directly.  To
change the training parameters for language models or phrase tables, modify the
parameters or Makefiles appropriately in \code{models/lm} and
\code{models/tm} and regenerate those models. Be careful, though: the
\code{canoe.ini} file will include all models generated in the same
framework, so you should work in different copies of the framework if you want
to experiment with different training parameters for those models.
\tip\margintip Soft links can save you a lot of duplicated work if you have
related experiments.\tipend

\tip\margintip Any time a \code{canoe.ini} configuration file is manipulated,
always run \code{configtool check} on the resulting \code{canoe.ini} file to
verify the validity of its contents.\tipend

To change the other decoding parameters, modify \code{canoe.ini.template}.
For example, you can add additional language models (trained outside the
framework) by adding them in the \code{[lmodel-file]} section, separated by
whitespace or a newline from \code{<LMS>}.

The other decoding parameters control the search strategy.  They have been set
to reasonable defaults, but may need adjustment depending on the
speed/quality trade-offs you're willing to make or other models you wish to
use.

\subsubsection{Running COW (Canoe Optimize Weights)}

Besides the configuration file, the other main arguments required by
\code{cow.sh} are a directory in which to store temporary files, a source
file and one or more correct (i.e., human) translations of the source
file.\footnote{This example uses only one translation, but when multiple
reference translations are available, it is advantageous to use them.}
Here we call the temporary directory \code{foos}, and we use the \code{dev1}
files for weight tuning:
\begin{small}
\begin{alltt}
   > \textbf{make all}
   mkdir -p foos
   cow.sh -v -parallel:"-n 4" -maxiter 15 -nbest-list-size 100 \bs
      -filt -nofloor -workdir foos -f canoe.ini
      ../../corpora/dev1_fr.rule ../../corpora/dev1_en.lc \bs
      >& log.canoe.ini.cow
   rm -f -r foos *.FILT.gz *.FILT.bkoff canoe.ini.FILT canoe.ini.FILT.cow
\end{alltt}
\end{small}
Because \code{cow.sh} takes a while to run (between 30 minutes and three
hours in this example, depending on your system)\footnote{Running time for
\code{cow.sh} can be reduced by executing it in parallel. Type
\code{cow.sh -h} to see documentation on this option; modify
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}DECODE} in \code{Makefile.params} to
adjust cow parallelism in this framework.} and writes large amounts of logging
information, it is usually a good idea to redirect all of its output to a log
file, as is done here; and also to run it in the background (not done here).
Progress is most easily monitored by looking at the file
\code{rescore-results},
which shows the translation quality (measured by BLEU score---see
\S\ref{Testing} for a description), as well as the current weights, for
each iteration.

\tip \margintip The scripts \code{best-rr-val} and \code{all-best-rr-vals} can
be handy if you are monitoring multiple experiments. \tipend

Note that the BLEU score does not necessarily increase
monotonically, as can be seen in several places in the \code{rescore-results}
file obtained with this example:
% cat rescore-results | perl -pe 's/(\.\d\d\d\d)\d+(:| \S|$)/\1\2/g; s/   -d/  -d/' | cut -c1-92
\begin{footnotesize}
\begin{alltt}
> \textbf{cat rescore-results}
BLEU score: 0.175702  -d 1 -w 0 -lm 1 -tm 1:1 -ftm 1:1
BLEU score: 0.138512  -d 0.0305 -w 0.0627 -lm 1 -tm 0.1566:0.1862 -ftm -0.1986:0.0424
BLEU score: 0.168389  -d 0.4184 -w -0.0291 -lm 1 -tm 0.0050:0.1416 -ftm 0.0128:0.0588
BLEU score: 0.168268  -d 0.0422 -w -0.7845 -lm 1 -tm 0.0637:0.1968 -ftm 0.5699:0.4498
BLEU score: 0.169137  -d 0.4170 -w -0.4870 -lm 1 -tm 0.4649:-0.0837 -ftm 0.3092:0.5002
BLEU score: 0.201762  -d 0.4026 -w -0.6294 -lm 1 -tm 0.3919:0.0030 -ftm -0.0031:0.3425
BLEU score: 0.203570  -d 0.6529 -w -0.5167 -lm 1 -tm 0.3340:0.0316 -ftm -0.1029:0.3483
BLEU score: 0.210349  -d 0.5032 -w -0.4448 -lm 1 -tm 0.3490:0.0305 -ftm -0.0990:0.3400
BLEU score: 0.210680  -d 0.4978 -w -0.4444 -lm 1 -tm 0.3489:0.0304 -ftm -0.0991:0.3363
BLEU score: 0.210787  -d 0.4966 -w -0.4423 -lm 1 -tm 0.3483:0.0303 -ftm -0.0938:0.3363
BLEU score: 0.210787  -d 0.4952 -w -0.4423 -lm 1 -tm 0.3484:0.0302 -ftm -0.0939:0.3363
BLEU score: 0.210787  -d 0.4953 -w -0.4424 -lm 1 -tm 0.3484:0.0302 -ftm -0.0939:0.3364
\end{alltt}
\end{footnotesize}
When the last iteration still shows an improvement (which is not the case
here), it is sometimes a sign that we should have allowed \code{cow.sh} to run
more iterations, by setting the \code{-maxiter} option to a higher value.  You
will notice that there are two weights after \code{-tm} (truncated in the
output shown above): one for each of the backward probability estimates in the
merged phrase table.  For the same reason, there are two weights after
\code{-ftm}.  If we were using LDMs, there would be seven weights after
\code{-d}: standard WordDisplacement model (``distortion penalty''), as well as
the six-feature lexicalized distortion model.

While \code{cow.sh} is working, it saves $n$-best lists and other intermediate
files to a temporary work directory called \code{foos}. If you wish to prevent
these (large) files from being automatically cleaned up on completion, remove
the \code{rm} command executed after \code{cow.sh} in \code{Makefile}.

\tip \margintip A lot of information is logged to \code{log.canoe.ini.cow};
this can be summarized using the command \code{cowpie.py log.canoe.ini.cow}
(type \code{cowpie.py -h} for details). \tipend

\tip \margintip The scripts \code{cow-timing.pl} and
\code{canoe-timing-stats.pl} provide detailed running time information for the
various components of \code{cow.sh}. \tipend

\tip \margintip If you have experiments in various directories with names
following, say, \code{framework.<experiment name>}, go in their common parent
directory and run \code{summarize-canoe-results.py framework.*} to see where
they're all at in terms of BLEU scores and cow iterations completed.  \tipend

The final output from \code{cow.sh} is written to the file
\code{canoe.ini.cow} (assuming the original configuration file was called
\code{canoe.ini}). This duplicates the contents of \code{canoe.ini}, but
adds the weights tuned on the development corpus:
% cat canoe.ini.cow | perl -pe 's/(\.\d\d\d\d\d\d)\d+/\1/g; s/(\.\d\d\d\d\d\d)\d+/\1/g if /weight-d/; s/^/   /'
\begin{small}
\begin{alltt}
   > \textbf{cat canoe.ini.cow}
   [ttable-multi-prob]
      models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.gz
   [lmodel-file]
      models/lm/train_en-kn-5g.binlm.gz
   [weight-d] 0.4966295362
   [weight-w] -0.4423593283
   [weight-l] 1
   [weight-t] 0.3483754396:0.03033262119
   [weight-f] -0.09389718622:0.3363513947
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [distortion-model]
      WordDisplacement
   [segmentation-model] none
   [bypass-marked]
   [cube-pruning]
   [use-ftm]
\end{alltt}
\end{small}
The \code{weight-} parameters above pertain to the distortion model
(\code{weight-d}), the word-length penalty (\code{weight-w}), the language
model (\code{weight-l}), the backward scores of the translation models
(\code{weight-t}), and the forward scores of the translation models
(\code{weight-f}).
The language model often obtains the highest weight, as is the
case here.\footnote{A note of warning about the LM weight: following Doug Paul's
``ARPA'' LM format, all LM formats we know use base-10 log probs (including our
own binary LM and TPLM formats), but canoe interprets them as natural logs:
throughout \PS, logs are natural by default, not base 10.  This known bug has
minimal impact; correcting it simply requires multiplying the desired
\code{weight-l} values by log(10), which \code{cow.sh}, \code{rat.sh} and
\code{rescore\_train} do implicitly. We chose not to fix it to avoid having to
adjust all previously tuned sets of weights.}


\subsection{Training a Confidence Estimation Model} \label{CE}

If you are going to use confidence estimation (CE), the final step is to train a
model to estimate the confidence of the system on the decoder's output. This
model uses features about the source text, the target text, translation memory
information if available, and so on, to come up with a confidence score between
0 and 1. As we mentioned before, it is critical that the data used to tune the
CE model be completely unseen data. It can't have been part of your training
data or the data you used to tune the decoder weights or the rescoring model.
Otherwise the confidence estimates produced will not be useful.

At this stage we have to return to one previous step. We did not train all
the models we need for CE: we need an LM for the source language. Edit
\code{Makefile.params} in the main \code{toy.experiment} directory and set
\code{DO_CE} to \code{1} (by uncommenting the approriate line). If you run
\code{make confidence} in the main directory, all necessary steps will be done
automatically.  As usual, we'll break it down a bit.
\begin{small}
\begin{alltt}
   > \textbf{cd models}
   > \textbf{make lm.fr}
   make -C lm all LM_LANG=fr
   [...]
\end{alltt}
\end{small}
This command will build the source-side LM, using the toolkit you selected
earlier.

Now we can work on the CE model itself. Note that our CE module does not
currently work with rescoring, only with decoding. We build on the decoder
model tuned in section~\ref{COW}.

Just as for decoding and rescoring, CE works with a model described in a text
file.  In this framework we provide a template model that does not use a
translation memory.  If you have access to the results of looking up your
source text in a translation memory, it is worth incorporating them into your
CE model.  In the template provided here, all the translation memory related
features are commented out.  See \url{doc/README.confidence} for more
details.

Now we build the input CE model file:
\begin{small}
\begin{alltt}
   > \textbf{cd models/confidence}
   > \textbf{make ce-notm.ini}
   sed -e 's#dev1.mixlm#devCE.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow
   configtool check canoe.ini.cow
   ok
   cat ce-notm.template \bs
      | sed -e "s#IBM\bs\bs(.\bs\bs)FWD#models/tm/ibm\bs\bs1.train.en_given_fr.gz#" \bs
            -e "s#IBM\bs\bs(.\bs\bs)BKW#models/tm/ibm\bs\bs1.train.fr_given_en.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)FWD#models/tm/hmm\bs\bs1.train.en_given_fr.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)BKW#models/tm/hmm\bs\bs1.train.fr_given_en.gz#" \bs
            -e "s#LM_SRC#models\/lm\/train_fr-kn-5g.binlm.gz#" \bs
            -e "s#LM_TGT#models\/lm\/train_en-kn-5g.binlm.gz#" \bs
      > ce-notm.ini
\end{alltt}
\end{small}

The result is a \code{ce-notm.ini} file with the LMs and IBM models filled in,
which we now feed to \code{ce_translate.pl -train}.  The output will be a
trained CE model.

\begin{small}
\begin{alltt}
   > \textbf{make all}
   ce_translate.pl -n=4 -train -src=fr -tgt=en -notok -nolc -nl=s \bs
      -k=5 -desc=ce-notm.ini canoe.ini.cow ce_model \bs
      ../../corpora/devCE_fr.lc ../../corpora/devCE_en.lc \bs
      >& log.ce_model.cem
\end{alltt}
\end{small}

The trained model is saved to the file \code{ce_model.cem}.  This file is
actually a gzipped-tarred archive, so you could examine it using the
command \code{tar -xOzf ce_model.cem | less}.\footnote{Note that the command
\code{tar -xOzf} contains the capital letter \code{O} (oh), not the number
\code{0} (zero).} However, the contents don't have much intuitive meaning, so
this is only useful for curiosity's sake.

\subsection{Training a Rescoring Model} \label{RAT}

If you are using rescoring, the final training step is to create a model for
rescoring $n$-best lists.  Rescoring means having \code{canoe} generate a list
of $n$ (typically 1000) translation hypotheses for each source sentence, then
choosing the best translations from among these.  The advantage of this
procedure is that the choice can be made on the basis of information that is
too expensive for \code{canoe} to use during search. This step sometimes gives
a modest improvement over the results obtained using \code{canoe} alone. Often,
however, it gives no significant improvement while being fairly slow, so if
translation speed is an issue, you probably want to skip rescoring; experiment
with your own data to determine the best choice.  In this framework, rescoring
is only done if \code{DO_RESCORING} is set to 1 in \code{Makefile.params}.

Training a rescoring model involves generating $n$-best lists, then calculating
the values of selected \emph{features} for each hypothesis in each list. A
feature is any real-valued function that is intended to capture the relation
between a source sentence and a translation hypothesis. A rescoring model
consists of a vector of feature weights set so as to optimize translation
performance when a weighted combination of feature values is used to reorder
the $n$-best lists.

You can run all the steps below by typing \code{make rescore} or
\code{make rat} in your main \code{toy.experiment} directory, but we'll break
it down as usual.

Rescoring is not currently compatible with the new truecasing method (using
source side information).  In order to complete this section, you will need to
edit \code{Makefile.params} and set \code{DO_RESCORING = 1} and comment out
remove \code{TC_USE_SRC_MODELS = 1}.

Rescoring is also incompatible with confidence estimation, but the framework
can be used to train both a rescoring and a confidence estimation model: you
just won't be able to use them together.

\subsubsection{The Input Rescoring Model}

Training is carried out by the \code{rat.sh} script. This takes as input a
rescoring model that specifies which features to use, and it returns optimal
weights for these features.

The default model created by this framework contains a small set of useful
features:
\begin{small}
\begin{alltt}
   > \textbf{cd models/rescore}
   > \textbf{make rescore-model.ini}
   configtool rescore-model:ffvals models/decode/canoe.ini.cow \bs
      | cut -f 1 -d ' ' > rescore-model.ini
   cat rescore-model.template \bs
      | sed -e "s#IBM\bs\bs(.\bs\bs)FWD#models/tm/ibm\bs\bs1.train.en_given_fr.gz#" \bs
            -e "s#IBM\bs\bs(.\bs\bs)BKW#models/tm/ibm\bs\bs1.train.fr_given_en.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)FWD#models/tm/hmm\bs\bs1.train.en_given_fr.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)BKW#models/tm/hmm\bs\bs1.train.fr_given_en.gz#" \bs
      >> rescore-model.ini
   > \textbf{cat rescore-model.ini}
   FileFF:ffvals,1
   FileFF:ffvals,2
   FileFF:ffvals,3
   FileFF:ffvals,4
   FileFF:ffvals,5
   FileFF:ffvals,6
   FileFF:ffvals,7
   # NB: we omit some features that are slow to compute.
   # Use rescore_train -H for a complete list.
   LengthFF
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz
   HMMTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz
   HMMSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz
   HMMVitTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz
   HMMVitSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz
   nbestWordPostSrc:1#<ffval-wts>#<pfx>
   nbestWordPostTrg:1#<ffval-wts>#<pfx>
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx>
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx>
   nbestNgramPost:2#1#<ffval-wts>#<pfx>
   nbestSentLenPost:1#<ffval-wts>#<pfx>
   ParMismatch
   QuotMismatch:ef
   #CacheLM:<src>.id
   RatioFF
\end{alltt}
\end{small}
There are two kinds of features included in our rescoring model:
\begin{itemize}
\item Those that look like \code{FileFF:ffvals,}$i$ tell \code{rat.sh} to
use the $i$\/th feature generated by the \code{canoe} decoder itself. It is
standard practice to use all decoder features when rescoring, as is done
automatically by the framework via the \code{configtool} command executed
above.
\item The other features, following the format \emph{FeatureName:Args}, tell
\code{rat.sh} to generate values for the feature \emph{FeatureName} using
arguments \emph{Args}.  For example, the string
\url{IBM2TgtGivenSrc:model/tm/ibm2.train.en_given_fr.gz} says to calculate
the feature \code{IBM2TgtGivenSrc} using the IBM model
\code{model/tm/ibm2.train.en_given_fr.gz}, which we trained earlier. To
see a list of all available features, type \code{rescore_train -H}.  In this
framework, you should set your features in \code{rescore-model.template}.
The special tokens all in upper case (\code{IBM1FWD}, etc) you see in the
provided template are replaced by the actual models you trained earlier.
\end{itemize}

Lines starting with \code{\#} are comments and are ignored by the software.

\subsubsection{Running RAT (Rescore And Translate)}

Apart from the rescoring model, \code{rat.sh} needs a source file and one or
more reference translations for it (same as \code{cow.sh}). These may be the
same files used for \code{cow.sh}, but it is sometimes better to use different
ones, so here we use \code{dev2}:\footnote{As with \code{cow.sh}, you can
speed this up using parallelism, in this case via the -n option to
\code{rat.sh}, given before the \code{train} token (type \code{rat.sh -h}
for details). In this framework, parallelism is controlled via the
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}RESCORE} variable in
\code{Makefile.params}.}
\begin{small}
\begin{alltt}
   > \textbf{make all}
   sed -e 's#dev1.mixlm#dev2.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow.dev2
   configtool check canoe.ini.cow.dev2
   ok
   Tuning the rescoring model.
   rat.sh -lb -n 4 train -v -K 1000 -o rescore-model \bs
      -msrc ../../corpora/dev2_fr.rule -f canoe.ini.cow.dev2 rescore-model.ini \bs
      ../../corpora/dev2_fr.lc ../../corpora/dev2_en.lc >& log.rescore-model
   rm -f -r workdir-dev2_fr.rule-1000best
\end{alltt}
\end{small}

The output from \code{rat.sh} is written to the file \code{rescore-model}:
\begin{small}
\begin{alltt}
   > \textbf{cat rescore-model}
   FileFF:ffvals,1 0.146268636
   FileFF:ffvals,2 0.0670145303
   FileFF:ffvals,3 -0.02778978646
   FileFF:ffvals,4 0.00745564606
   FileFF:ffvals,5 0.09763943404
   FileFF:ffvals,6 0.007505016867
   FileFF:ffvals,7 -0.0189950224
   # NB: we omit some features that are slow to compute. 0
   # Use rescore_train -H for a complete list. 0
   LengthFF 8.133889787e-05
   IBM1TgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz -0.000415542132
   IBM1SrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.01087670028
   IBM2TgtGivenSrc:models/tm/ibm2.train.en_given_fr.gz 0.00176046392
   IBM2SrcGivenTgt:models/tm/ibm2.train.fr_given_en.gz 0.00724581303
   HMMTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz -0.00221155909
   HMMSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz -0.00353578873
   HMMVitTgtGivenSrc:models/tm/hmm3.train.en_given_fr.gz -0.00428659326
   HMMVitSrcGivenTgt:models/tm/hmm3.train.fr_given_en.gz 0.00148704838
   IBM1WTransTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz 0.0112034809
   IBM1WTransSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz -0.0190998213
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.train.en_given_fr.gz -1
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.train.fr_given_en.gz 0.433482093
   nbestWordPostSrc:1#<ffval-wts>#<pfx> -0.004983890336
   nbestWordPostTrg:1#<ffval-wts>#<pfx> 0.001077609486
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx> -0.0001558708027
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx> 0.0002006395371
   nbestNgramPost:2#1#<ffval-wts>#<pfx> 0.02301278338
   nbestSentLenPost:1#<ffval-wts>#<pfx> -0.1470126659
   ParMismatch 6.123407275e-06
   QuotMismatch:ef -5.882521691e-06
   #CacheLM:<src>.id 0
   RatioFF -0.8815089464
\end{alltt}
\end{small}
This is a copy of \code{rescore-model.ini} with a weight assigned to each
feature. Other by-products created by \code{rat.sh} are found in the directory
\code{workdir-dev2_fr.lc-1000best} and include the $n$-best lists
\code{1000best.gz}, and the corresponding decoder features \code{ffvals.gz}
and additional features \code{ff.*}. All of these files are compressed to
save space. These files are automatically deleted unless there is a problem.
Remove the \code{rm} command executed after \code{rat.sh} in Makefile if
you want to preserve them.

Before continuing, comment out \code{DO_RESCORING = 1} and set
\code{TC_USE_SRC_MODELS = 1} again in \code{Makefile.params}, since these are
the values assumed below.

\section{Translating and Testing} \label{TranslatingTesting}

\subsection{Translating} \label{Translating}

Once training is complete, the system can be used to translate new text or the
test corpus.

Some of the steps below will be performed if you run \code{make translate} in
your main \code{toy.experiment} directory, but the final output you need
depends on what you are doing and might not be produced by default. The actual output
produced depends on the various \code{DO_*} variables in
\code{Makefile.params}.

\subsubsection{Decoding Only} \label{Decoding}

As mentioned before, there are three options for translating. The simplest is
to decode using the configuration file produced by \code{cow.sh} and stop.
To do so, make sure \code{DO_RESCORING} is not set in \code{Makefile.params}.
\footnote{You can accomplish the same result by adding \code{DO\us{}RESCORING=}
to the make command line, i.e., type \\
\hspace*{.2in}\code{make translate DO\us{}RESCORING=} \\
instead of just \code{make translate} (notice there is nothing after the
\code{=} sign). This syntax can always be used to temporarily override a
variable definition on a given call to \code{make}: adding \code{V=} undefines
\code{V}, while adding \code{V=value} sets V to the given value.}
\begin{small}
\begin{alltt}
   > \textbf{cd translate}
   > # edit ../Makefile.params and comment out DO_RESCORING = 1
   > \textbf{make translate}
   sed -e 's#models/lm/dev1.mixlm#test.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow.test
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.out
   canoe-parallel.sh -lb -n 4 canoe -f canoe.ini.cow.test -palign \bs
      < ../corpora/test_fr.rule 2> log.test.out \bs
      | nbest2rescore.pl -canoe -tagoov -palout=test.out.pal  2> log.test.out.pal \bs
      | tee test.out.oov | perl -pe 's/<OOV>(.+?)<\bs/OOV>/\bs1/g;' > test.out
\end{alltt}
\end{small}
This produces the output file \code{test.out}, containing one line for each
source line in \url{test.rule}, as well as \code{test.out.pal} and
\code{test.out.oov}, needed for truecasing.

\subsubsection{Decoding plus Confidence Estimation} \label{CETrans}

The second option for translating is to run the decoder and then estimate the
confidence of the system on the output of the decoder.  This option is not
compatible with rescoring: the confidence estimate is for the one-best output
of the decoder only.  We use the decoder model tuned in section~\ref{COW}, and
then the CE model trained in section~\ref{CE}.  The program
\code{translate.pl} handles both of these steps:
\begin{small}
\begin{alltt}
   > \textbf{cd translate}
   > \textbf{make confidence}
   ln -fs models/confidence/ce_model.cem
   Generating test.ce
   translate.pl -with-ce -n 4 -notok -nl s -tc -encoding UTF-8 -src fr -tgt en \bs
      -f canoe.ini.cow.test -model ce_model.cem ../corpora/test_fr.al \bs
      > test.ce 2> log.test.ce
\end{alltt}
\end{small}
The main output file, \code{test.ce}, is different from the \code{test.out}
file we saw in the two previous steps in that 1) a confidence estimate is added
at the beginning of each line, and 2) the output has already been
truecased and detokenized. However, the translations are actually the same as in
\code{test.out}, with some postprocessing performed on them.

\subsubsection{Decoding plus Rescoring} \label{RATTrans}

If you executed the commands in the \emph{Decoding Only} section above, please
run \code{make clean} before proceeding.  To run this variant, edit
\code{Makefile.params} and set (or uncomment the line) \code{DO_RESCORING = 1},
and comment out \code{TC_USE_SRC_MODELS = 1}.  We recommend using the new
truecasing workflow, but we have to disable it temporarily since it is not yet
compatible with rescoring, as we mentioned before.
\begin{small}
\begin{alltt}
   > \textbf{cd translate}
   > # edit ../Makefile.params, set DO_RESCORING = 1 and comment out TC_USE_SRC_MODELS.
   > \textbf{make clean}
\end{alltt}
\end{small}

The third option for translating is to generate $n$-best lists and rescore
them using the model generated in section~\ref{RAT}. To do this, we first run
the decoder model tuned in section~\ref{COW}, and then the rescoring model
tuned in section~\ref{RAT}.  The \code{rat.sh} script, used in translation
mode, performs both steps for us:
\begin{small}
\begin{alltt}
   > \textbf{make translate}
   sed -e 's#models/lm/dev1.mixlm#test.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow.test
   configtool check canoe.ini.cow.test
   ok
   Generating test.rat
   rat.sh -lb -n 4 trans -v -K 1000 -msrc ../corpora/test_fr.rule \bs
      -f canoe.ini.cow.test models/rescore/rescore-model \bs
      ../corpora/test_fr.lc >& log.test.rat \bs
      && mv test_fr.rule.rat test.rat
   cp workdir-test_fr.rule-1000best/1best test.out
\end{alltt}
\end{small}
This produces both \code{test.out}, the 1-best output of the decoder, and
\code{test.rat}, the best translation according to the rescoring model.  The
file \code{test.out} should be identical to the one produced in
section~\ref{Decoding}, although they might differ in minor ways because of
rounding differences.

Before continuing, comment out \code{DO_RESCORING = 1} and set
\code{TC_USE_SRC_MODELS = 1} again in \code{Makefile.params}, since these are
the values assumed below.

\subsubsection{Truecasing and Detokenizing} \label{Truecasing}

If you inspect the \code{test.out}, and/or code{test.rat} output files, you will
notice that they contain lowercase, tokenized text.  Two postprocessing steps
are required to restore normal case and normally spaced text: truecasing and
detokenizing.  The \code{test.ce} output file, on the other hand, already
contains truecased, detokenized text.

Truecasing is done by the \code{truecase.pl} script using the truecasing models
trained previously:
\begin{small}
\begin{alltt}
   > \textbf{make tc}
   Truecasing test.out.tc
   truecase.pl -text test.out.oov -bos -encoding UTF-8 \bs
      -lm models/tc/train_en-kn-3g.binlm.gz -map models/tc/train_en.map \bs
      -src ../corpora/test_fr.al -pal test.out.pal -srclang fr \bs
      -srclm models/tc/train_fr.nc1.binlm.gz > test.out.tc 2> log.test.out.tc
\end{alltt}
\end{small}

If you are using the old truecasing workflow, the same files will be generated
by \code{truecase.pl}, but the options will be different (the \code{-src},
\code{-pal}, \code{-srclang} and \code{-srclm} options will be omitted), and you
will not get all the benefits of the new workflow.

If you are using rescoring, \code{make tc} will also truecase the rescoring
output, generating \code{test.rat.tc}.

Detokenizing is done by the script \code{udetokenize.pl}, which has
hand-coded rules to detokenize French or English text encoded in UTF-8. Other
languages are not supported at this point but may already be handled partly
correctly by this script. As for other encodings, latin1 and cp-1252 are
supported by \code{detokenize.pl}.

\begin{small}
\begin{alltt}
   > \textbf{make detok}
   Detokenizing test.out
   udetokenize.pl -lang=en test.out test.out.detok
   Detokenizing test.out.tc
   udetokenize.pl -lang=en test.out.tc test.out.tc.detok
\end{alltt}
\end{small}

Detokenizing is not performed here on the \code{test.ce} file because
it was already done by \code{translate.pl} along with truecasing.

If you are using rescoring, \code{make detok} will also detokenize the
rescoring output, generating \code{test.rat.tc.detok} and
\code{test.rat.detok}.

% TODO fix the framework so that it does handle truecasing.

\subsection{Testing} \label{Testing}

Translation quality can be evaluated automatically using the BLEU metric, which
is a measure of how well the translation matches one or more reference
translations that are known to be correct (normally human translations). It is
based on the number of short word sequences that the translation has in common
with the references, and varies between 0 for no matches to 1 for a perfect
match. BLEU is calculated by the program \code{bleumain}:
\begin{small}
\begin{alltt}
   > \textbf{make bleu}
   Calculating BLEU for test.out.bleu
   bleumain -c test.out ../corpora/test_en.lc > test.out.bleu
   Calculating BLEU for test.rat.bleu
   bleumain -c test.rat ../corpora/test_en.lc > test.rat.bleu
   grep Human *.bleu
   test.out.bleu:Human readable value: 19.15 +/- 3.25
   test.rat.bleu:Human readable value: 17.93 +/- 3.28
\end{alltt}
\end{small}
Here we show the BLEU score for rescoring too, for illustration, but without
\code{DO_RESCORING} you would only see \code{test.out.bleu}.

The human readable value is the BLEU score multiplied by 100 rounded to 2
decimals. The full output from \code{bleumain}, saved in \code{*.bleu},
contains match statistics of various orders, followed by the global BLEU score
with a 95\% confidence interval, as shown above.

The result above can also be obtained by typing \code{make eval}, or
\code{make all}, in your \code{toy.experiment} directory.

Here, we calculate only the BLEU scores on the lowercase output, using the
lowercase reference, but you can get BLEU scores for the truecased output by
giving the truecase translation and reference(s) to \code{bleumain}.

These results indicate that the translation produced by rescoring is worse
than the one produced by plain decoding. Given the small size of the test set,
it seems unlikely that the difference is statistically significant. To test
this hypothesis, we can use \code{bleucompare}, which does a comparison using
pairwise bootstrap resampling:
\begin{small}
\begin{alltt}
   > \textbf{bleucompare test.rat test.out REFS ../corpora/test_en.lc }
   Comparing using BLEU
   test.rat got max BLEU score in 1.3% of samples
   test.out got max BLEU score in 98.7% of samples
\end{alltt}
\end{small}
This indicates that the difference is in fact significant, at least at the
p=0.05 level, though not at the p=0.01 level.  (Note that you may get
completely different results here, since the training corpus used is much too
small to produce reliable results.)

\section{Translating New Text}

The translation methods shown in the previous section work well with the test
set you dropped into the \code{corpora} directory, but it is not convenient for
translating new text.  You can use the \code{translate.pl} script for new text.
It is aware of the way this framework works and where it puts its models, so it
makes it easy to carry out the whole translation pipeline: tokenization,
lowercasing, decoding, truecasing, detokenization, with optional plugins you
can use to insert ad hoc code to handle special cases correctly.  See
\code{translate.pl -h} for details.  The \code{translate.sh} script you will
find in the framework calls \code{translate.pl} with some of the options
automatically filled in from the parameters you set in \code{Makefile.params}.
See \code{./translate.sh -h} for details.

\section{PortageLive}

Now that we have a fully trained and tested system, you might want to deploy it
on a PortageLive translation server.

The PortageLive models are optimized in various ways for a run-time
environment.  Most importantly, they use our tightly packed model file format,
accessed via memory-mapped IO.
%; the various phrase tables are collapsed into a single table with the weights
%pre-applied; filtering of the phrase table, normally done on the fly, is
%pre-performed; and so on.
The result is a system that will run faster and with less memory on the
run-time server than equivalent requests would within the framework.

There is a \code{make} target in the framework to facilitate this process:
\code{make portageLive}.  This target, run directly from the top framework
directory (\code{toy.experiment}), will cause all models to be converted and
filtered as required, and it will create a directory structure with symbolic
links to all the model files needed on a run-time translation server.  You can
then copy this structure to your run-time server by using a recursive copy
command that dereferences symbolic links.

\begin{small}
\begin{alltt}
   > \textbf{make portageLive}
   [...]
   [lots of prerequisite stuff happens]
   [...]
   make -C models portageLive
   mkdir -p portageLive
   [...]
   [lots of model conversion and preparation happens]
   [the portageLive directory structure is filled with the necessary symbolic links]
   [...]
   You now have all that is needed for PortageLive.
   From the framework root, run one of the following
   to transfer your PortageLive models:
   rsync -Larz models/portageLive/* <RHOST>:/<DEST_DIR_RHOST>
   scp -r models/portageLive/* <RHOST>:/<DEST_DIR_RHOST>
   cp -Lr models/portageLive/* /<DEST_DIR>
\end{alltt}
\end{small}

The three sample commands printed at the end of the execution show you how to
perform a deep copy of the structure created, which will expand symbolic links
during the copy.

You can use \code{du -hL} to find out the size that structure will have once
fully expanded:
\begin{small}
\begin{alltt}
   > \textbf{du -hL models/portageLive/}
   14M     models/portageLive/models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.tppt
   26M     models/portageLive/models/tm
   2.4M    models/portageLive/models/tc/nc1-lm.fr.tplm
   700K    models/portageLive/models/tc/tc-map.en.tppt
   2.0M    models/portageLive/models/tc/tc-lm.en.tplm
   5.0M    models/portageLive/models/tc
   5.7M    models/portageLive/models/lm/train_fr-kn-5g.tplm
   5.4M    models/portageLive/models/lm/train_en-kn-5g.tplm
   12M     models/portageLive/models/lm
   42M     models/portageLive/models
   42M     models/portageLive
\end{alltt}
\end{small}


\section{Resource Summary}

Now that the whole experiment has run, we can get a global summary of resource
usage from all the parts of the framework, as shown in Figure~\ref{FigTimeMem}.

\begin{sidewaysfigure}
\caption{Time and memory resource summary}
\label{FigTimeMem}
\begin{footnotesize}
\begin{alltt}
> \textbf{make time-mem}
Resource summary for $PORTAGE/toy.experiment:
         log.ce_model.cem:TIME-MEM                             WALL TIME: 30s      CPU TIME: 1m3s       VSZ: 0.110G   RSS: 0.012G
      confidence:TIME-MEM                                      WALL TIME: 30s      CPU TIME: 1m3s       VSZ: 0.110G   RSS: 0.012G
         log.canoe.ini.cow:TIME-MEM                            WALL TIME: 8m0s     CPU TIME: 18m38s     VSZ: 0.138G   RSS: 0.014G
      decode:TIME-MEM                                          WALL TIME: 8m0s     CPU TIME: 18m38s     VSZ: 0.138G   RSS: 0.014G
         log.train_en-kn-5g.binlm:TIME-MEM                     WALL TIME: 11s      CPU TIME: 10s        VSZ: 0.079G   RSS: 0.010G
         log.train_en-kn-5g.lm:TIME-MEM                        WALL TIME: 2s       CPU TIME: 2s         VSZ: 0.059G   RSS: 0.017G
         log.train_en-kn-5g.tplm:TIME-MEM                      WALL TIME: 10s      CPU TIME: 6s         VSZ: 0.074G   RSS: 0.011G
         log.train_fr-kn-5g.binlm:TIME-MEM                     WALL TIME: 15s      CPU TIME: 14s        VSZ: 0.040G   RSS: 0.006G
         log.train_fr-kn-5g.lm:TIME-MEM                        WALL TIME: 2s       CPU TIME: 3s         VSZ: 0.035G   RSS: 0.006G
         log.train_fr-kn-5g.tplm:TIME-MEM                      WALL TIME: 12s      CPU TIME: 6s         VSZ: 0.072G   RSS: 0.009G
      lm:TIME-MEM                                              WALL TIME: 52s      CPU TIME: 41s        VSZ: 0.079G   RSS: 0.017G
         log.rescore-model:TIME-MEM                            WALL TIME: 3m44s    CPU TIME: 13m57s     VSZ: 0.123G   RSS: 0.042G
      rescore:TIME-MEM                                         WALL TIME: 3m44s    CPU TIME: 13m57s     VSZ: 0.123G   RSS: 0.042G
         log.train_en-kn-3g.binlm:TIME-MEM                     WALL TIME: 4s       CPU TIME: 4s         VSZ: 0.079G   RSS: 0.009G
         log.train_en-kn-3g.lm:TIME-MEM                        WALL TIME: 1s       CPU TIME: 1s         VSZ: 0.054G   RSS: 0.012G
         log.train_en-kn-3g.tplm:TIME-MEM                      WALL TIME: 5s       CPU TIME: 2s         VSZ: 0.076G   RSS: 0.010G
         log.train_en.map:TIME-MEM                             WALL TIME: 0s       CPU TIME: 0s         VSZ: 0.078G   RSS: 0.009G
         log.train_en.map.tppt:TIME-MEM                        WALL TIME: 10s      CPU TIME: 0s         VSZ: 0.068G   RSS: 0.009G
         log.train_en.nc1:TIME-MEM                             WALL TIME: 2s       CPU TIME: 1s         VSZ: 0.079G   RSS: 0.010G
         log.train_en.nc1.binlm:TIME-MEM                       WALL TIME: 4s       CPU TIME: 4s         VSZ: 0.090G   RSS: 0.010G
         log.train_en.nc1.lm:TIME-MEM                          WALL TIME: 1s       CPU TIME: 1s         VSZ: 0.056G   RSS: 0.015G
         log.train_en.tokss:TIME-MEM                           WALL TIME: 3s       CPU TIME: 3s         VSZ: 0.048G   RSS: 0.008G
         log.train_fr.nc1.binlm:TIME-MEM                       WALL TIME: 10s      CPU TIME: 5s         VSZ: 0.090G   RSS: 0.010G
         log.train_fr.nc1.lm:TIME-MEM                          WALL TIME: 1s       CPU TIME: 1s         VSZ: 0.057G   RSS: 0.014G
         log.train_fr.nc1.tplm:TIME-MEM                        WALL TIME: 5s       CPU TIME: 2s         VSZ: 0.070G   RSS: 0.009G
         log.train_fr.tokss:TIME-MEM                           WALL TIME: 2s       CPU TIME: 2s         VSZ: 0.048G   RSS: 0.008G
      tc:TIME-MEM                                              WALL TIME: 48s      CPU TIME: 25s        VSZ: 0.090G   RSS: 0.015G
         log.cpt.merged.hmm3-rf-zn.train.fr2en:TIME-MEM        WALL TIME: 14s      CPU TIME: 17s        VSZ: 0.036G   RSS: 0.006G
         log.cpt.merged.hmm3-rf-zn.train.fr2en.tppt:TIME-MEM   WALL TIME: 15s      CPU TIME: 7s         VSZ: 0.049G   RSS: 0.007G
         log.hmm3.train.en_given_fr:TIME-MEM                   WALL TIME: 3m15s    CPU TIME: 7m45s      VSZ: 0.102G   RSS: 0.019G
         log.hmm3.train.fr_given_en:TIME-MEM                   WALL TIME: 3m33s    CPU TIME: 7m16s      VSZ: 0.124G   RSS: 0.018G
         log.ibm1.train.en_given_fr:TIME-MEM                   WALL TIME: 2m51s    CPU TIME: 1m54s      VSZ: 0.099G   RSS: 0.012G
         log.ibm1.train.fr_given_en:TIME-MEM                   WALL TIME: 2m57s    CPU TIME: 1m51s      VSZ: 0.102G   RSS: 0.012G
         log.ibm2.train.en_given_fr:TIME-MEM                   WALL TIME: 3m34s    CPU TIME: 1m57s      VSZ: 0.113G   RSS: 0.020G
         log.ibm2.train.fr_given_en:TIME-MEM                   WALL TIME: 2m47s    CPU TIME: 1m51s      VSZ: 0.113G   RSS: 0.014G
         log.jpt.hmm3.train.fr-en:TIME-MEM                     WALL TIME: 27s      CPU TIME: 44s        VSZ: 0.192G   RSS: 0.015G
         log.jpt.ibm2.train.fr-en:TIME-MEM                     WALL TIME: 30s      CPU TIME: 28s        VSZ: 0.192G   RSS: 0.015G
      tm:TIME-MEM                                              WALL TIME: 20m23s   CPU TIME: 24m10s     VSZ: 0.192G   RSS: 0.020G
   models:TIME-MEM                                             WALL TIME: 34m17s   CPU TIME: 58m54s     VSZ: 0.192G   RSS: 0.042G
      log.test.ce:TIME-MEM                                     WALL TIME: 32s      CPU TIME: 1m14s      VSZ: 0.148G   RSS: 0.025G
      log.test.out:TIME-MEM                                    WALL TIME: 25s      CPU TIME: 1m0s       VSZ: 0.107G   RSS: 0.011G
      log.test.out.pal:TIME-MEM                                WALL TIME: 0s       CPU TIME: 0s         VSZ: 0G       RSS: 0G
      log.test.out.tc:TIME-MEM                                 WALL TIME: 5s       CPU TIME: 1s         VSZ: 0.049G   RSS: 0.009G
      log.test.rat:TIME-MEM                                    WALL TIME: 2m14s    CPU TIME: 13m29s     VSZ: 0.123G   RSS: 0.027G
   translate:TIME-MEM                                          WALL TIME: 3m11s    CPU TIME: 15m43s     VSZ: 0.148G   RSS: 0.027G
TIME-MEM                                                       WALL TIME: 37m28s   CPU TIME: 1h14m37s   VSZ: 0.192G   RSS: 0.042G
\end{alltt}
\end{footnotesize}
\end{sidewaysfigure}

The output of \code{make time-mem} tells us how much RAM (``RSS'') and
virtual memory (``VSZ'') was used by each step of the process. It also tells us
the total amount of CPU time and the actual elapsed time (``wall time'').  When
running in parallel, the total CPU time will be higher than the wall time, as is
the case here.

In each summary line (the ones outdented, those for each component, and the
global summary at the end), the wall and CPU times are simply the sum of the
lines it summarizes, whereas the two memory figures are the maximum values from
the lines that it summarizes.

The memory figures might be a bit misleading: they reflect the maximum
RAM/virtual memory needed by any one component within a run.  In the case of a
process that is parallelized, say, 10 ways, the figure reflects the most memory
any one of the 10 parallel processes used, not the total used at any given
time.  When using this output to plan resources, you may need to multiply the
memory figures by the parallelism level you chose for each step.

The numbers here are not very interesting because we ran on such a small
system, but they'll be a lot more informative when you run on real data.

The second type of resource summary you can produce is the disk space occupied
by the models.  When you run \code{make summary}, you will first get the
\code{time-mem} report, then the following disk space report:
\begin{small}
\begin{alltt}
   > \textbf{make summary}
   [output from time-mem plus:]

   Disk usage for all models:
   4.7M    models/tm/ibm1.train.en_given_fr.gz
   4.6M    models/tm/ibm1.train.fr_given_en.gz
   3.7M    models/tm/ibm2.train.en_given_fr.gz
   148K    models/tm/ibm2.train.en_given_fr.pos.gz
   3.8M    models/tm/ibm2.train.fr_given_en.gz
   152K    models/tm/ibm2.train.fr_given_en.pos.gz
   4.0K    models/tm/hmm3.train.en_given_fr.dist.gz
   2.0M    models/tm/hmm3.train.en_given_fr.gz
   4.0K    models/tm/hmm3.train.fr_given_en.dist.gz
   2.1M    models/tm/hmm3.train.fr_given_en.gz
   3.5M    models/tm/jpt.hmm3.train.fr-en.gz
   1.3M    models/tm/jpt.ibm2.train.fr-en.gz
   3.8M    models/tm/jpt.merged.train.fr-en.gz
   12M     models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.gz
   14M     models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.tppt
   3.4M    models/lm/train_en-kn-5g.binlm.gz
   5.4M    models/lm/train_en-kn-5g.lm.gz
   5.4M    models/lm/train_en-kn-5g.tplm
   3.7M    models/lm/train_fr-kn-5g.binlm.gz
   5.9M    models/lm/train_fr-kn-5g.lm.gz
   5.7M    models/lm/train_fr-kn-5g.tplm
   340K    translate
   12K     models/confidence/ce_model.cem
   17M     models/tc
   98M     total

   Disk usage for portageLive models:
   14M     models/portageLive/models/tm/cpt.merged.hmm3-rf-zn.train.fr2en.tppt
   26M     models/portageLive/models/tm
   2.4M    models/portageLive/models/tc/nc1-lm.fr.tplm
   700K    models/portageLive/models/tc/tc-map.en.tppt
   2.0M    models/portageLive/models/tc/tc-lm.en.tplm
   5.0M    models/portageLive/models/tc
   5.7M    models/portageLive/models/lm/train_fr-kn-5g.tplm
   5.4M    models/portageLive/models/lm/train_en-kn-5g.tplm
   12M     models/portageLive/models/lm
   42M     models/portageLive/models
   42M     models/portageLive
\end{alltt}
\end{small}

This report differs from a simple \code{du -h} in that it looks specifically
for the model files. It also shows the size of the portageLive models prepared
for deployment on a run-time translation server.

\section{Final Note}
Because of differences in rounding, optimization, random number generation,
compilers, hardware, etc., results, especially numerical ones, are expected to
vary on different systems and are shown in this document only as an indication
of the type of output to expect, especially given the trivial size of the
corpus used.

\end{document}
