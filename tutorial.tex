\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper, top=1in, bottom=1in, left=1in, right=1.50in]{geometry}
\usepackage{isolatin1}
\usepackage{xspace}
\usepackage{pifont}
\usepackage{url}
\usepackage{rotating}
\usepackage{tabularx}

% Need straight quotes in verbatim text.
\usepackage{upquote}
\usepackage{textcomp}
\newcommand\upquote[1]{\textquotesingle#1\textquotesingle}
\usepackage{alltt}
% This is needed in order to get bold in tt.
\renewcommand{\ttdefault}{txtt}
\newcommand{\bs}{\textbackslash{}}

% Official typesetting of PORTAGEshared, now called Portage 1.x
\newcommand{\PS}{Portage 1.5\xspace}

% Try generating a PDF with coloured hyperlinks.
\newif\ifcolourlinks
%\colourlinksfalse
\colourlinkstrue

\ifcolourlinks
   \usepackage{color}
   \usepackage[colorlinks=true,
               pdftex,
               linktocpage,
               backref=page,
               pdftitle={Portage 1.5 Tutorial},
               pdfauthor={Joanis, Stewart, Larkin and Foster}
              ]{hyperref}
\else
   \usepackage{hyperref}
\fi



% \code formats an inline code snippet without line breaking; it treats the
% underscore as a normal character. Use \url to format a code snippet with
% automatic line breaking, but then underscores are not rendered as characters
% on which copy/paste works.
% \code breaks for text containing underscores when used inside a footnote;
% for \code calls inside footnotes, use \us{} to specify an underscore.
% Use \upquote{quote-text} for straight single quotes around text within a \code
% call.
\def\code{\begingroup\catcode`\_=12 \codex}
\newcommand{\codex}[1]{\texttt{#1}\endgroup}
\chardef\us=`\_

\newcommand{\phs}{\tild{s}}   % source phrase
\newcommand{\pht}{\tild{t}}   % target phrase
\newcommand{\tip}{\textbf{Useful Tip \large{\ding{43}} }}
\newcommand{\margintip}{\marginpar[{\textbf{Tip \large{\ding{43}}}}]{\textbf{\reflectbox{\large{\ding{43}}} Tip}}}
\newcommand{\tipsummary}{\noindent\textbf{Tip summary \large{\ding{43}} }}
\newcommand{\tipend}{\textbf{ \reflectbox{\large{\ding{43}}}}}

\usepackage{ifpdf}
\ifpdf
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\else
\fi

\title{\PS Tutorial: \\
       A toy experiment using the \\
       experimental framework}
\date{Last updated June 2012}
\author{Eric Joanis, Darlene Stewart, Samuel Larkin, George Foster}

\begin{document}

\vfill

\maketitle

\vfill

\begin{center}
An adaptation of George Foster's \emph{Running Portage: A Toy Example} \\
to Samuel Larkin's experimental framework,\\
%updated to reflect recommended usage of \PS.
updated to reflect recommended usage of Portage 1.5.0.
\end{center}

\vfill
\vfill

\begin{center}
{~} \\ \footnotesize
   Technologies langagi{\`e}res interactives /
      Interactive Language Technologies \\
   Technologies de l'information et des communications /
      Information and Communications Technologies \\
   Conseil national de recherches Canada /
      National Research Council Canada \\
   Copyright \copyright\ 2008--2012, Sa Majest{\'e} la Reine du Chef du Canada
   \\ Copyright \copyright\ 2008--2012, Her Majesty in Right of Canada
\end{center}

\vfill

\newpage

%\vfill

\tableofcontents

%\vfill

\newpage


\section{Introduction}

This document describes how to run an experiment from end to end using the \PS
experimental framework. It is intended as a tutorial on using \PS, as well as a
starting point for further experiments.  Although the framework automates most
of the steps described below, we go through them one by one here in order to
better explain how to use the \PS software suite.

\PS can be viewed as a set of programs for turning a bilingual corpus into a
translation system. Here this process is illustrated with a small ``toy''
example of French to English translation, using text from the Hansard corpus.
The training corpus is too small for good translation, but is used the same way
a more realistic setup would be. Total running time is one to several hours.

\subsection{Making Sure \PS is Installed}

To begin, you must build or obtain the \PS executables and ensure that they are
in your path, typically by sourcing the \code{SETUP.bash} file as
customized for your environment during installation of \PS.\footnote{There is
  also a \code{SETUP.tcsh} for users of that shell. The examples in
  this document assume the use of bash.} You should also
set your environment variable \code{\$PORTAGE} to the directory where \PS is
installed, which is also done by \code{SETUP.bash}.  Follow the
instructions in \code{INSTALL} before you proceed with this document.

To make sure \PS is installed properly, try \code{canoe -h}, \code{tune.py
-h}, \code{utokenize.pl -h}, \code{ce.pl -h}, and \code{filter-nc1.py -h}.  If
you get usage information for each of these programs, you should be ready to
proceed.  If one of them gives you an error message, then some part of your
installation is incomplete.  See the section \emph{Verifying your installation}
in \code{INSTALL} for troubleshooting suggestions.

\subsection{Running the Toy Experiment}

Once \PS is installed, you should make a complete copy of the framework
directory hierarchy, because it is designed to work in place, creating the
models within the hierarchy itself.  The philosophy of the framework is that
each experiment is done in a separate copy, where you might do various
customizations depending on what each experiment is intended to test.

For example:
\begin{small}
\begin{alltt}
   > \textbf{cd $PORTAGE}
   > \textbf{cp -pr framework toy.experiment}
   > \textbf{cd toy.experiment}
\end{alltt}
\end{small}

All commands provided in the rest of this document assume they are being run in
the \code{toy.experiment} directory or in a subdirectory thereof, which will
be specified relative to \code{toy.experiment}. Similary, whenever we quote a
\code{cd} command, it will be assumed to be executed from this
\code{toy.experiment} directory, not from the location of the previous
commands.

As you work through the example, the commands that you should type\footnote{This
   PDF document was generated in such a way that you can copy and paste commands
   from here onto the command line of your interactive shell if you wish.}
are shown in bold and preceded by a prompt, \code{>}, and the system's response
is not.  System output is not usually fully reproduced here, for brevity's
sake. When it is, results (especially numbers) may vary a little from the ones
shown, due to platform differences. Note that many results presented here are
truncated in precision for presentation purposes.

Many of the commands are expressed as \code{make} targets. This has the
advantage of requiring less typing, while still allowing you to see the actual
commands executed by the system because they are always echoed by \code{make}.
(You could also type them directly.) \code{make} also lets you skip
sections of this document (except for the first one, since it is done
manually). For example, if you are not interested in any steps before decoder
weight optimization, you can go directly to \S\ref{COW} and type
\code{make tune} to begin at that point. \code{make} will automatically run all
the commands required from previous sections before doing the step you
specifically requested. Here are some other useful \code{make} commands:
\begin{itemize}
\item \code{make all}: run all remaining steps at any point.
\item \code{make clean}: clean up and return the directory to its initial state
\item \code{make -j} \emph{target} or \code{make -j} \emph{N target}: build
      \emph{target} by running commands in parallel whenever possible (up to
      \emph{N} ways parallel if \emph{N} is specified). This lets you take
      advantage of a computing cluster if you have one. If you use a single
      multi-core computer, you don't need \code{-j}, since many commands in the
      framework are already internally parallelized, as discussed in
      \S\ref{FrameworkParams}.
\item \code{make help}: display some help and the main targets available in
      the makefile.
\item \code{make summary}: display the resources used by the framework: time
      and memory used, as well as disk space for the runtime models (most
      informative once training has been completed; discussed further in
      \S\ref{FrameworkParams}).
\end{itemize}

\subsection{Overview of the Process}

Here is an overview of the \PS process, as described in the following
sections:
\begin{enumerate}
\item Corpus preparation (\S\ref{CorpusPreparation}) includes
      tokenization,
      alignment,
      corpus splitting (\S\ref{Splitting}),
      and
      lowercasing (\S\ref{Lowercasing}).
\item Model training (\S\ref{Training}) includes
      language model (\S\ref{LM}),
      truecasing model (\S\ref{TC}),
      translation model (\S\ref{TM}),
      hierarchical lexicalized distortion model (\S\ref{LDM}),
      mixture models (\S\ref{MIX}),
      decoder weight optimization (\S\ref{COW}),
      confidence estimation model training (\S\ref{CE}),
      and
      rescoring model training (\S\ref{RAT}).
\item Translating and testing (\S\ref{TranslatingTesting}) includes
      decoding (\S\ref{Decoding}),
      confidence estimation(\S\ref{CETrans}),
      rescoring (\S\ref{RATTrans}),
      truecasing (\S\ref{Truecasing}),
      and
      testing (\S\ref{Testing}).
\end{enumerate}
These steps are fairly standard, but of course there are many variants on the
sample process illustrated here. For more information, see the \PS user manual
(found in \url{$PORTAGE/doc/user-manual.html} on the CD).
For detailed information about a particular program, run the program with the
\code{-h} flag (or see \url{$PORTAGE/doc/usage.html} on the CD).  


\section{Corpus Preparation} \label{CorpusPreparation}

Corpus preparation involves converting raw document pairs into tokenized,
sentence-aligned files in the format expected by Portage.  We provide a
tokenizer, a sentence aligner and sample corpus pre-processing scripts with
\PS, which can help you with these steps.  We also provide a tool to extract
text from a translation memory in TMX format.  For details, see section
\emph{Text Processing} in the user manual.

We don't actually perform any of the steps mentioned above in this toy example
because they are highly dependent on your actual data.  You should plan to
invest some time in preprocessing your data well if you want to obtain good
results with \PS.

\subsection{Splitting the Corpus} \label{Splitting}

The corpus, which we assume you have tokenized and sentence-aligned, as
discussed above, must be split into separate portions in order to run
experiments. Distinct, non-overlapping sub-corpora are required for model
training (see \S\ref{Training}), for tuning decoder weights (\S\ref{COW}) and
rescoring weights (\S\ref{RAT}), for confidence estimation (if you use it;
\S\ref{CE}), and for testing (\S\ref{Testing}).\footnote{In this example, we
use separate dev sets for tuning decoder and rescoring weights, but this is not
necessary.  However, confidence estimation must absolutely have its own
separate tuning set, which can be reused as a test set, but not as a decoder or
rescoring tuning set.} Typically, the tuning (or ``dev'', for development) and
testing sets are 1000 to 2000 segments each.  If the corpus is chronological,
then it is a good idea to choose these sets from the most recent material,
which is likely to resemble future text more closely.

Since proper splitting of a corpus must take into account its structure
and nature, these steps are not handled by the experimental framework. For the
toy experiment, we provide small data sets that can be found here:
\url{$PORTAGE/test-suite/tutorial-data}. These sets
are very small, to minimize running time, so the resulting translations are of
poor quality. In this toy experiment, the tuning and testing sets contain
just 100 segments each.

To drop these sets into the framework, copy them (or make symbolic
links) into your copy's corpora directory:
\begin{small}
\begin{alltt}
   > \textbf{cp $PORTAGE/test-suite/tutorial-data/*.al* corpora/}
   > \textbf{wc_stats corpora/*.al* | expand-auto.pl}
      #Lines   #Words   #Char     [...]   filename
      100      2115     11589     [...]   corpora/dev1_en.al
      100      2330     13680     [...]   corpora/dev1_fr.al
      100      2095     11462     [...]   corpora/dev2_en.al
      100      2443     13905     [...]   corpora/dev2_fr.al
      100      2383     13061     [...]   corpora/dev3_en.al
      100      2840     16209     [...]   corpora/dev3_fr.al
      8896     182273   981921    [...]   corpora/lm-train_en.al.gz
      8893     208404   1178207   [...]   corpora/lm-train_fr.al.gz
      100      2379     13292     [...]   corpora/test1_en.al
      100      2616     15622     [...]   corpora/test1_fr.al
      100      2383     13061     [...]   corpora/test2_en.al
      100      2840     16209     [...]   corpora/test2_fr.al
      8892     182186   981455    [...]   corpora/tm-train_en.al.gz
      8892     208400   1178176   [...]   corpora/tm-train_fr.al.gz

      36573    805687   4457849   [...]   TOTAL
      [...]
\end{alltt}
\end{small}

In your own experiments, the files you need to copy into \code{corpora}
should be plain text in original truecase (if you're using truecasing),
tokenized, sentence-split and aligned, just like the ones we provide here.

Although the framework does not support compressed dev and test files, the
training files can and should be compressed, as shown here.  Most programs and
modules in Portage transparently handle compressed files, compressing and
decompressing them on the fly as needed.

If you inspect them, you might notice that the \code{lm-train*} and
\code{tm-train*} files are almost identical, but that the \code{lm-train*} have
more text.  When training the language model, it is a good idea to add
available monolingual material to the target side of the parallel corpus, as we
simulated doing here.

\subsection{Setting Framework Parameters} \label{FrameworkParams}

Now you need to edit \code{Makefile.params} to set some global parameters:
\begin{itemize}
\item swap the values of \code{SRC_LANG} and
\code{TGT_LANG}, to select translation from French to English, rather than
the other way around, which is the default;
\item \code{TRAIN_LM} and \code{TRAIN_TM} already have the right values, so
they don't need to be changed;
\item set \code{TUNE_RESCORE} to \code{dev2} and \code{TUNE_CE} to \code{dev3}
by uncommenting the lines defining them, i.e., by removing the \code{\#} at the
beginning of these lines;
\item \code{TEST_SET} already points to our two test sets, so no change is
needed;
\item select a language modeling option (see below for more info about this
  choice): set the \code{LM_TOOLKIT} variable to \code{SRI} or \code{MIT} to
  use SRILM or MITLM, respectively.
%\item set \code{DO_RESCORING} to \code{1} by uncommenting the relevant line.
\end{itemize}
In this toy experiment, we will use the default value for all other parameters.

While in \code{Makefile.params}, you should read through all the variables in
the \emph{User definable variables} section of the file.  This is where most of
the configurable behaviours are set, such as whether to do rescoring and/or
truecasing, which optional models to use, the levels of parallelism, etc.

We recommend you use SRILM (\url{http://www.speech.sri.com/projects/srilm/}) as
your language modeling toolkit, if your licensing requirements permit it; if
not, we recommend MITLM (\url{http://code.google.com/p/mitlm/}), another
excellent LM toolkit, which allows commercial use.  Although not recommended,
IRSTLM remains another option.\footnote{These recommendations are based on our
empirical results: we get similar BLEU scores when using MITLM and SRILM, but
lower ones when using IRSTLM.} See \code{Makefile.params} and type \code{make
help} for more details.

Another set of parameters you might want to adjust are the various
\code{PARALLELISM_LEVEL_*} variables.  \PS is written in such a way as to take
advantage of multi-processor computers and/or multi-node computing clusters,
doing tasks in parellel where possible.  If you're not running on a cluster,
the number of CPUs on your machine is a good choice for all these variables
(this is the default).  If you are running on a cluster, the framework uses
\code{qsub} to submit jobs, via the \code{run-parallel.sh} and \code{psub}
scripts, and you can set these variables according to resources available to
you.

When running this framework, you might notice that many commands are preceded
by control variables \code{RP_PSUB_OPTS=...} or \code{_LOCAL=1}.  These strings
only have an impact when running on a cluster, and are ignored otherwise.  When
working on a cluster, commands preceded by \code{_LOCAL=1} are inexpensive ones
that get run directly instead of being submitted to the queueing system, while
\code{RP_PSUB_OPTS=...} is used to specify additional options to \code{psub},
which is used in \PS to encapsulate the invocation of \code{qsub}.  If your
cluster has specific usage rules or if you require additional parameters to
\code{qsub}, you might want to customize \code{psub} itself or add options as
required in this framework.

\tip\margintip Many commands run in this framework will also be preceded by
\code{time-mem}. This utility script measures the time taken by a command, just
like the standard \code{time} utility, as well as memory usage. At any time
while things are running, or afterwards, you can type \code{make summary} to
get a summary of resources used by all components of the framework. While things
are running, you may get some error messages, but these are safe to ignore: the
report should still reflect the situation so far.  The output of \code{make
time-mem} can be very useful to determine which steps are taking the most time
and/or the most memory.  They can help you determine if you have enough
computing resources to process your corpora.  They can also help you determine
the cost of various choices you can make in \PS.  The command \code{make
summary} will give you the \code{time-mem} information as well as the space on
disk of the models needed at runtime, such as would be deployed on a
translation server.\tipend

For the sake of brevity, when we quote executed commands in this document, we
only show the command itself, leaving out \code{time-mem} and the control
variables mentioned above.

Most commands in the framework produce logs called
\code{log.\emph{output-file-name}}.  If you encounter errors, look for
explanations in these log files, that's usually where the error messages will
end up.  Again, for brevity, we don't show these log files in this document.

\subsection{Lowercasing and Adding Escapes} \label{Lowercasing}

To reduce data sparseness, we convert all files to lowercase.\footnote{We show
\code{utf8\_casemap} being used.  If you installed Portage without ICU, you
will see \code{lc-utf8.pl} instead.} We keep the lowercase and truecase
versions separate, because we'll use the lowercase version to train language
and translation models, while use the truecase version will be used to train a
truecasing model.
\begin{small}
\begin{alltt}
   > \textbf{cd corpora}
   > \textbf{make lc}
   cat dev1_fr.al | utf8_casemap -c l > dev1_fr.lc
   [...]
   zcat lm-train_fr.al.gz | utf8_casemap -c l | gzip > lm-train_fr.lc.gz
\end{alltt}
\end{small}

The decoder, \code{canoe}, treats \code{<}, \code{>} and \verb*X\X as
special characters in order to support markup for special translation rules.
We won't use any markup in this example, but we still need to escape any
occurrences of the three special characters in the files we'll provide as input
to \code{canoe}: the source side of the dev and test files.

\begin{small}
\begin{alltt}
   > \textbf{make rule}
   canoe-escapes.pl -add < dev1_fr.lc > dev1_fr.rule
   canoe-escapes.pl -add < dev2_fr.lc > dev2_fr.rule
   canoe-escapes.pl -add < dev3_fr.lc > dev3_fr.rule
   canoe-escapes.pl -add < test1_fr.lc > test1_fr.rule
   canoe-escapes.pl -add < test2_fr.lc > test2_fr.rule
\end{alltt}
\end{small}


\section{Training} \label{Training}

This step creates various models and parameter files that are required for
translation. There are many steps in training.  Three are mandatory: creating a
language model (\S\ref{LM}), creating a translation model (\S\ref{TM}), and
optimizing decoder weights (\S\ref{COW}).  Several are optional: creating a
truecasing model (\S\ref{TC}), creating a (possibly hierarchical) lexicalized
distortion model (\S\ref{LDM}), using mixture language and translation models
for domain adaptation (\S\ref{MIX}), training a rescoring model (\S\ref{RAT}),
and training a confidence estimation (CE) model (\S\ref{CE}).

\subsection{Creating a Language Model} \label{LM}

\PS does not come with programs for language model training. However, it
accepts models in the widely-used ``ARPA'' format which is produced by most
language modelling toolkits.  The following sections describe the training
process for SRILM and MITLM.

By default, we train language models of order five.  We have found this is a
good compromise between translation quality and size of the models.  Higher
order language models might sometimes be useful, but only for very large
corpora, and at a cost in space and decoding speed.

In this toy example, we manually added a few sentences to the target language
part of the parallel training corpus to illustrate using more text to train the
language model than the translation model.  If you have access to relatively
small amounts of additional monolingual text, adding it to your main LM
training corpus is the simplest option.  If you have access to large amounts of
monolingual text in your target language, you can use it to train additional
language models, or mixture language models.  To train separate language
models, drop the corpora into \code{corpora} and list all the corpus stems in
\code{TRAIN\us{}LM}; if the LM is trained externally, add its name to
\code{LM\us{}PRETRAINED\us{}TGT\us{}LMS} instead.  For mixture language models,
see \S\ref{MIXLM}.

Commands below are assumed to be issued from the \code{toy.experiment}
directory, so run \code{cd ..} from the corpora directory if you have not
already done so.

\subsubsection{SRILM}

If you are using the SRILM toolkit, here is the command used to produce this
model:
\begin{small}
\begin{alltt}
   > \textbf{make lm}
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format lm-train_en-kn-5g.lm.gz
   ngram-count -interpolate -kndiscount -order 5 \bs
      -text ../../corpora/lm-train_en.lc.gz -lm lm-train_en-kn-5g.lm.gz
   arpalm2binlm lm-train_en-kn-5g.lm.gz lm-train_en-kn-5g.binlm.gz
\end{alltt}
\end{small}
The first command executed, \code{ngram-count}, produces the language model
itself in ``ARPA'' format.  The second command, \code{arpalm2binlm},
converts it into the Portage binary language model format for fast loading.


\subsubsection{MITLM}

If you are using the MITLM toolkit, here is the command used to produce the
model needed for this example, \code{lm-train_en-kn-5g.binlm.gz}:
\begin{small}
\begin{alltt}
   > \textbf{make lm}
   [...]
   make -C models lm
   make -C lm all LM_LANG=en
   Creating ARPA text format lm-train_en-kn-5g.lm.gz
   zcat ../../corpora/lm-train_en.lc.gz \bs
      | perl -ple 's/^{\bs}s*(.*?){\bs}s*$/$1/; s/{\bs}s+/ /g;' \bs
      | perl -ne 'BEGIN\{$M=4095\} $l=$_; while(length($l)>$M [...]' \bs
      | estimate-ngram -order 5 -smoothing ModKN -text /dev/stdin \bs
        -write-lm lm-train_en-kn-5g.lm.gz
   arpalm2binlm lm-train_en-kn-5g.lm.gz lm-train_en-kn-5g.binlm.gz
\end{alltt}
\end{small}
The first command executed, \code{estimate-ngram}, produces the language model
itself in ``ARPA'' format.  The first \code{perl} filter that precedes it
removes extraneous whitespace, because \code{estimate-ngram} is picky about its
input.  The second \code{perl} filter cuts lines that are longer than 4KB at
the last word boundary before this internal limit of MITLM's, to avoid breaking
words (or worse, utf-8 characters) apart.  The second command,
\code{arpalm2binlm}, converts the model into the Portage binary language model
format for fast loading.


\subsection{Creating a Truecasing Model} \label{TC}

Decoding is done in lowercase to reduce data sparseness, but at the end of the
translation process, you will probably want to restore proper mixed case to
your output.  We call this step truecasing.  To train a truecasing model, you
need both a lowercased version and the original ``truecase'' version of the
training corpus in the target language.

The basic truecasing model consists of two different models: a ``casemap'',
which maps each lower case word to its possible truecase variants, as observed
in the training corpus, and a standard language model trained on the truecase
corpus.

Since version 1.4.3, we use a variant of truecasing which carries casing
information from the source sentence to the target sentence, including the
casing for the first word in the sentence, the casing of out-of-vocabulary
words (OOVs), and unusual casing for sequences of several words (such as all
caps).

Truecasing with source information is the default.\footnote{If you want to
use basic truecasing, as in versions of Portage before 1.4.3, comment out the
line \code{TC\_USE\_SRC\_MODELS=1} in the advanced configuration section of
\code{Makefile.params}.}  It requires three language models: source-side and
target-side case-normalizing\footnote{``Normalized case'' means that the first
character of the sentence is in lowercase unless the word inherently requires
the upper case.  E.g., ``his flat in is London.'' and ``London is on the
Thames.'' are in normalized case.  The acronym ``nc1'', used in some file
names, stands for ``normalized-cased first word'', i.e., normalized case.}
language models, as well as the main truecasing LM.  The target-side
case-normalizing LM is used only for generating a normalized-case target-side
training corpus; it is not used in translation.  The main truecasing LM and the
``casemap'' are trained on the normalized-case target-side training corpus.
Together, they are used to determine the case for the output of the decoder.
Then additional case information from the source sentence, including the first
word, is transfered to that truecased output of the decoder.

Running \code{make tc} at the root of the framework will build all necessary
files and models.  Breaking with the convention in rest of this document, we'll
interleave the commands executed by make and our comments about them, to
improve readability.

\begin{small}
\begin{alltt}
   > \textbf{make tc}
   make -C models tc
   make -C tc all
   zcat -f ../../corpora/lm-train_en.tc.gz \bs
        | perl -ne 'use encoding "UTF-8"; s/^[^[:lower:]]+$/{\bs}n/; print $_ unless /^$/;' \bs
        | utokenize.pl -pretok -paraline -ss -p -lang=en \bs
        | gzip > lm-train_en.tokss.gz
\end{alltt}
\end{small}

We first generate \code{lm-train_en.tokss.gz}, a variant of the target-side
training corpus with all-caps sentences (which are misleading for training
truecasing models) removed and sentence splitting re-done.  Sentence splitting
is re-applied because corpus text from some sources such as TMX files may
contain lines with multiple sentences, and beginning-of-sentence detection is
important for case normalization.

\begin{small}
\begin{alltt}
   zcat -f lm-train_en.tokss.gz | filter-nc1.py -enc UTF-8 \bs
        | reverse.pl | gzip > lm-train_en.revtokss.gz
\end{alltt}
\end{small}

Second, we generate \code{lm-train_en.revtokss.gz}, an inversion of the
target-side training corpus needed to train the target-side case-normalizing
LM.

\begin{small}
\begin{alltt}
   make -f ../lm/Makefile CORPUS_EXT=.revtokss.gz CORPORA_DIR=../tc LM_DESC=.nc1 \bs
        lm-train_en.nc1.binlm.gz
   [commands to train case-normalizing target-language LM lm-train_en.nc1.binlm.gz]
\end{alltt}
\end{small}

Third, we internally re-invoke \code{make} to train a language model (as in
\S\ref{LM}) on this permuted corpus, producing \code{lm-train_en.binlm.gz},
the target-side case-normalizing language model.  The actual commands use
depend on your LM toolkit.

\begin{small}
\begin{alltt}
   normc1 -ignore 1 -extended -notitle -loc en_CA.UTF-8 lm-train_en.nc1.binlm.gz \bs
        lm-train_en.tokss.gz \bs
        | perl -pe 's/(.)$/$1 /; s/(.){\bs}n/$1/' | gzip > lm-train_en.nc1.gz
\end{alltt}
\end{small}

Next, \code{normc1} uses the case-normalizing LM to produce
\code{lm-train_en.nc1.gz}, the normalized-case target-side training corpus.

\begin{small}
\begin{alltt}
   zcat -f lm-train_en.nc1.gz |  utf8_casemap -c l \bs
        | compile_truecase_map lm-train_en.nc1.gz - > lm-train_en.map
\end{alltt}
\end{small}

Here we generate the casemap for the main target-side truecasing model,
\code{lm-train_en.map}, using \code{compile_truecase_map}, which compiles the
casemap by processing the normalized-case and lowercase versions of the corpus
simultaneously, and recording, for each lower case word, all the cased variants
found in the normalized-case file, along with their distribution.

\begin{small}
\begin{alltt}
   make -f ../lm/Makefile CORPUS_EXT=.nc1.gz CORPORA_DIR=../tc lm-train_en-kn-3g.binlm.gz
   [commands to train truecasing LM lm-train_en-kn-3g.binlm.gz]
\end{alltt}
\end{small}

Then, we invoke \code{make} again, this time to produce
\code{lm-train_en-kn-3g.binlm.gz}, the main target-size truecasing LM, trained on
the normalized-case corpus.

\begin{small}
\begin{alltt}
   zcat -f ../../corpora/lm-train_fr.al.gz \bs
        | utokenize.pl -pretok -paraline -ss -lang=fr \bs
        | perl -pe 'use encoding "UTF-8"; s/^[^[:lower:]]+($|( : ))//;' \bs
        | gzip > lm-train_fr.tokss.gz
   zcat -f lm-train_fr.tokss.gz | filter-nc1.py -enc UTF-8 \bs
        | reverse.pl | gzip > lm-train_fr.revtokss.gz
   make -f ../lm/Makefile CORPUS_EXT=.revtokss.gz CORPORA_DIR=../tc LM_DESC=.nc1 \bs
        LM_LANG=fr lm-train_fr.nc1.binlm.gz
   [commands to train case-normalizing source-language LM lm-train_fr.nc1.binlm.gz]
\end{alltt}
\end{small}

The final three commands do the same thing as the first three, this time to
produce the source-side case-normalizing language model needed by the new
truecasing workflow, \code{lm-train_fr.nc1.binlm.gz}.

\subsection{Creating a Translation Model} \label{TM}

Creating a translation model involves two main steps: 1) training IBM2 and HMM word
alignment models in both directions, and optionally IBM4, then 2) using them to
extract phrase pairs from the corpus.

There are many ways to combine the counts obtained from different alignments.
Through ongoing experimentation, our recommendations have changed in the past
and will likely change again; the best setup depends on your data and
resources, but we try to maintain a good general setup as the default in the
framework.

Here we illustrate the method we currently recommend: merge the counts from all
the alignment methods to estimate the main probability feature, using one
alignment method to estimate a lexical smoothing feature.
You can do all this by typing \code{make tm} in your \code{toy.experiment}
directory, but we will break it down into several steps here.

By default, \code{make tm} will only include IBM2 (\S\ref{IBM2}) and HMM
(\S\ref{HMM}) word-alignment models.  In \S\ref{IBM4}, we'll show how you can
include a IBM4 word-alignment model and tally its counts along with the other
two methods.

In \S\ref{PI}, we'll show how you can also produce alignment indicator features
telling the system which aligner produced which phrase pairs.  The alignment
indicator features are helpful because the different alignments make different
kinds of error in the phrase pairs they produce; the high diversity of phrase
pairs obtained from two or three separate alignment methods helps the system,
while the indicator features allow the system to learn to give more weight to
alignments suggested by the more reliable alignment method.  Now, we don't need
to know how reliable each method is, because the system will learn that
automatically during tuning (see \S\ref{COW}).

In \S\ref{MIXTM}, finally, we'll look at interpolating probability estimates
coming from various corpora, in a way that is adapted to your in-domain
material.

\subsubsection{Creating a Translation Model Using IBM2 Alignments} \label{IBM2}

\subsubsection*{Training IBM2 Models}

First we train IBM2 word alignment models, which requires training IBM1 models
as a prerequisite.  We do this for both directions.
\begin{small}
\begin{alltt}
   > \textbf{cd models/tm}
   > \textbf{make ibm2_model}
   cat.sh -n 4 -pn 4 -v -n1 5 -n2 0 -bin ibm1.tm-train.en_given_fr.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
   cat.sh -n 4 -pn 4 -v -r -n1 5 -n2 0 -bin ibm1.tm-train.fr_given_en.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \bs
      -bin -i ibm1.tm-train.en_given_fr.gz ibm2.tm-train.en_given_fr.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -slen 20 -tlen 20 -bksize 20 \bs
      -bin -i ibm1.tm-train.fr_given_en.gz ibm2.tm-train.fr_given_en.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
\end{alltt}
\end{small}
The \code{train_ibm} program could have been used directly, but we use
\code{cat.sh} instead, which runs \code{train_ibm} in parallel. Here the
options \code{-n 4 -pn 4} mean 4-ways parallel: the default is the use the
number of CPUs available, as controlled by the \code{PARALLELISM_LEVEL_TM}
variable in \code{Makefile.params}.

The commands above write log files containing information about training.  For
example, the IBM log files show convergence and pruning statistics for each
iteration:
\begin{small}
\begin{alltt}
   > \textbf{grep -h ppx log.ibm[12]*en_given_fr}
   parallel iter (IBM1): prev ppx = 351.448, size = 1402947 word pairs.
   parallel iter (IBM1): prev ppx = 108.875, size = 1332170 word pairs.
   parallel iter (IBM1): prev ppx = 66.4431, size = 1178494 word pairs.
   parallel iter (IBM1): prev ppx = 52.8119, size = 1015983 word pairs.
   parallel iter (IBM1): prev ppx = 47.9137, size = 874944 word pairs.
   parallel iter (IBM2): prev ppx = 45.759, size = 874673 word pairs.
   parallel iter (IBM2): prev ppx = 29.4061, size = 874333 word pairs.
   parallel iter (IBM2): prev ppx = 23.4947, size = 850139 word pairs.
   parallel iter (IBM2): prev ppx = 20.8628, size = 741848 word pairs.
   parallel iter (IBM2): prev ppx = 19.5686, size = 617840 word pairs.
\end{alltt}
\end{small}

The log files also contain time and memory resource information, which you can
summarize at any level just as you can globally:
\begin{footnotesize}
\begin{alltt}
   > \textbf{make time-mem}
   find -type f -name log.\* | sort | xargs time-mem-tally-pl -no-dir -m tm \bs
      | second-to-hms.pl | expand-auto.pl
      log.ibm1.tm-train.en_given_fr:TIME-MEM  WALL TIME: 1m3s   CPU TIME: 28s   VSZ: 0.102G  RSS: 0.012G
      log.ibm1.tm-train.fr_given_en:TIME-MEM  WALL TIME: 1m0s   CPU TIME: 28s   VSZ: 0.099G  RSS: 0.013G
      log.ibm2.tm-train.en_given_fr:TIME-MEM  WALL TIME: 57s    CPU TIME: 24s   VSZ: 0.099G  RSS: 0.012G
      log.ibm2.tm-train.fr_given_en:TIME-MEM  WALL TIME: 58s    CPU TIME: 24s   VSZ: 0.099G  RSS: 0.014G
   tm:TIME-MEM                                WALL TIME: 3m58s  CPU TIME: 1m44s VSZ: 0.102G  RSS: 0.014G
\end{alltt}
\end{footnotesize}

The IBM models are written to files \code{ibm[12].*} and contain word
translation/alignment probabilities.

\subsubsection*{Creating the alignment file}

Now we use the alignment model to produce and save the alignment of the
training corpus according to the IBM2 alignment method.  We did not use to save
these, because they are reasonably cheap to reproduce for IBM2, and we did not
usually reuse them anyway, but for HMM and other alignment models, they are
expensive to reproduce and, furthermore, we now reuse the alignments for other
purposes, so it's worth saving them.

\begin{small}
\begin{alltt}
   > \textbf{make ibm2_jpt}
   align-words -o sri -ibm 2 ibm2.tm-train.en_given_fr.gz  ibm2.tm-train.fr_given_en.gz \bs
      ./../../corpora/tm-train_fr.lc.gz ./../../corpora/tm-train_en.lc.gz \bs
      | gzip > tm-train.ibm2.fr2en.align.gz
   [...]
\end{alltt}
\end{small}

\subsubsection*{Training Joint-Count Phrase Tables}

With the alignment saved, extracting the joint-count phrase table (or JPT) from
the parallel corpus is reasonably quick.  The make command issued above should
have also launched this:
\begin{small}
\begin{alltt}
   gen-jpt-parallel.sh -n 4 -nw 4 -o jpt.ibm2.tm-train.fr-en.gz -w 1 \bs
      GPT -v -m 8 -1 fr -2 en -ext
      ibm2.tm-train.en_given_fr.gz ibm2.tm-train.fr_given_en.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz \bs
      tm-train.ibm2.fr2en.align.gz
\end{alltt}
\end{small}
The JPT contains the co-occurrence frequency for each phrase pair observed in
the parallel corpus.

\subsubsection*{Training Conditional Phrase Tables}

We used to train conditional phrase tables separately from the IBM2 and the HMM
JPT, but now we merge them first, so we'll come back to this step after doing
the HMM models.

\subsubsection{Creating a Translation Model Using HMM Alignments} \label{HMM}

Now we repeat the same steps using HMM word alignment models, using the
``hmm3'' variant, which is the default.  This variant corresponds to the HMM
parameter settings we recommend using to create translation models.  If you
inspect the alignments, you will find that ``hmm3'' alignment leave a lot of
words unaligned and seem to be of rather poor quality.  They were tuned
specifically to maximize BLEU scores on a complete translation system, not for
independent use.  If you want to produce alignments for display or other uses
that exploit the alignment directly, we recommend the ``hmm2'' or ``hmm1''
variant instead.

\begin{small}
\begin{alltt}
   > \textbf{make hmm3_jpt}
   cat.sh -n 4 -pn 4 -v -n1 0 -n2 5 -hmm -end-dist -anchor \bs
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \bs
      -i ibm1.tm-train.en_given_fr.gz hmm3.tm-train.en_given_fr.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
   cat.sh -n 4 -pn 4 -v -r -n1 0 -n2 5 -hmm -end-dist -anchor \bs
      -max-jump 20 -alpha 0.0 -lambda 1.0 -p0 0.6 -up0 0.5 -bin \bs
      -i ibm1.tm-train.fr_given_en.gz hmm3.tm-train.fr_given_en.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz
   align-words -o sri -hmm hmm3.tm-train.en_given_fr.gz hmm3.tm-train.fr_given_en.gz \bs
      ./../../corpora/tm-train_fr.lc.gz ./../../corpora/tm-train_en.lc.gz \bs
      | gzip > tm-train.hmm3.fr2en.align.gz
   gen-jpt-parallel.sh -n 4 -nw 4 -o jpt.hmm3.tm-train.fr-en.gz -w 1 \bs
      GPT -v -m 8 -hmm -1 fr -2 en -ext \bs
      hmm3.tm-train.en_given_fr.gz hmm3.tm-train.fr_given_en.gz \bs
      ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz \bs
      tm-train.hmm3.fr2en.align.gz
\end{alltt}
\end{small}

There are many variants of the HMM models.  In this framework, we've included
recipes to produce three of them: one with lexically conditioned jump
parameters, following He (WMT-2006), invoked via the \code{hmm1_*} targets;
one similar to the baseline described in Liang et al (ACL 2006), invoked via
the \code{hmm2_*} targets; and one tuned in-house to maximize BLEU scores on
French-English material, invoked via the \code{hmm3_*} targets.  Many other
variants are available, as documented by \code{train_ibm -h}. To select one or
more of them, you'll need to modify \url{models/tm/Makefile} and
\url{models/tm/Makefile.toolkit}. It's probably easier to modify the commands
associated with the \code{hmm1_model}, \code{hmm2_model} or \code{hmm3_model}
targets than to create new targets.

To modify the default models generated, change the \code{PT_TYPES} variable
in \url{toy.experiment/Makefile.params} to, for instance, \code{ibm2_cpt
hmm1_cpt}: this variable enumerates which models you want to generate when you
type \code{make all}. All models you generate will be used in subsequent
steps, so generate only the ones you intend to use. (You should work in
different copies of the framework if you want to experiment with different
settings.)

\subsubsection{Creating a Conditional Phrase Table}

Now that we have JPTs from our IBM2 and HMM
word-alignment models, we merge their counts together in a single JPT.
\begin{small}
\begin{alltt}
   > \textbf{make jpt.merged.tm-train.fr-en.gz}
   merge_counts jpt.merged.tm-train.fr-en.gz \bs
      jpt.ibm2.tm-train.fr-en.gz jpt.hmm3.tm-train.fr-en.gz
\end{alltt}
\end{small}

Finally we compute the conditional phrase table (or CPT), using two different
smoo\-thers to calculate the probabilities.\footnote{In general, we obtain
better translation quality when using phrase probabilities estimated from
occurrence counts (relative frequency, ``RFSmoother'', or a smoothed variant
such as ``KNSmoother'' or ``GTSmoother'') along with probability estimates
calculated using Zens-Ney lexical smoothing (``ZNSmoother''). Other smoothing
options are also available. Results in the past couple years have shown that
using Kneser-Ney smoothing with Zens-Ney smoothing, without the relative
frequency estimate, gives the best results in a wide range of cases.  This is
now the default.  The variables \code{CPT\us{}SMOOTHERS} and
\code{SMOOTHERS\us{}DESCRIPTION} in \code{models/tm/Makefile.params} control
the choice of smoothers.}
Although stored together in the same phrase table, they are separate probability
models, whose relative weights will be tuned during decoder weight optimization
(see \S\ref{COW}).
\begin{small}
\begin{alltt}
   > \textbf{make merged_cpt}
   joint2cond_phrase_tables -prune1 100 -v -i -z -hmm -1 fr -2 en \bs
      -s "KNSmoother 3" -s ZNSmoother -multipr fwd -o cpt.merged.hmm3-kn3-zn.tm-train \bs
      -ibm_l2_given_l1 hmm3.tm-train.en_given_fr.gz \bs
      -ibm_l1_given_l2 hmm3.tm-train.fr_given_en.gz \bs
      -reduce-mem -no-sort jpt.merged.tm-train.fr-en.gz
\end{alltt}
\end{small}
The phrase table \code{cpt.merged.hmm3-kn3-zn.tm-train.fr2en.gz} is the main source
of information for French to English translation. Each of its lines is of the
form:
\begin{alltt}
   \emph{fr} ||| \emph{en} ||| p1(\emph{fr}|\emph{en}) p2(\emph{fr}|\emph{en}) p1(\emph{en}|\emph{fr}) p2(\emph{en}|\emph{fr})
\end{alltt}
%
where \code{\emph{fr}} is a French source phrase, \code{\emph{en}} is an
English target phrase, \code{p(\emph{fr}|\emph{en})} is the probability that
\code{\emph{fr}} is the translation of \code{\emph{en}} (the ``backward''
probability), and \code{p(\emph{en}|\emph{fr})} is the probability that
\code{\emph{en}} is the translation of \code{\emph{fr}} (the ``forward''
probability). In our example here, \code{p1} is the relative-frequency estimate
of this probability, while \code{p2} is the Zens-Ney lexical smoothing based
estimate: those are the smoothers we recommend using. There can be any number
of backward and forward probability estimates, reflecting the number of
smoothers you use.
%
Here are two sample lines from this file:
\begin{small}
\begin{alltt}
   > \textbf{zgrep '| proposed regulations |' cpt.merged.hmm3-kn3-zn.tm-train.fr2en.gz}
   projets de règlement ||| proposed regulations ||| 0.55 0.0016 0.82 0.026
   règlements proposés ||| proposed regulations ||| 0.33 0.018 1 0.42
\end{alltt}
\end{small}
Note that this example is carefully chosen; because of the small size of the
training corpus, many other entries in the phrase table are not such good
translations.

\subsubsection{Creating a Translation Model Using IBM4 Alignments} \label{IBM4}

Recent experiments have shown that is it beneficial to combine the JPTs
obtained from IBM2 and HMM models with an additional one obtained from IBM4
models.  \PS does not include software to train or use IBM4 models, however, so
if you wish to use this option, you must independently obtain Giza (or,
preferebaly, the MGiza++ variant, which the framework is configured to
support) and make sure it is found on your PATH.

\begin{small}
\begin{alltt}
   > \textbf{make ibm4_jpt}
   [lots of commands get run]
\end{alltt}
\end{small}
%
Running \code{make ibm4_jpt} in \code{models/tm} will invoke MGiza++ to train
IBM4 models and align the training corpus using it, performing necessary
conversions before and after to ensure compatibility with \PS software.  Our
own \code{align-words} and \code{gen-jpt-parallel.sh} are then used to produce
a standard symmetrized alignment file and extract phrase pairs from it,
finishing with the same procedure as with IBM2 and HMM models.

If you wish to build and use IBM4 translation models, you should edit
\code{Makefile.params} at the root of the framework, and add \code{IBM4} to the
\code{MERGED_CPT_JPT_TYPES} variable, along with the \code{HMM3} and
\code{IBM2} values, which you will find already there.

When you create IBM4-based JPTs, their counts will be merged along
with IBM2's and HMM's to create the merged CPT, as described above.

\subsubsection{Adding alignment indicator features} \label{PI}

In our NIST OpenMT 2012 system, we found that the decoder could benefit from
knowing which alignment method suggested which phrase pairs.  With the new
lattice MIRA tuning method (which is now the default), we're able to throw more
features at the system than before, such as the alignment indicator features,
and it is able to exploit them to improve the quality of the translations
produced (as measured by BLEU).  Unlike the Powell tuning algorithm,
MIRA can exploit a large number of features effectively without overfitting the
dev set.

The alignment indicator features are simply flags, one per alignment method.
To enable generating them, you can set
\code{MERGED_CPT_USE_ALIGNMENT_INDICATORS} to \code{1} in the top-level
\code{Makefile.params}.  Having done so, the default merged CPT file
(\code{cpt.merged...}) will not be generated, instead you will get a
\code{cpt.PI...} file with identical contents in the first three columns, plus
a fourth column containing two or three values (one per aligner), with values 1
(the alignment produced the phrase pair) or 0.3 (otherwise).  The 0.3 works as
a constant arbitrary penalty that gets applied whenever a given aligner did not
produce a given phrase pair.  The penalty value is arbitrary here in the phrase
table, but its weight gets tuned so as to maximize BLEU scores on your dev set
later.

To generate the CPT with alignment indicator features at this point, you can
run \code{make indicator_cpt}:
\begin{small}
\begin{alltt}
   > \textbf{make indicator_cpt}
   mkdir -p j2m/ibm4
   cd j2m/ibm4 && ln -s -f ../../jpt.ibm4*fr-en.gz .
   mkdir -p j2m/hmm3
   cd j2m/hmm3 && ln -s -f ../../jpt.hmm3*fr-en.gz .
   mkdir -p j2m/ibm2
   cd j2m/ibm2 && ln -s -f ../../jpt.ibm2*fr-en.gz .
   joint2multi_cpt -prune1w 100 -v -i -z -1 fr -2 en \bs
      -s '0:KNSmoother 3' -s ZNSmoother -a '1-100:PureIndicator' \bs
      -dir fwd -o cpt.PI-kn3-zn.hmm3 \bs
      -ibm_l2_given_l1  hmm3.tm-train.en_given_fr.gz \bs
      -ibm_l1_given_l2  hmm3.tm-train.fr_given_en.gz \bs
      j2m/ibm4 j2m/hmm3 j2m/ibm2
\end{alltt}
\end{small}

The \code{joint2multi_cpt} program used above is a versatile tool for
generating phrase tables with all sorts of features.  It can produce anything
that \code{joint2cond_phrase_table} can, plus additional features that reflect
the provenance of each phrase pair.  Here we use it with just one particular
setting that has been proved to be effective empirically.

Important warning: if you've created both the \code{cpt.merge...} and
\code{cpt.PI...} files in the same instance of the framework, the tuning step
will use the both, which is not a good idea.  You should remove one or the
other before proceding:
\begin{small}
\begin{alltt}
   > \textbf{rm cpt.merged.*}
\emph{or}
   > \textbf{rm cpt.PI*}
\end{alltt}
\end{small}
You will need to work in two separate copies of the framework if you want to
experiment with both of these methods.

\subsection{Creating a Hierarchical Lexicalized Distortion Model} \label{LDM}

Hierarchical Lexicalized Distortion Models (HLDMs) are not used by default in this
framework.  If you want to enable their creation and use, you should set
\code{USE_HLDM} to \code{1} in \code{Makefile.params} (by removing the initial
\code{\#} to uncomment the appropriate line).

The framework also support using regular (non-hierarchical) Lexicalized
Distortion Models (LDMs), but recent experiments have shown that HLDMs are much
more effective, so we recommend using those instead.  The key difference between
the two variants is how one defines the different distortion configurations.
HLDMs use a more natural definition that overcomes some of the weaknesses of
LDMs.

HLDMs and LDMs are created in two steps: 1) counting occurrences of the
different types of distortion instances over the training corpus, using the
program \code{dmcount}, and 2) estimating scores from these counts, using the
program \code{dmestm}.  If you trained more than one phrase table, as we did
above, the first step is repeated for each phrase table.
\begin{small}
\begin{alltt}
   > \textbf{cd models/ldm}
   > \textbf{make all}
   parallelize.pl -nolocal -psub -1 -n 4 -np 4 -w 100000 \bs
      -s ../../corpora/tm-train_fr.lc.gz -s ../../corpora/tm-train_en.lc.gz \bs
      -s ../tm/tm-train.hmm3.fr2en.align.gz \bs
      -merge 'merge_multi_column_counts -' \bs
      'dmcount -ext -v -m 8 -hier \bs
         ../../corpora/tm-train_fr.lc.gz ../../corpora/tm-train_en.lc.gz \bs
         ../tm/tm-train.hmm3.fr2en.align.gz > hldm.hmm3.counts.gz'
   parallelize.pl [...] 'dmcount [...] > hldm.ibm2.counts.gz'
   parallelize.pl [...] 'dmcount [...] > hldm.ibm4.counts.gz'
   merge_multi_column_counts -fillup - \bs
      hldm.ibm4.counts.gz hldm.hmm3.counts.gz hldm.ibm2.counts.gz \bs
      | dmestm -s -g hldm.ibm4+hmm3+ibm2.fr2en.bkoff \bs
        -wtu 5 -wtg 5 -wt1 5 -wt2 5 \bs
      | gzip > hldm.ibm4+hmm3+ibm2.fr2en.gz
\end{alltt}
\end{small}

Estimating lexicalized distortion parameters (step 2, \code{dmestm}) might
require a lot of memory, sometimes two to three times as much as training
phrase tables.  As mentioned before, you can use \code{make time-mem} to see
your resource usage after the process has completed to plan your resource
allocation.

We generally observe a fair gain in translation quality (as measured by BLEU)
when using HLDMs, although you will need to determine, depending on your
situation, whether this gain is worth the added memory and time requirements.

If you need to reduce the memory footprint of \code{dmestm}, you can turn on
singleton filtering, which will remove from the count files all phrase pairs
that occur only once, before doing the estimation.  As a result, you will need
a lot less memory, but the model might not be as good, but it's a trade-off that
you can measure empirically on your data.  To turn on singleton filtering, edit
\code{models/ldm/Makefile.params} and uncomment the line defining
\code{HLDM_FILTER_SINGLETONS}.

\tip\margintip The commands for step 1 illustrate the use of one of our generic utilities:
\code{parallelize.pl}.  This script can be used to parallelize the execution
of any command where each input line is processed independently.  It takes care
of splitting the input(s) into chunks, running the chunks in parallel, and
concatenating (or otherwise merging) the outputs.  In this example the output
from all instances of \code{dmcount} are merged using
\code{merge_multi_column_counts}, a tool that adds counts together when it sees
the same key in two files.  The default is to concatenate the parts
together, which is appropriate for programs that produce one line of output per
line of input.  Run \code{parallelize.pl -h} for more details.\tipend

\subsection{Mixture Models and Domain Adaptation} \label{MIX}

In the previous sections, we have always assumed that the training data for
language and translation models consists of a single corpus.  This is often not
realistic: you may have data that comes from different domains, for example
data from web material vs. newswire, or from different translation memories.
You might also have a large amount of generic data that you want to combine
with smaller, in-domain data.  There are several ways to combine these corpora
into a system that is adapted to a particular domain while taking advantage of
data from other domains.

In this section, we discuss the use of mixture language and translation models.
Mixture models use a linear combination of their component models, which means
they will give a reasonably high scores to translation hypotheses that at least
one of their components likes.  The simple linear combination allows models to
be used in a kind of fall-back way: if the in-domain model knows the text and
likes it, we accept it, but if the in-domain model is faced with unseen
material, we'll still trust the other models and accept text they find
acceptable.  In contrast, you could also combine multiple LMs and TMs in the
log-linear model: in that case, the models all have to agree that a hypothesis
is good before it gets an overall good score.  Thus, the log-linear combination
is more like a system where each model has a veto on all decisions.\footnote{Of
course, this characterisation is over-simplistic: decoder weight tuning
(\S\ref{COW}) still adjusts how much the decoder will care about each model's
opinions.}

We assume that you have three domains or translation memories that you will use
together to create a system specialized for the first domain.  The first set of
training files, as well as the dev and test sets, should come from the primary
domain (in-domain text), while the other training sets are from the other two
domains (out-of-domain text).  We assume that new text you plan to submit to
this system will also be in domain.

To simulate this scenario, we've added three tiny corpora to
\url{$PORTAGE/test-suite/tutorial-data/mix}.
They don't actually come from different domains here, but we'll pretend they do
for the sake of our tutorial, and we'll pretend only the first one is in
domain.  You should copy these files into your \code{corpora} directory, adding
them to the files already there:
\begin{small}
\begin{alltt}
   > \textbf{cp $PORTAGE/test-suite/tutorial-data/mix/*.al* corpora/}
   > \textbf{ls corpora/*train?_*}
   corpora/lm-train1_en.al.gz  corpora/tm-train1_en.al.gz
   corpora/lm-train1_fr.al.gz  corpora/tm-train1_fr.al.gz
   corpora/lm-train2_en.al.gz  corpora/tm-train2_en.al.gz
   corpora/lm-train2_fr.al.gz  corpora/tm-train2_fr.al.gz
   corpora/lm-train3_en.al.gz  corpora/tm-train3_en.al.gz
   corpora/lm-train3_fr.al.gz  corpora/tm-train3_fr.al.gz
   \end{alltt}
\end{small}

This section in the tutorial is meant to replace instructions in \S\ref{LM} and
\S\ref{TM}: you should follow the instructions there or here, but ideally not
both in the same instance of the framework.  If you create both regular LMs and
TMs as well as mixture LMs and TMs, things will work anyway, but tuning and
decoding will pick up and use both sets of models.  Under some circumstances
that is in fact desirable, but in that case it's best to do so by defining
variables appropriately in the \code{Makefile.params} file, so that training of
your system can be reproducible.

\subsubsection{Mixture Language Models} \label{MIXLM}

The first mixture models we will look at are the mixture language models
(MixLMs).  To tell the framework to use them, you should edit
\code{Makefile.params} and modify the \code{MIXLM} variable to list the stems
of the three training sets for LMs: \code{lm-train1 lm-train2 lm-train3}.  For
Mixture LMs, it's also often useful to include an additional language model
trained on the union of all the corpora together.  Here, \code{lm-train} is
in fact the union of the three pretend domains, so we'll throw in
\code{lm-train} at the end of \code{MIXLM}:
\begin{small}
\begin{alltt}
   MIXLM = lm-train1 lm-train2 lm-train3 lm-train
\end{alltt}
\end{small}

Having done this, you should also comment out the definition of
\code{TRAIN_LM}, since we'll use the mixture LM instead of the regular LM.
Unfortunately, doing so disables the default definition of \code{TRAIN_TC}, so
we need to set that explicitely: \code{TRAIN_TC = lm-train1}, thus using the
in-domain corpus for truecasing.

With the variables defined as discussed, training of the MixLM is automatic,
but we'll break it down a bit.
\begin{small}
\begin{alltt}
   > \textbf{make corpora}
   [commands to lowercase lm-train?_*]
   > \textbf{cd models/mixlm}
   > \textbf{make all}
   [commands to create lm-train1_fr-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train2_fr-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train3_fr-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train_fr-kn-5g.lm.gz and .binlm.gz]
   echo "`basename models/mixlm/lm-train1_fr*.binlm.gz` \bs
         `basename models/mixlm/lm-train2_fr*.binlm.gz` \bs
         `basename models/mixlm/lm-train3_fr*.binlm.gz` \bs
         `basename models/mixlm/lm-train_fr*.binlm.gz`" "" \bs
      | tr " " "\bs{}n" > components_fr
   mx-calc-distances.sh -v em components_fr ../../corpora/dev1_fr.lc > dev1.distances
   mx-dist2weights -v normalize dev1.distances > dev1.weights
\end{alltt}
\end{small}

The first command above, \code{make corpora}, forces the framework to do the
lowercasing of the new files we just added there.  Then \code{make all} does a
lot of work.  It creates the three source-side per-domain language models, as
well as the global one.  The source-side language models are used to tune the
weights of the component LMs in the MixLM: \code{mx-calc-distances.sh} uses the
EM algorithm to find the component weights that allow the source-side MixLM to
best model the source side of our dev set, here \code{dev1_fr.lc}.  Since the
dev set is in domain, we typically expect the in-domain component to get the
highest weight.

\begin{small}
\begin{alltt}
   [... continued from above]
   [commands to create lm-train1_en-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train2_en-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train3_en-kn-5g.lm.gz and .binlm.gz]
   [commands to create lm-train_en-kn-5g.lm.gz and .binlm.gz]
   echo "`basename models/mixlm/lm-train1_en*.binlm.gz` \bs
         `basename models/mixlm/lm-train2_en*.binlm.gz` \bs
         `basename models/mixlm/lm-train3_en*.binlm.gz` \bs
         `basename models/mixlm/lm-train_en*.binlm.gz`" "" \bs
      | tr " " "\bs{}n" > components_en
   mx-mix-models.sh mixlm dev1.weights components_en \bs
      ../../corpora/dev1_fr.lc > dev1.mixlm
\end{alltt}
\end{small}

Having tuned the weights on the source-side, the target-side component LMs are
created and the mixture model is written by pasting the weights in
\code{dev1.weights} onto the target-side components in \code{components_en}:
\begin{small}
\begin{alltt}
   > \textbf{cat dev1.mixlm}
   lm-train1_en-kn-5g.binlm.gz     3.50733e-05
   lm-train2_en-kn-5g.binlm.gz     0.000873166
   lm-train3_en-kn-5g.binlm.gz     0.287847
   lm-train_en-kn-5g.binlm.gz      0.711245
\end{alltt}
\end{small}

As mentioned earlier, we would have expected the in-domain component to have
the highest weight.  This didn't actually happen here because our toy data
violates our assumptions: the EM algorithm recognized that \code{lm-train} in
fact better models the dev set and gave it the highest weight.  With real data,
you should get more reasonable results.

The file \code{dev1.mixlm} is our mixture language model, adapted for the
dev set \code{dev1}.  When we tune decoder weights, we'll use it as is.  When
we use this model on new data, we might use the weights tuned on
\code{dev1}: we call this using a static mixture model, which works well as
long as the new data is similar enough to the dev data.  Or we might tune
mixture weights specifically for the new document or test set: we call this
using a dynamic mixture model because the weights are dynamically adjusted to
new text, which works well as long as the new document is large enough,
containing scores to hundreds of sentences.

\subsubsection{Mixture Translation Models} \label{MIXTM}

Just as we linearly combine LMs into a mixture language model, we can linearly
combine TMs that come from several corpora into a mixture translation model
(MixTM).  To enable MixTMs in the framework, edit \code{Makefile.params} and
set \code{MIXTM} to the three per-domain corpora:
\begin{small}
\begin{alltt}
   MIXTM = tm-train1 tm-train2 tm-train3
\end{alltt}
\end{small}

For the MixLM, putting the in-domain corpus first was not actually critical,
but here it is: the word-alignment model from the first corpus is assumed to be
in domain.

As with MixLMs, you should remove the definition of \code{TRAIN_TM}, since the
MixTM replaces the regular TM.

\begin{small}
\begin{alltt}
   > \textbf{make corpora}
   [commands to lowercase tm-train?_*]
   > \textbf{cd models/tm}
   > \textbf{make all}
   [commands to create IBM1, IBM2, HMM, and IBM4 models for each component corpus]
   [commands to create *.align.gz files for each corpus and alignment method]
   [commands to create jpt.* files for each corpus and alignment method]
   merge_counts jpt.merged.tm-train1.fr-en.gz \bs
      jpt.ibm4.tm-train1.fr-en.gz jpt.hmm3.tm-train1.fr-en.gz jpt.ibm2.tm-train1.fr-en.gz
   [repeat merge_counts for tm-train2, tm-train3]

   joint2cond_phrase_tables -prune1w 100 -v -i -z -hmm -1 fr -2 en \bs
      -s "KNSmoother 3" -s ZNSmoother -multipr fwd \bs
      -o cpt.merged.hmm3-kn3-zn.tm-train1 \bs
      -ibm_l2_given_l1  hmm3.tm-train1.en_given_fr.gz \bs
      -ibm_l1_given_l2  hmm3.tm-train1.fr_given_en.gz \bs
      -reduce-mem -no-sort jpt.merged.tm-train1.fr-en.gz
   [repeat joint2cond_phrase_tables for tm-train2 and tm-train3]
\end{alltt}
\end{small}

The MixTM training process can take a long time, because a lot of models and
files are created, in particular, separate alignment and JPT files are created
for the full matrix of alignment methods and component corpora.  These are
created in the same way as in \S\ref{TM}, so we left out most of the details
here: create word-alignment models; use them to word-align the corpora;
generate JPTs; combine the counts from the JPTs and finish with
\code{joint2cond_phrase_tables} to produce CPTs with probabilities estimated
from each component corpus, separately.

Note that \code{make} might not run the commands in the order shown here: we've
reordered them to be simplify the discussion, but the end-result should be the
same.

\begin{small}
\begin{alltt}
   [... continued from above]
   [commands to create *.align.gz and jpt.* for the dev1 set, using IBM2 and
    HMM3 models from tm-train1, assumed to be in domain]
   merge_counts jpt.merged.dev1.fr-en.gz jpt.hmm3.dev1.fr-en.gz jpt.ibm2.dev1.fr-en.gz
   train_tm_mixture -v -o cpt.mix.fr2en.gz \bs
      cpt.merged.hmm3-kn3-zn.tm-train1.fr2en.gz cpt.merged.hmm3-kn3-zn.tm-train2.fr2en.gz \bs
      cpt.merged.hmm3-kn3-zn.tm-train3.fr2en.gz jpt.merged.dev1.fr-en.gz
   ln -sf mixtm/cpt.mix.fr2en.gz .
\end{alltt}
\end{small}

Now that we have one CPT for each component corpus, we need to determine how to
weigh them in the linear combination.  We generate another JPT, this one on our
dev set, using the word-alignment models from the in-domain
component.\footnote{We can't use the dev set itself to create these
word-alignment models, because it is much too small for that purpose.  That's
why we need to assume the first component is in domain.}  Then we use the EM
algorithm (as for MixLMs) to find weights that maximize the probability of the
phrase pairs in the dev JPT according to the mixture model.  The final result
should give higher weights to components that are in-domain or closely related,
while keeping all information from out-of-domain or generic components.

The result of this combination is written to \code{cpt.mix.fr2en.gz}, in
the standard CPT format, so that the decoder does not need to be aware that
this is a MixTM rather than a regular TM.

\subsubsection{Generic and Pre-trained Models} \label{Generic}

So far we assumed that all the models are trained locally, in the current
instance of the framework.  Sometimes, you might have models already trained in
other instances of the framework, or coming from generic models for which you
may not have the training corpus.  In that case, you can add the name of the
target-side pre-trained language models in \code{.binlm.gz} format to
\code{MIXLM_PRETRAINED_TGT_LMS} and they will be included in the mixture
language model.  The source-side pre-trained language models are also required,
to tune the mixture weights, and should have the same name except for the
language code.

Similarly, generic phrase tables and pre-trained phrase tables from other
domains can be included in your MixTMs by adding the name(s) of the of the CPT
file(s) to \code{MIXTM_PRETRAINED_TMS}.  In this case all the CPTs (the one
trained in the framework and the pre-trained ones) must have the same number of
scores, which is four by default in the framework, as discussed in \S\ref{TM}.
Also, the in-domain phrase table cannot be pre-trained, its corpus must still
be the first one listed in \code{MIXTM}.

\subsection{Optimizing Decoder Weights} \label{COW}

The language and translation models are the main sources of information used
for translation, which is performed by the \code{canoe} decoder.  Optionally,
we might have trained an HLDM as an additional source of information, and we
might have included alignment indicator features.  In order to get reasonable
translation quality, the weights on these (and other) sources of information
need to be tuned. The tuning process is carried out by a script called
\code{tune.py}, which runs \code{canoe} several times, refining the weights at
each iteration.  \code{tune.py} takes an initial \code{canoe.ini} configuration
file and a dev set (with reference translations), and produces a tuned
configuration file, \code{canoe.ini.cow}, optimized to maximize BLEU on that
dev set.

We used to perform the weight optimization with an algorithm called Powell's,
using a script called \code{cow.sh} (where COW stands for Canoe Optimize
Weights).  Both are still part of \PS, but we are now using a better tuning
algorithm, MIRA (as mentioned in \S\ref{PI}), and a more general script that
supports an array of tuning algorithms, \code{tune.py}.  Although Powell's
algorithm worked reasonably well, we found it did not cope well with large
feature sets, and it was not stable: the weights found would sometimes overfit
the dev set and not generalize well to new data.  MIRA overcomes both of these
issues.  Furthermore, it consistently finds better weights than Powell did.
MIRA comes in two variants, one using the full decoder lattice, one working
from N-best lists.  We use lattice MIRA for optimizing decoder weights, and
n-best MIRA for optimizing rescoring weights (\S\ref{RAT}).

You can run all the steps below by typing \code{make decode} in your main
\code{toy.experiment} directory, but we'll break it down into several steps
here.

\subsubsection{The Decoder Configuration File: \code{canoe.ini}}

In this framework, we start with a template configuration file,
\url{models/decode/canoe.ini.template}:
\begin{small}
\begin{alltt}
   > \textbf{cd models/decode}
   > \textbf{cat canoe.ini.template}
   [ttable-multi-prob]
     <CPTS>
   [lmodel-file]
     <LMS>
   [use-ftm]
   ...
\end{alltt}
\end{small}

The framework will automatically insert the phrase tables, language models
and (H)LDMs generated in the previous steps into the \code{canoe.ini} file when
you make it.  These are the template parameters it will replace:
\begin{itemize}
\item \code{<SL>}:   source language code, e.g. ``fr'';
\item \code{<TL>}:   target language code, e.g., ``en'';
\item \code{<CPTS>}: all CPTs trained above and found in the
                     \code{models/tm} directory; and
\item \code{<LMS>}:  all language models trained trained above and found in
                     the \code{models/lm} directory);
\item If \code{USE_HLDM} or \code{USE_LDM} is set in \code{Makefile.params},
special processing is also done to insert the necessary (H)LDM parameters into
the \code{canoe.ini} file.
\end{itemize}

So we make the \code{canoe.ini} file:\footnote{Note that here, we kept all the
optional paths discussed in previous sections: alignment indicator features,
HLDMs, IBM4 alignments.  Output will differ if you choose a different path
through this tutorial.}
\begin{small}
\begin{alltt}
   > \textbf{make canoe.ini}
   HLDM: models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.gz
   CPTs: models/tm/cpt.PI-kn3-zn.hmm3.fr2en.gz
   LMs: models/lm/lm-train_en-kn-5g.binlm.gz
   cat canoe.ini.template \bs
      | sed -e 's/<SL>/fr/g' \bs
            -e 's/<TL>/en/g' \bs
            -e 's#<CPTS>#models/tm/cpt.PI-kn3-zn.hmm3.tm-train.fr2en.gz#g' \bs
            -e 's#<LMS>#models/lm/lm-train_en-kn-5g.binlm.gz#g' \bs
            -e 's/^\bs(\bs[\bs(weight-f\bs|ftm\bs)\bs]\bs)/##mid##\bs1/' \bs
      | configtool -p "args:-dist-phrase-swap -distortion-model WordDisplacement::\bs
        back-hlex#m#H:back-hlex#s#H:back-hlex#d#H:fwd-hlex#m#H:fwd-hlex#s#H:fwd-hlex#d#H\bs
        -lex-dist-model-file ::models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.gz#H " - \bs
      > canoe.ini
   configtool check canoe.ini
   ok
\end{alltt}
\end{small}
This command creates a soft link to the \code{models} directory---this way, all
models appear to be relative to the current directly, which makes it easier to
find them.  We create such symlinks everywhere we need access to the models.
The \code{sed} commands replace the template parameters by the appropriate
values.  The first \code{configtool} command inserts the distortion parameters
chosen by your framework parameters.  The final command, \code{configtool check
canoe.ini}, confirms that the \code{canoe.ini} file produced is error-free---it
checks that all parameters are compatible and that all models can be found.

Here is the result:
\begin{small}
\begin{alltt}
   > \textbf{cat canoe.ini}
   [ttable-multi-prob] 
      models/tm/cpt.PI-kn3-zn.hmm3.fr2en.gz
   [lex-dist-model-file] 
      models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.gz#H
   [lmodel-file] 
      models/lm/lm-train_en-kn-5g.binlm.gz
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [dist-limit-simple]
   [dist-phrase-swap]
   [bypass-marked]
   [cube-pruning]
   [use-ftm]
   [weight-w] -1
   [distortion-model] 
      WordDisplacement
      back-hlex#m#H
      back-hlex#s#H
      back-hlex#d#H
      fwd-hlex#m#H
      fwd-hlex#s#H
      fwd-hlex#d#H
\end{alltt}
\end{small}

When using this framework, do not modify \code{canoe.ini} directly.
To change the training parameters for language models or phrase tables, modify
the parameters or Makefiles appropriately in \code{models/lm} and
\code{models/tm} and regenerate those models. Be careful, though: the
\code{canoe.ini} file will include all models generated in the same
framework, so you should work in different copies of the framework if you want
to experiment with different training parameters for those models.
\tip\margintip Soft links can save you a lot of duplicated work if you have
related experiments.\tipend

\tip\margintip Any time a \code{canoe.ini} configuration file is manipulated,
always run \code{configtool check} on the resulting \code{canoe.ini} file to
verify the validity of its contents.\tipend

To change the other decoding parameters, modify \code{canoe.ini.template}.
For example, you can add additional language models (trained outside the
framework) by adding them in the \code{[lmodel-file]} section, separated by
whitespace or a newline from \code{<LMS>}.

The other decoding parameters control the search strategy.  They have been set
to reasonable defaults, but may need adjustment depending on the
speed/quality trade-offs you're willing to make or other models you wish to
use.

\subsubsection{Running \code{tune.py}}

Besides the configuration file, the other main arguments required by
\code{tune.py} are a directory in which to store temporary files, a source
file and one or more human translations of the source file, which we call
reference translations.\footnote{This example uses only one translation, but
when multiple reference translations are available, it is advantageous to use
them all.} Here we call the temporary directory \code{foos}, and we use the
\code{dev1} files for weight tuning:
\begin{small}
\begin{alltt}
   > \textbf{make all}
   mkdir -p foos
   filter_models -z -r -tm-soft-limit cpt.dev1 -ldm < ../../corpora/dev1_fr.rule
   tune.py -v -o canoe.ini.cow.FILT -p 4 -c 1 -m 15 -n 100 -a lmira --workdir=foos \bs
      -f canoe.ini.FILT ../../corpora/dev1_fr.rule ../../corpora/dev1_en.lc
   configtool -p args:"`configtool weights canoe.ini.cow.FILT`" canoe.ini > canoe.ini.cow
   rm -f -r foos *.FILT.gz *.FILT.bkoff canoe.ini.FILT canoe.ini.FILT.cow
\end{alltt}
\end{small}
Because \code{tune.py} takes a while to run (between 30 minutes and three
hours in this example, depending on your system)\footnote{Running time for
\code{tune.py} can be reduced by executing it in parallel. Type
\code{tune.py -h} to see documentation on this option; modify
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}DECODE} in \code{Makefile.params} to
adjust tuning parallelism in this framework.} and writes large amounts of logging
information, it is usually a good idea to redirect all of its output to a log
file, as is done here; and also to run it in the background (not done here).
Progress is most easily monitored by looking at the files \code{summary}, which
shows the translation quality (measured by BLEU score---see \S\ref{Testing} for
a description), and \code{summary.wts}, which shows the decoder weights for
each iteration.

Note that the BLEU score does not always increase monotonically, as can
be seen in the \code{summary} file.
% cat summary | perl -pe 's/(\.\d\d\d\d)\d+(:| \S|$)/\1\2/g; s/   -d/  -d/'
% cat summary.wts | perl -pe 's/(\.\d\d\d)\d+(:| \S|$)/\1\2/g; s/   -d/  -d/' | tr ' ' $'\t' | expand-auto.pl -t 1
\begin{footnotesize}
\begin{alltt}
> \textbf{cat summary}
decode-score=0.1773 prev-optimizer-score=0 prev-nbest-size=0 avg-wt-diff=0.0
decode-score=0.1931 prev-optimizer-score=0.007G prev-nbest-size=0 avg-wt-diff=0.6506
decode-score=0.2033 prev-optimizer-score=0.007G prev-nbest-size=0 avg-wt-diff=0.0983
decode-score=0.2077 prev-optimizer-score=3.669G prev-nbest-size=0 avg-wt-diff=0.0902
decode-score=0.2119 prev-optimizer-score=0.007G prev-nbest-size=0 avg-wt-diff=0.0699
decode-score=0.2114 prev-optimizer-score=0.007G prev-nbest-size=0 avg-wt-diff=0.0762
decode-score=0.2038 prev-optimizer-score=1.671G prev-nbest-size=0 avg-wt-diff=0.0496
decode-score=0.2088 prev-optimizer-score=0.007G prev-nbest-size=0 avg-wt-diff=0.0603
[...]
> \textbf{cat summary.wts}
1.0   1.0    1.0   1.0   1.0   1.0   1.0   -1.0   1.0  1.0   1.0   1.0   1.0   1.0   1.0   1.0
0.224 0.409  0.250 0.346 0.424 0.309 0.308 -0.358 1.0  0.017 0.215 0.264 0.276 0.284 0.346 0.550
0.188 0.532 -0.081 0.404 0.555 0.155 0.234 -0.548 1.0 -0.013 0.213 0.231 0.190 0.402 0.332 0.740
0.337 0.541  0.093 0.259 0.439 0.180 0.177 -0.763 1.0 -0.035 0.037 0.131 0.268 0.341 0.274 0.797
0.292 0.507 -0.055 0.357 0.413 0.140 0.232 -0.667 1.0 -0.018 0.102 0.395 0.133 0.332 0.315 0.839
0.196 0.407  0.066 0.276 0.283 0.192 0.179 -0.630 1.0 -0.007 0.116 0.187 0.185 0.423 0.322 0.676
0.256 0.477  0.083 0.421 0.309 0.184 0.125 -0.521 1.0 -0.006 0.194 0.256 0.193 0.326 0.279 0.684
0.216 0.582  0.079 0.355 0.397 0.199 0.159 -0.584 1.0 -0.044 0.144 0.173 0.186 0.374 0.299 0.800
[...]
\end{alltt}
\end{footnotesize}
When the last iteration still shows an improvement (which is not the case
here), it is sometimes a sign that we should have allowed \code{tune.py} to run
more iterations, by setting the \code{-m} option to a higher value (which is
controlled by the \code{MERT_MAX_ITER} variable in the framework.

It's hard to make sense of the contents of \code{summary.wts} here: it shows
the weigth of each feature at each iteration.  A more verbose version of the
same information is shown in \code{logs/log.decode}, along with lots of logging
information about decoding.  Here is an excerpt from the end of that log file:
\begin{small}
\begin{alltt}
Log-linear model used:
index  weight      feature description
1      0.240071    DistortionModel:WordDisplacement
2      0.464304    DistortionModel:back-hlex#m#0
3      0.0923178   DistortionModel:back-hlex#s#0
4      0.373179    DistortionModel:back-hlex#d#0
5      0.360726    DistortionModel:fwd-hlex#m#0
6      0.190095    DistortionModel:fwd-hlex#s#0
7      0.0599524   DistortionModel:fwd-hlex#d#0
8      -0.45526    LengthFeature
9      1           LanguageModel:models/lm/lm-train_en-kn-5g.binlm.gz;SimpleAutoVoc
10     -0.0114234  TranslationModel:cpt.dev1.FILT.gz(col=0)
11     0.238571    TranslationModel:cpt.dev1.FILT.gz(col=1)
12     0.187018    ForwardTranslationModel:cpt.dev1.FILT.gz(col=2)
13     0.12948     ForwardTranslationModel:cpt.dev1.FILT.gz(col=3)
14     0.371249    AdirectionalModel:cpt.dev1.FILT.gz(col=0)
15     0.227184    AdirectionalModel:cpt.dev1.FILT.gz(col=1)
16     0.667892    AdirectionalModel:cpt.dev1.FILT.gz(col=2)
\end{alltt}
\end{small}
Here the features used are listed with their respective weights in a way that
is easy to read, whereas in the \code{summary.wts} file you can see how the
weights evolved to get to the final weights we see here.
You will notice that there are two ``TranslationModel''
weights and features: one for each of the backward probability estimates in the
merged phrase table.  Similary, there are two ``ForwardTranslationModel''
weights and features.  The three ``AdirectionalModel'' are the alignment
indicator features we generated.  Since we are using HLDMs, there are seven
``DistortionModel'' weights and features.  Finally, we have our language model
and the length feature which, with its negative weight here, encourages the
decoder to explore longer sentences, at least partially in compensation with
the language model's tendency to prefer short sentences.

While \code{tune.py} is working, it saves lattices or $n$-best lists, and other
intermediate files, to a temporary work directory called \code{foos}. If you
wish to prevent these (large) files from being automatically cleaned up on
completion, remove the \code{rm} command executed after \code{tune.py} in
\code{Makefile}.

% EJJ No longer works, and I don't see an equivalent for tune.py...  I suppose
% the summary file itself is the equivalent now.
%\tip \margintip A lot of information is logged to \code{log.canoe.ini.cow}
%this can be summarized using the command \code{cowpie.py log.canoe.ini.cow}
%(type \code{cowpie.py -h} for details). \tipend

% EJJ No equivalent to cow-timeing.pl for tune.py, which is unfortunate.
%\tip \margintip The scripts \code{cow-timing.pl} and
%\code{canoe-timing-stats.pl} provide detailed running time information for the
%various components of \code{cow.sh}. \tipend
\tip \margintip The script \code{canoe-timing-stats.pl} provides statistics
about the time the decoder spends loading models and translating sentences,
globally and on a per-sentence basis.  Run from the parent directory of your
experiments, this command can be helpful: \code{canoe-timing-stats.pl
*/models/decode/logs/log.decode}; it can alert you if there are performance
problems with some of your systems. \tipend

\tip \margintip The script \code{summarize-canoe-results.py} can be handy if
you are monitoring multiple experiments: try \code{summarize-canoe-results.py
*} in the parent directory of your experiments.  It will summarize all the
experiments you did, reporting BLEU scores obtained on your dev set as well as
on your test set(s).  For experiments still in progress, it reports iterations
completed and the BLEU score on dev so far.\tipend

The final output from \code{tune.py} is written to the file
\code{canoe.ini.cow} This duplicates the contents of \code{canoe.ini}, but adds
the weights tuned on the development corpus:
% cat canoe.ini.cow | perl -pe 's/(\.\d\d\d\d\d\d)\d+/\1/g; s/(\.\d\d\d\d)\d+/\1/g if /weight-d/; s/^/   /'
\begin{small}
\begin{alltt}
   > \textbf{cat canoe.ini.cow}
   [ttable-multi-prob]   models/tm/cpt.PI-kn3-zn.hmm3.fr2en.gz
   [lex-dist-model-file] models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.gz
   [lmodel-file]         models/lm/lm-train_en-kn-5g.binlm.gz
   [weight-l] 1
   [weight-t] -0.018770:0.102704
   [weight-f] 0.395305:0.133720
   [weight-a] 0.332245:0.315304:0.839850
   [ttable-limit] 30
   [ttable-threshold] 0
   [stack] 20000
   [beam-threshold] 0.001
   [distortion-limit] 7
   [dist-limit-simple]
   [dist-phrase-swap]
   [bypass-marked]
   [cube-pruning]
   [use-ftm]
   [weight-d] 0.2921:0.5074:-0.0554:0.3579:0.4131:0.1402:0.2326
   [weight-w] -0.667455
   [distortion-model] WordDisplacement
      back-hlex#m#0 back-hlex#s#0 back-hlex#d#0
      fwd-hlex#m#0 fwd-hlex#s#0 fwd-hlex#d#0
\end{alltt}
\end{small}

The set of \code{weight-} parameters above provide the decoder features weights:
\begin{itemize}
\item the language model (\code{weight-l}),
\item the backward scores of the translation models (\code{weight-t}),
\item the forward scores of the translation models (\code{weight-f}),
\item the adirectional features (alignment indicator features) of the translation models (\code{weight-a}),
\item the distortion models (basic + HLDM) (\code{weight-d}), and
\item the word-length penalty (\code{weight-w}).
\end{itemize}
The language model often obtains the highest weight, as is the case
here.\footnote{A note of warning about the LM weight: following Doug Paul's
``ARPA'' LM format, all LM formats we know use base-10 log probs (including our
own binary LM and TPLM formats), but canoe interprets them as natural logs:
throughout \PS, logs are natural by default, not base 10.  This known bug has
minimal impact; correcting for it simply requires multiplying the desired
\code{weight-l} values by log(10), which \code{tune.py} and \code{rat.sh} do
implicitly. We chose not to fix it to avoid having to adjust all previously
tuned sets of weights.}


\subsection{Training a Confidence Estimation Model} \label{CE}

If you are going to use confidence estimation (CE), the final training step is
to create a model to estimate the confidence of the system on its own output.
This model uses features about the source text, the target text, translation
memory information if available, and so on, to come up with a confidence score
between 0 and 1. As we mentioned before, it is critical that the data used to
tune the CE model be completely unseen data. It can't have been part of your
training data or the dev set you used to tune the decoder weights.  Otherwise
the confidence estimates produced will not be useful.

At this stage we have to return to one previous step. We did not train all
the models we need for CE: specifically, we need an LM for the source language.
Edit \code{Makefile.params} in the main \code{toy.experiment} directory and set
\code{DO_CE} to \code{1} (by uncommenting the approriate line). If you run
\code{make confidence} in the main directory, all necessary steps will be done
automatically.  As usual, we'll break it down a bit.
\begin{small}
\begin{alltt}
   > \textbf{cd models}
   > \textbf{make lm.fr}
   make -C lm all LM_LANG=fr
   [...]
\end{alltt}
\end{small}
This command will build the required source-side LM using the toolkit you
selected earlier.

Now we can work on the CE model itself. Note that our CE module does not
currently work with rescoring, only with decoding. We build on the decoder
model tuned in \S\ref{COW}.

Just as for decoding and rescoring, CE works with a model described in a text
file.  In this framework we provide a template model that does not use a
translation memory.  If you have access to the results of looking up your
source text in a translation memory, it is worth incorporating them into your
CE model.  In the template provided here, all the translation memory related
features are commented out.  See \url{doc/README.confidence} for more
details.

Now we build the input CE model file:
\begin{small}
\begin{alltt}
   > \textbf{cd models/confidence}
   > \textbf{make ce-notm.ini}
   sed -e 's#dev1.mixlm#dev3.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow
   configtool check canoe.ini.cow
   ok
   cat ce-notm.template \bs
      | sed -e "s#IBM\bs\bs(.\bs\bs)FWD#models/tm/ibm\bs\bs1.tm-train.en_given_fr.gz#" \bs
            -e "s#IBM\bs\bs(.\bs\bs)BKW#models/tm/ibm\bs\bs1.tm-train.fr_given_en.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)FWD#models/tm/hmm\bs\bs1.tm-train.en_given_fr.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)BKW#models/tm/hmm\bs\bs1.tm-train.fr_given_en.gz#" \bs
            -e "s#LM_SRC#models\/lm\/lm-train_fr-kn-5g.binlm.gz#" \bs
            -e "s#LM_TGT#models\/lm\/lm-train_en-kn-5g.binlm.gz#" \bs
      > ce-notm.ini
\end{alltt}
\end{small}

The result is a \code{ce-notm.ini} file with the LMs and IBM models filled in,
which we now feed to \code{ce_translate.pl -train}.  The output will be a
trained CE model.

\begin{small}
\begin{alltt}
   > \textbf{make all}
   ce_translate.pl -n=4 -train -src=fr -tgt=en -notok -nolc -nl=s \bs
      -k=5 -desc=ce-notm.ini canoe.ini.cow ce_model \bs
      ../../corpora/dev3_fr.lc ../../corpora/dev3_en.lc
\end{alltt}
\end{small}

The trained model is saved to the file \code{ce_model.cem}.  This file is
actually a gzipped-tarred archive, so you could examine it using the
command \code{tar -xOzf ce_model.cem | less}.\footnote{Note that the command
\code{tar -xOzf} contains the capital letter \code{O} (oh), not the number
\code{0} (zero).} However, the contents don't have much intuitive meaning, so
this is only useful for curiosity's sake.

\subsection{Training a Rescoring Model} \label{RAT}

If you are using rescoring, the final training step is to create a model for
rescoring $n$-best lists.  Rescoring means having \code{canoe} generate a list
of $n$ (typically 1000) translation hypotheses for each source sentence, then
choosing the best translations from among these.  The advantage of this
procedure is that the choice can be made on the basis of information that is
too expensive for \code{canoe} to use during search. This step sometimes gives
a modest improvement over the results obtained using \code{canoe} alone. Often,
however, it gives no significant improvement while being fairly slow, so if
translation speed is an issue, you probably want to skip rescoring; experiment
with your own data to determine the best choice.  In this framework, rescoring
is only done if \code{DO_RESCORING} is set to 1 in \code{Makefile.params}.

Training a rescoring model involves generating $n$-best lists, then calculating
the values of selected \emph{features} for each hypothesis in each list. A
feature is any real-valued function that is intended to capture the relation
between a source sentence and a translation hypothesis. A rescoring model
consists of a vector of feature weights set so as to optimize translation
performance when a weighted combination of feature values is used to reorder
the $n$-best lists.

Rescoring is not currently compatible with the new truecasing method (using
source side information).  In order to complete this section, you will need to
edit \code{Makefile.params} and set \code{DO_RESCORING = 1} and comment out
remove \code{TC_USE_SRC_MODELS = 1}.

Rescoring is also incompatible with confidence estimation, but the framework
can be used to train both a rescoring and a confidence estimation model: you
just won't be able to use them together.

You can run all the steps below by typing \code{make rescore} or \code{make
rat} in your main \code{toy.experiment} directory, but we'll break it down as
usual.

\subsubsection{The Input Rescoring Model}

Training is carried out by the \code{rat.sh} script. This takes as input a
rescoring model that specifies which features to use, and it finds optimal
weights for these features, maximizing BLEU on your rescoring dev set.

The default model created by this framework contains a small set of useful
features:
\begin{small}
\begin{alltt}
   > \textbf{cd models/rescore}
   > \textbf{make rescore-model.ini}
   configtool rescore-model:ffvals models/decode/canoe.ini.cow \bs
      | cut -f 1 -d ' ' > rescore-model.ini
   cat rescore-model.template \bs
      | sed -e "s#IBM\bs\bs(.\bs\bs)FWD#models/tm/ibm\bs\bs1.tm-train.en_given_fr.gz#" \bs
            -e "s#IBM\bs\bs(.\bs\bs)BKW#models/tm/ibm\bs\bs1.tm-train.fr_given_en.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)FWD#models/tm/hmm\bs\bs1.tm-train.en_given_fr.gz#" \bs
            -e "s#HMM\bs\bs(.\bs\bs)BKW#models/tm/hmm\bs\bs1.tm-train.fr_given_en.gz#" \bs
      | egrep -v '^#' \bs
      >> rescore-model.ini
   > \textbf{cat rescore-model.ini}
   FileFF:ffvals,1
   FileFF:ffvals,2
   FileFF:ffvals,3
   [...]
   FileFF:ffvals,15
   FileFF:ffvals,16
   LengthFF
   IBM1TgtGivenSrc:models/tm/ibm1.tm-train.en_given_fr.gz
   IBM1SrcGivenTgt:models/tm/ibm1.tm-train.fr_given_en.gz
   IBM2TgtGivenSrc:models/tm/ibm2.tm-train.en_given_fr.gz
   IBM2SrcGivenTgt:models/tm/ibm2.tm-train.fr_given_en.gz
   [...]
   nbestNgramPost:2#1#<ffval-wts>#<pfx>
   nbestSentLenPost:1#<ffval-wts>#<pfx>
   ParMismatch
   QuotMismatch:ef
   RatioFF
\end{alltt}
\end{small}
There are two kinds of features included in our rescoring model:
\begin{itemize}
\item Those that look like \code{FileFF:ffvals,}$i$ tell \code{rat.sh} to
use the $i$\/th feature generated by the \code{canoe} decoder itself. It is
standard practice to use all decoder features when rescoring, as is done
automatically by the framework via the \code{configtool} command executed
above.
\item The other features, following the format \emph{Feature:Args}, tell
\code{rat.sh} to generate values for the feature \emph{Feature} using
arguments \emph{Args}.  For example, the string
\url{IBM2TgtGivenSrc:model/tm/ibm2.tm-train.en_given_fr.gz} says to calculate
the feature \code{IBM2TgtGivenSrc} using the IBM model
\code{model/tm/ibm2.tm-train.en_given_fr.gz}, which we trained earlier. To
see a list of all available features, type \code{rescore_train -H}.  In this
framework, you should set your features in \code{rescore-model.template}.
The special tokens all in upper case (\code{IBM1FWD}, etc) you see in the
provided template are replaced by the actual models you trained earlier.
\end{itemize}

Lines starting with \code{\#} are comments and are ignored by the software.

\subsubsection{Running RAT (Rescore And Translate)}

Apart from the rescoring model, \code{rat.sh} needs a source file and one or
more reference translations for it (same as \code{tune.py}). These may be the
same files used for \code{tune.py}, but it is sometimes better to use different
ones, so here we use \code{dev2}:\footnote{As with \code{tune.py}, you can
speed this up using parallelism, in this case via the -n option to
\code{rat.sh}, given before the \code{train} token (type \code{rat.sh -h}
for details). In this framework, parallelism is controlled via the
\code{PARALLELISM\us{}LEVEL\us{}TUNE\us{}RESCORE} variable in
\code{Makefile.params}.}
\begin{small}
\begin{alltt}
   > \textbf{make all}
   sed -e 's#dev1.mixlm#dev2.mixlm#g' models/decode/canoe.ini.cow > canoe.ini.cow.dev2
   configtool check canoe.ini.cow.dev2
   ok
   Tuning the rescoring model.
   rat.sh -lb -n 4 train -a mira -v -K 1000 -o rescore-model \bs
      -msrc ../../corpora/dev2_fr.rule -f canoe.ini.cow.dev2 rescore-model.ini \bs
      ../../corpora/dev2_fr.lc ../../corpora/dev2_en.lc
   rm -f -r workdir-dev2_fr.rule-1000best
\end{alltt}
\end{small}

As mentioned in \S\ref{COW}, we now use N-best MIRA for rescoring instead of
Powell's algorithm.  The \code{-a mira} switch provided to \code{rat.sh} above
selects this tuning method.

The output from \code{rat.sh} is written to the file \code{rescore-model}:
% cat rescore-model | perl -pe 's/(\.\d\d\d\d\d\d)\d+/\1/g'
\begin{footnotesize}
\begin{alltt}
   > \textbf{cat rescore-model}
   FileFF:ffvals,1 0.157270
   FileFF:ffvals,2 0.205718
   FileFF:ffvals,3 0.342265
   FileFF:ffvals,4 0.158950
   FileFF:ffvals,5 0.058979
   FileFF:ffvals,6 0.232693
   FileFF:ffvals,7 0.046713
   FileFF:ffvals,8 0.003745
   FileFF:ffvals,9 0.460983
   FileFF:ffvals,10 0.251291
   FileFF:ffvals,11 -0.329647
   FileFF:ffvals,12 0.483315
   FileFF:ffvals,13 -0.029638
   FileFF:ffvals,14 -0.066872
   FileFF:ffvals,15 -0.127707
   FileFF:ffvals,16 -0.418385
   LengthFF 0.091527
   IBM1TgtGivenSrc:models/tm/ibm1.tm-train.en_given_fr.gz 0.449773
   IBM1SrcGivenTgt:models/tm/ibm1.tm-train.fr_given_en.gz 0.639853
   IBM2TgtGivenSrc:models/tm/ibm2.tm-train.en_given_fr.gz 0.248381
   IBM2SrcGivenTgt:models/tm/ibm2.tm-train.fr_given_en.gz -0.006419
   HMMTgtGivenSrc:models/tm/hmm3.tm-train.en_given_fr.gz -0.227869
   HMMSrcGivenTgt:models/tm/hmm3.tm-train.fr_given_en.gz -0.206212
   HMMVitTgtGivenSrc:models/tm/hmm3.tm-train.en_given_fr.gz -0.149313
   HMMVitSrcGivenTgt:models/tm/hmm3.tm-train.fr_given_en.gz -0.490217
   IBM1WTransTgtGivenSrc:models/tm/ibm1.tm-train.en_given_fr.gz -0.214304
   IBM1WTransSrcGivenTgt:models/tm/ibm1.tm-train.fr_given_en.gz 0.067143
   IBM1DeletionTgtGivenSrc:models/tm/ibm1.tm-train.en_given_fr.gz 0.036256
   IBM1DeletionSrcGivenTgt:models/tm/ibm1.tm-train.fr_given_en.gz -0.009989
   nbestWordPostSrc:1#<ffval-wts>#<pfx> -0.024783
   nbestWordPostTrg:1#<ffval-wts>#<pfx> 0.578329
   nbestPhrasePostSrc:1#<ffval-wts>#<pfx> -0.130987
   nbestPhrasePostTrg:1#<ffval-wts>#<pfx> 0.304290
   nbestNgramPost:2#1#<ffval-wts>#<pfx> 0.201856
   nbestSentLenPost:1#<ffval-wts>#<pfx> 0.148052
   ParMismatch 9.999999E-4
   QuotMismatch:ef 9.999999E-4
   RatioFF 0.006092
\end{alltt}
\end{footnotesize}
This is a copy of \code{rescore-model.ini} with a weight assigned to each
feature. Other by-products created by \code{rat.sh} are found in the directory
\code{workdir-dev2_fr.lc-1000best} and include the $n$-best lists
\code{1000best.gz}, and the corresponding decoder features \code{ffvals.gz}
and additional features \code{ff.*}. All of these files are compressed to
save space. These files are automatically deleted unless there is a problem.
Remove the \code{rm} command executed after \code{rat.sh} in \code{Makefile} if
you want to preserve them.

Before continuing, comment out \code{DO_RESCORING = 1} and set
\code{TC_USE_SRC_MODELS = 1} again in \code{Makefile.params}, since these are
the values assumed below.

\section{Translating and Testing} \label{TranslatingTesting}

\subsection{Translating} \label{Translating}

Once training is complete, the system can be used to translate new text or the
test corpus.

Some of the steps below will be performed if you run \code{make translate} in
your main \code{toy.experiment} directory, but the final output you need
depends on what you are doing and might not be produced by default. The actual
output produced here depends on the various \code{DO_*} variables in
\code{Makefile.params}.

The primary point of the translations we produce here is evaluation using the
BLEU score.  Refer to \S\ref{NewText} and \S\ref{PLive} for how to handle new
text in easier ways.

\subsubsection{Decoding Only} \label{Decoding}

As mentioned before, there are three options for translating. The simplest is
to decode using the configuration file produced by \code{tune.py} and stop.
To do so, make sure \code{DO_RESCORING} is not set in \code{Makefile.params}.
\footnote{You can accomplish the same result by adding \code{DO\us{}RESCORING=}
to the make command line, i.e., type \\
\hspace*{.2in}\code{make translate DO\us{}RESCORING=} \\
instead of just \code{make translate} (notice there is nothing after the
\code{=} sign). This syntax can always be used to temporarily override a
variable definition on a given call to \code{make}: adding \code{V=} undefines
\code{V}, while adding \code{V=value} sets V to the given value.}
\begin{small}
\begin{alltt}
   > \textbf{cd translate}
   > # edit ../Makefile.params and comment out DO_RESCORING = 1
   > \textbf{make translate}
   ln -fs ../models
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test1
   configtool check canoe.ini.cow.test1
   ok
   Generating translate/test1.out
   canoe-parallel.sh -n 4 canoe -f canoe.ini.cow.test1 -palign \bs
      < ../corpora/test1_fr.rule \bs
      | nbest2rescore.pl -canoe -tagoov -palout=test1.out.pal \bs
      | tee test1.out.oov | perl -pe 's/<OOV>(.+?)<\bs/OOV>/\bs1/g;' > test1.out
   [repeat for test2]
\end{alltt}
\end{small}
This produces the output files \code{test1.out} and \code{test2.out},
containing one line for each source line in \url{test*.rule}, as well as
\code{test*.out.pal} and \code{test*.out.oov}, needed for truecasing.

If you look at the output files, remember that we trained this toy system on a
trivial amount of data.  Translation quality is expected to be horrendous.  You
need a lot more data to get good results.

\subsubsection{Decoding plus Confidence Estimation} \label{CETrans}

The second option for translating is to run the decoder and then estimate the
confidence of the system on the output of the decoder.  This option is not
compatible with rescoring: the confidence estimate is for the one-best output
of the decoder only.  We use the decoder model tuned in \S\ref{COW}, and
then the CE model trained in \S\ref{CE}.  The program
\code{translate.pl} handles both of these steps:
\begin{small}
\begin{alltt}
   > \textbf{cd translate}
   > \textbf{make confidence}
   ln -fs models/confidence/ce_model.cem
   Generating translate/test1.ce
   translate.pl -with-ce -n 4 -notok -nl s -tc -encoding UTF-8 -src fr -tgt en \bs
      -f canoe.ini.cow.test1 -model ce_model.cem ../corpora/test1_fr.al > test1.ce
   [repeat for test2]
\end{alltt}
\end{small}
The main output file, \code{test.ce}, is different from the \code{test.out}
file we saw in the two previous steps in that 1) a confidence estimate is added
at the beginning of each line, and 2) the output has already been
truecased and detokenized. However, the translations are actually the same as in
\code{test.out}, with some postprocessing performed on them.

\subsubsection{Decoding plus Rescoring} \label{RATTrans}

If you executed the commands in the \emph{Decoding Only} section above, please
run \code{make clean} before proceeding.  To run this variant, edit
\code{Makefile.params} and set (or uncomment the line) \code{DO_RESCORING = 1},
and comment out \code{TC_USE_SRC_MODELS = 1}.  We recommend using the new
truecasing workflow, but we have to disable it temporarily since it is not yet
compatible with rescoring, as we mentioned before.

The third option for translating is to generate $n$-best lists and rescore
them using the model generated in \S\ref{RAT}. To do this, we first run
the decoder model tuned in \S\ref{COW}, and then the rescoring model
tuned in \S\ref{RAT}.  The \code{rat.sh} script, used in translation
mode, performs both steps for us:
\begin{small}
\begin{alltt}
   > \textbf{make translate}
   ln -fs ../models
   cat models/decode/canoe.ini.cow > canoe.ini.cow.test1
   configtool check canoe.ini.cow.test1
   ok
   Generating test1.rat
   rat.sh -lb -n 4 trans -v -K 1000 -msrc ../corpora/test1_fr.rule \bs
      -f canoe.ini.cow.test1 models/rescore/rescore-model ../corpora/test1_fr.lc \bs
      && mv test1_fr.rule.rat test1.rat
   cp workdir-test1_fr.rule-1000best/1best test1.out
   [repeat for test2]
\end{alltt}
\end{small}
This produces both \code{test*.out}, the 1-best output of the decoder, and
\code{test*.rat}, the best translation according to the rescoring model.  The
files \code{test*.out} should be identical to the ones produced in
\S\ref{Decoding}, although they might differ in minor ways because of
rounding differences.

Before continuing, comment \code{DO_RESCORING = 1} back out and set
\code{TC_USE_SRC_MODELS = 1} again in \code{Makefile.params}, since these are
the values assumed below.  Also, run \code{make clean} to remove files
incompatible with the new truecasing workflow.

\subsubsection{Truecasing and Detokenizing} \label{Truecasing}

If you inspect the \code{test.out}, and/or code{test.rat} output files, you will
notice that they contain lowercase, tokenized text.  Two postprocessing steps
are required to restore normal case and normally spaced text: truecasing and
detokenizing.  The \code{test.ce} output file, on the other hand, already
contains truecased, detokenized text, because \code{translate.pl} itself
performs the steps we're about to do.

Truecasing is done by the \code{truecase.pl} script using the truecasing models
trained previously:
\begin{small}
\begin{alltt}
   > \textbf{make tc}
   Truecasing translate/test1.out.tc
   truecase.pl -text test1.out.oov -bos -encoding UTF-8 \bs
      -lm models/tc/lm-train_en-kn-3g.binlm.gz -map models/tc/lm-train_en.map \bs
      -src ../corpora/test1_fr.al -pal test1.out.pal -srclang fr \bs
      -srclm models/tc/lm-train_fr.nc1.binlm.gz > test1.out.tc
   [repeat for test2]
\end{alltt}
\end{small}

If you are using the old truecasing workflow, the same files will be generated
by \code{truecase.pl}, but the options will be different (the \code{-src},
\code{-pal}, \code{-srclang} and \code{-srclm} options will be omitted), and you
will not get the benefits of the new workflow.

If you are using rescoring, \code{make tc} will also truecase the rescoring
output, generating \code{test1.rat.tc}.

Detokenizing is done by the script \code{udetokenize.pl}, a rule-based
detokenizez for French, English or Spanish text encoded in UTF-8. Other
languages are not supported at this point but may already be handled partly
correctly by this script. As for other encodings, latin1 and cp-1252 are
supported by \code{detokenize.pl}.

\begin{small}
\begin{alltt}
   > \textbf{make detok}
   Detokenizing test1.out
   udetokenize.pl -lang=en test1.out test1.out.detok
   Detokenizing test1.out.tc
   udetokenize.pl -lang=en test1.out.tc test1.out.tc.detok
   [repeat for test2]
\end{alltt}
\end{small}

Detokenizing is not performed here on the \code{test*.ce} files because
it was already done by \code{translate.pl} along with truecasing.

If you are using rescoring, \code{make detok} will also detokenize the
rescoring output, generating \code{test*.rat.tc.detok} and
\code{test*.rat.detok}.

\subsection{Testing} \label{Testing}

Translation quality can be evaluated automatically using the BLEU metric, which
is a measure of how well the translation matches one or more reference
translations that are known to be correct (normally human translations). It is
based on the number of short word sequences that the translation has in common
with the references, and varies between 0 for no matches to 1 for a perfect
match. BLEU is calculated by the program \code{bleumain}:
\begin{small}
\begin{alltt}
   > \textbf{make bleu}
   Calculating BLEU for translate/test1.out.bleu
   bleumain -c test1.out ../corpora/test1_en.lc > test1.out.bleu
   Calculating BLEU for translate/test1.rat.bleu
   bleumain -c test1.rat ../corpora/test1_en.lc > test1.rat.bleu
   [repeat for test2]
   grep Human *.bleu
   test1.out.bleu:Human readable value: 19.41 +/- 3.31
   test1.rat.bleu:Human readable value: 19.34 +/- 3.37
   test2.out.bleu:Human readable value: 19.79 +/- 3.32
   test2.rat.bleu:Human readable value: 19.29 +/- 3.46
\end{alltt}
\end{small}
Here we show the BLEU score for rescoring too, for illustration, but without
\code{DO_RESCORING} you would only see \code{test*.out.bleu}.

The human readable value is the BLEU score multiplied by 100 rounded to 2
decimals. The full output from \code{bleumain}, saved in \code{*.bleu},
contains match statistics of various orders, followed by the global BLEU score
with a 95\% confidence interval, shown above.

The result above can also be obtained by typing \code{make eval}, or
\code{make all}, in your \code{toy.experiment} directory.

Here, we calculate only the BLEU scores on the lowercase output, using the
lowercase reference, but you can get BLEU scores for the truecased output by
giving the truecase translation and reference(s) to \code{bleumain}.

These results indicate that the translation produced by rescoring is worse
than the one produced by plain decoding. Given the small size of the test set,
it seems unlikely that the difference is statistically significant. To test
this hypothesis, we can use \code{bleucompare}, which does a comparison using
pairwise bootstrap resampling:
\begin{small}
\begin{alltt}
   > \textbf{bleucompare test1.rat test1.out REFS ../corpora/test1_en.lc}
   Comparing using BLEU
   test1.rat got max BLEU score in 26.2% of samples
   test1.out got max BLEU score in 73.8% of samples
   > \textbf{bleucompare test2.rat test2.out REFS ../corpora/test2_en.lc}
   Comparing using BLEU
   test2.rat got max BLEU score in 9.4% of samples
   test2.out got max BLEU score in 90.6% of samples
\end{alltt}
\end{small}
This indicates that the difference is indeed not significant for \code{test1}.
For \code{test2}, the difference is significant at the p=0.1 level, which is
not usually considered significant.  (Note that you may get completely
different results here, since the training corpus used is much too small to
produce predictable results.)

\tip \margintip Now that you have completed both training and testing, try
running \code{summarize-canoe-results.py *} in the parent directory of your
experiments again, it will show more interesting output.  \tipend

Once again, if you produced BLEU scores for the rescoring output, as shown
here, run \code{make clean} here before proceding to \S\ref{PLive}.

\section{Translating New Text} \label{NewText}

The translation methods shown in the previous section work well with the test
sets you dropped into the \code{corpora} directory, but it is not convenient for
translating new text.  You can use the \code{translate.pl} script for new text.
It is aware of the way this framework works and where it puts its models, so it
makes it easy to carry out the whole translation pipeline: tokenization,
lowercasing, decoding, truecasing, detokenization, with optional plugins you
can use to insert ad hoc code to handle special cases correctly.  See
\code{translate.pl -h} for details.  The \code{translate.sh} script you will
find in the framework calls \code{translate.pl} with some of the options
automatically filled in from the parameters you set in \code{Makefile.params}.
See \code{./translate.sh -h} for details.

\section{PortageLive} \label{PLive}

Now that we have a fully trained and tested system, you might want to deploy it
on a PortageLive translation server.

The PortageLive models are optimized in various ways for a run-time
environment.  Most importantly, they use our tightly packed model file format,
accessed via memory-mapped IO.
%; the various phrase tables are collapsed into a single table with the weights
%pre-applied; filtering of the phrase table, normally done on the fly, is
%pre-performed; and so on.
The result is a system that will run faster and with less memory on the
run-time server than equivalent requests would within the framework.

There is a \code{make} target in the framework to facilitate this process:
\code{make portageLive}.  This target, run directly from the top framework
directory, (\code{toy.experiment}), will cause all models to be converted and
filtered as required, and it will create a directory structure with symbolic
links to all the model files needed on a run-time translation server.  You can
then copy this structure to your run-time server by using a recursive copy
command that dereferences symbolic links.

\begin{small}
\begin{alltt}
   > \textbf{make portageLive}
   [...]
   [lots of prerequisite stuff happens]
   [...]
   make -C models portageLive
   mkdir -p portageLive
   [...]
   [lots of model conversion and preparation happens]
   [the portageLive directory structure is filled with the necessary symbolic links]
   [...]
   You now have all that is needed for PortageLive.
   From the framework root, run one of the following commands to
   transfer the PortageLive models to your server:
   rsync -Larz models/portageLive/* <REMOTE_HOST>:<DEST_DIR_ON_REMOTE_HOST>
   scp -r models/portageLive/* <REMOTE_HOST>:<DEST_DIR_ON_REMOTE_HOST>
   cp -Lr models/portageLive/* <DEST_DIR_ON_LOCAL_HOST>
\end{alltt}
\end{small}

The three sample commands printed at the end of the execution show you how to
perform a deep copy of the structure created, which will expand symbolic links
during the copy.

You can use \code{du -hL} to find out the total size that structure without
actually expanding it:
\begin{small}
\begin{alltt}
   > \textbf{du -hL models/portageLive/}
   12M     models/portageLive/models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.tpldm
   12M     models/portageLive/models/ldm
   15M     models/portageLive/models/tm/cpt.PI-kn3-zn.hmm3.fr2en.tppt
   15M     models/portageLive/models/tm
   1.9M    models/portageLive/models/tc/nc1-lm.fr.tplm
   516K    models/portageLive/models/tc/tc-map.en.tppt
   1.8M    models/portageLive/models/tc/tc-lm.en.tplm
   4.2M    models/portageLive/models/tc
   4.9M    models/portageLive/models/lm/lm-train_en-kn-5g.tplm
   4.9M    models/portageLive/models/lm
   36M     models/portageLive/models
   36M     models/portageLive/
\end{alltt}
\end{small}


\section{Resource Summary}

Now that the whole experiment has run, we can get a global summary of resource
usage from all the parts of the framework, as shown below:
%in Figure~\ref{FigTimeMem}.

%\begin{sidewaysfigure}
%\begin{figure}
%\caption{Time and memory resource summary}
%\label{FigTimeMem}
\begin{small}
\begin{alltt}
> \textbf{make time-mem}
\end{alltt}
\end{small}
\vspace{-2em}
\begin{tiny}
\begin{alltt}
Resource summary for $PORTAGE/toy.experiment:
         log.ce_model.cem:TIME-MEM                            WALL TIME: 39s      CPU TIME: 2m47s      VSZ: 0.137G    RSS: 0.019G
      confidence:TIME-MEM                                     WALL TIME: 39s      CPU TIME: 2m47s      VSZ: 0.137G    RSS: 0.019G
         log.canoe.ini.cow:TIME-MEM                           WALL TIME: 18m6s    CPU TIME: 32m47s     VSZ: 16.138G   RSS: 3.074G
         log.cpt.dev1.FILT:TIME-MEM                           WALL TIME: 6s       CPU TIME: 5s         VSZ: 0.042G    RSS: 0.007G
            log.aggregate:TIME-MEM                            WALL TIME: 0s       CPU TIME: 0s         VSZ: 0G        RSS: 0G
            log.decode:TIME-MEM                               WALL TIME: 11m18s   CPU TIME: 59m3s      VSZ: 0.107G    RSS: 0.016G
            log.eval:TIME-MEM                                 WALL TIME: 0s       CPU TIME: 0s         VSZ: 0G        RSS: 0G
            log.optimize:TIME-MEM                             WALL TIME: 33s      CPU TIME: 7m20s      VSZ: 16.003G   RSS: 3.669G
         logs:TIME-MEM                                        WALL TIME: 11m51s   CPU TIME: 1h6m23s    VSZ: 16.003G   RSS: 3.669G
      decode:TIME-MEM                                         WALL TIME: 30m3s    CPU TIME: 1h39m14s   VSZ: 16.138G   RSS: 3.669G
         log.hldm.hmm3.counts:TIME-MEM                        WALL TIME: 10s      CPU TIME: 9s         VSZ: 0.139G    RSS: 0.012G
         log.hldm.ibm2.counts:TIME-MEM                        WALL TIME: 7s       CPU TIME: 6s         VSZ: 0.140G    RSS: 0.012G
         log.hldm.ibm4.counts:TIME-MEM                        WALL TIME: 8s       CPU TIME: 7s         VSZ: 0.042G    RSS: 0.006G
         log.hldm.ibm4+hmm3+ibm2.fr2en:TIME-MEM               WALL TIME: 12s      CPU TIME: 9s         VSZ: 0.042G    RSS: 0.007G
         log.hldm.ibm4+hmm3+ibm2.fr2en.tpldm:TIME-MEM         WALL TIME: 15s      CPU TIME: 15s        VSZ: 0.042G    RSS: 0.007G
      ldm:TIME-MEM                                            WALL TIME: 52s      CPU TIME: 46s        VSZ: 0.140G    RSS: 0.012G
         log.lm-train_en-kn-5g.binlm:TIME-MEM                 WALL TIME: 15s      CPU TIME: 13s        VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en-kn-5g.lm:TIME-MEM                    WALL TIME: 4s       CPU TIME: 2s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en-kn-5g.tplm:TIME-MEM                  WALL TIME: 7s       CPU TIME: 5s         VSZ: 0.076G    RSS: 0.012G
         log.lm-train_fr-kn-5g.binlm:TIME-MEM                 WALL TIME: 13s      CPU TIME: 11s        VSZ: 0.042G    RSS: 0.007G
         log.lm-train_fr-kn-5g.lm:TIME-MEM                    WALL TIME: 3s       CPU TIME: 2s         VSZ: 0.042G    RSS: 0.007G
      lm:TIME-MEM                                             WALL TIME: 42s      CPU TIME: 32s        VSZ: 0.076G    RSS: 0.012G
         log.rescore-model:TIME-MEM                           WALL TIME: 2m10s    CPU TIME: 9m53s      VSZ: 15.900G   RSS: 0.017G
      rescore:TIME-MEM                                        WALL TIME: 2m10s    CPU TIME: 9m53s      VSZ: 15.900G   RSS: 0.017G
         log.lm-train_en-kn-3g.binlm:TIME-MEM                 WALL TIME: 5s       CPU TIME: 3s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en-kn-3g.lm:TIME-MEM                    WALL TIME: 2s       CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en-kn-3g.tplm:TIME-MEM                  WALL TIME: 5s       CPU TIME: 2s         VSZ: 0.057G    RSS: 0.008G
         log.lm-train_en.map:TIME-MEM                         WALL TIME: 2s       CPU TIME: 0s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en.map.tppt:TIME-MEM                    WALL TIME: 10s      CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en.nc1:TIME-MEM                         WALL TIME: 4s       CPU TIME: 2s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en.nc1.binlm:TIME-MEM                   WALL TIME: 4s       CPU TIME: 3s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en.nc1.lm:TIME-MEM                      WALL TIME: 2s       CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_en.tokss:TIME-MEM                       WALL TIME: 3s       CPU TIME: 2s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_fr.nc1.binlm:TIME-MEM                   WALL TIME: 5s       CPU TIME: 3s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_fr.nc1.lm:TIME-MEM                      WALL TIME: 2s       CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
         log.lm-train_fr.nc1.tplm:TIME-MEM                    WALL TIME: 4s       CPU TIME: 2s         VSZ: 0.072G    RSS: 0.008G
         log.lm-train_fr.tokss:TIME-MEM                       WALL TIME: 3s       CPU TIME: 2s         VSZ: 0.042G    RSS: 0.007G
      tc:TIME-MEM                                             WALL TIME: 51s      CPU TIME: 23s        VSZ: 0.072G    RSS: 0.008G
         log.cpt.merged.hmm3-kn3-zn.tm-train.fr2en:TIME-MEM   WALL TIME: 1s       CPU TIME: 0s         VSZ: 0G        RSS: 0G
         log.cpt.PI-kn3-zn.hmm3.fr2en:TIME-MEM                WALL TIME: 24s      CPU TIME: 23s        VSZ: 0.042G    RSS: 0.007G
         log.cpt.PI-kn3-zn.hmm3.fr2en.tppt:TIME-MEM           WALL TIME: 15s      CPU TIME: 17s        VSZ: 0.042G    RSS: 0.007G
         log.giza_filter:TIME-MEM                             WALL TIME: 0s       CPU TIME: 0s         VSZ: 0G        RSS: 0G
         log.hmm3.tm-train.en_given_fr:TIME-MEM               WALL TIME: 1m5s     CPU TIME: 3m5s       VSZ: 0.096G    RSS: 0.025G
         log.hmm3.tm-train.fr_given_en:TIME-MEM               WALL TIME: 1m0s     CPU TIME: 2m45s      VSZ: 0.096G    RSS: 0.025G
         log.ibm1.tm-train.en_given_fr:TIME-MEM               WALL TIME: 28s      CPU TIME: 42s        VSZ: 0.108G    RSS: 0.037G
         log.ibm1.tm-train.fr_given_en:TIME-MEM               WALL TIME: 27s      CPU TIME: 42s        VSZ: 0.108G    RSS: 0.036G
         log.ibm2.tm-train.en_given_fr:TIME-MEM               WALL TIME: 23s      CPU TIME: 35s        VSZ: 0.090G    RSS: 0.019G
         log.ibm2.tm-train.fr_given_en:TIME-MEM               WALL TIME: 23s      CPU TIME: 34s        VSZ: 0.090G    RSS: 0.019G
         log.jpt.hmm3.tm-train.fr-en:TIME-MEM                 WALL TIME: 13s      CPU TIME: 17s        VSZ: 0.240G    RSS: 0.028G
         log.jpt.ibm2.tm-train.fr-en:TIME-MEM                 WALL TIME: 8s       CPU TIME: 12s        VSZ: 0.243G    RSS: 0.031G
         log.jpt.ibm4.tm-train.fr-en:TIME-MEM                 WALL TIME: 10s      CPU TIME: 13s        VSZ: 0.250G    RSS: 0.032G
         log.jpt.merged.tm-train.fr-en:TIME-MEM               WALL TIME: 0s       CPU TIME: 0s         VSZ: 0G        RSS: 0G
         log.tm-train.hmm3.fr2en.align:TIME-MEM               WALL TIME: 22s      CPU TIME: 20s        VSZ: 0.041G    RSS: 0.007G
         log.tm-train.ibm2.fr2en.align:TIME-MEM               WALL TIME: 6s       CPU TIME: 5s         VSZ: 0.041G    RSS: 0.007G
         log.tm-train.ibm4.fr2en.align:TIME-MEM               WALL TIME: 3s       CPU TIME: 1s         VSZ: 0.041G    RSS: 0.007G
            B.gizacfg.log:TIME-MEM                            WALL TIME: 1m44s    CPU TIME: 6m28s      VSZ: 0.266G    RSS: 0.056G
            en.lc.vcb.classes.log:TIME-MEM                    WALL TIME: 10s      CPU TIME: 9s         VSZ: 0.041G    RSS: 0.007G
            F.gizacfg.log:TIME-MEM                            WALL TIME: 1m39s    CPU TIME: 6m10s      VSZ: 0.274G    RSS: 0.055G
            fr.lc.vcb.classes.log:TIME-MEM                    WALL TIME: 11s      CPU TIME: 10s        VSZ: 0.041G    RSS: 0.007G
         tm-train:TIME-MEM                                    WALL TIME: 3m44s    CPU TIME: 12m56s     VSZ: 0.274G    RSS: 0.056G
      tm:TIME-MEM                                             WALL TIME: 9m12s    CPU TIME: 23m7s      VSZ: 0.274G    RSS: 0.056G
   models:TIME-MEM                                            WALL TIME: 44m29s   CPU TIME: 2h16m41s   VSZ: 16.138G   RSS: 3.669G
      log.test1.ce:TIME-MEM                                   WALL TIME: 53s      CPU TIME: 2m7s       VSZ: 0.298G    RSS: 0.025G
      log.test1.out:TIME-MEM                                  WALL TIME: 44s      CPU TIME: 1m37s      VSZ: 0.107G    RSS: 0.016G
      log.test1.out.tc:TIME-MEM                               WALL TIME: 3s       CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
      log.test1.rat:TIME-MEM                                  WALL TIME: 3m54s    CPU TIME: 12m53s     VSZ: 0.106G    RSS: 0.023G
      log.test1.rat.tc:TIME-MEM                               WALL TIME: 2s       CPU TIME: 0s         VSZ: 0.042G    RSS: 0.007G
      log.test2.ce:TIME-MEM                                   WALL TIME: 53s      CPU TIME: 2m21s      VSZ: 0.556G    RSS: 0.236G
      log.test2.out:TIME-MEM                                  WALL TIME: 44s      CPU TIME: 1m47s      VSZ: 0.106G    RSS: 0.015G
      log.test2.out.tc:TIME-MEM                               WALL TIME: 3s       CPU TIME: 1s         VSZ: 0.042G    RSS: 0.007G
      log.test2.rat:TIME-MEM                                  WALL TIME: 4m31s    CPU TIME: 15m29s     VSZ: 0.107G    RSS: 0.025G
      log.test2.rat.tc:TIME-MEM                               WALL TIME: 2s       CPU TIME: 0s         VSZ: 0.042G    RSS: 0.007G
   translate:TIME-MEM                                         WALL TIME: 11m49s   CPU TIME: 36m16s     VSZ: 0.556G    RSS: 0.236G
TIME-MEM                                                      WALL TIME: 56m18s   CPU TIME: 2h52m57s   VSZ: 16.138G   RSS: 3.669G
\end{alltt}
\end{tiny}
%\end{sidewaysfigure}
%\end{figure}

The output of \code{make time-mem} tells us how much RAM (``RSS'') and
virtual memory (``VSZ'') was used by each step of the process. It also tells us
the total amount of CPU time and the actual elapsed time (``wall time'').  When
running in parallel, the total CPU time will be higher than the wall time, as is
the case here.

In each summary line (the ones outdented, those for each component, and the
global summary at the end), the wall and CPU times are the sum of the
lines it summarizes, whereas the two memory figures are the maximum values from
the lines that it summarizes.

The memory figures might be a bit misleading: they reflect the maximum
RAM/virtual memory needed by any one component within a run.  In the case of a
process that is parallelized, say, 10 ways, the figure reflects the most memory
any one of the 10 parallel processes used, not the total used at any given
time.  When using this output to plan hardware resources, you should take into
account the parallelism level you want to use for each step.

The numbers here are not very interesting because we ran on such a small
system, but they'll be a lot more informative when you run on real data.

The second type of resource summary you can produce is the disk space occupied
by the models.  When you run \code{make summary}, you will first get the
\code{time-mem} report, then the following disk space report:
\begin{small}
\begin{alltt}
   > \textbf{make summary}
   [output from time-mem plus:]

   Disk usage for all models:
   4.7M    models/tm/ibm1.tm-train.en_given_fr.gz
   4.6M    models/tm/ibm1.tm-train.fr_given_en.gz
   3.8M    models/tm/ibm2.tm-train.en_given_fr.gz
   148K    models/tm/ibm2.tm-train.en_given_fr.pos.gz
   3.8M    models/tm/ibm2.tm-train.fr_given_en.gz
   152K    models/tm/ibm2.tm-train.fr_given_en.pos.gz
   484K    models/tm/ibm4.tm-train.en_given_fr.gz
   460K    models/tm/ibm4.tm-train.fr_given_en.gz
   4.0K    models/tm/hmm3.tm-train.en_given_fr.dist.gz
   2.0M    models/tm/hmm3.tm-train.en_given_fr.gz
   4.0K    models/tm/hmm3.tm-train.fr_given_en.dist.gz
   2.1M    models/tm/hmm3.tm-train.fr_given_en.gz
   3.5M    models/tm/jpt.hmm3.tm-train.fr-en.gz
   1.3M    models/tm/jpt.ibm2.tm-train.fr-en.gz
   2.4M    models/tm/jpt.ibm4.tm-train.fr-en.gz
   4.4M    models/tm/jpt.merged.tm-train.fr-en.gz
   13M     models/tm/cpt.PI-kn3-zn.hmm3.fr2en.gz
   14M     models/tm/cpt.PI-kn3-zn.hmm3.fr2en.tppt
   3.4M    models/lm/lm-train_en-kn-5g.binlm.gz
   5.4M    models/lm/lm-train_en-kn-5g.lm.gz
   4.9M    models/lm/lm-train_en-kn-5g.tplm
   3.7M    models/lm/lm-train_fr-kn-5g.binlm.gz
   5.9M    models/lm/lm-train_fr-kn-5g.lm.gz
   568K    models/decode
   18M     translate
   16M     models/tc
   117M    total

   Disk usage for portageLive models:
   12M     models/portageLive/models/ldm/hldm.ibm4+hmm3+ibm2.fr2en.tpldm
   12M     models/portageLive/models/ldm
   14M     models/portageLive/models/tm/cpt.PI-kn3-zn.hmm3.fr2en.tppt
   14M     models/portageLive/models/tm
   1.9M    models/portageLive/models/tc/nc1-lm.fr.tplm
   516K    models/portageLive/models/tc/tc-map.en.tppt
   1.8M    models/portageLive/models/tc/tc-lm.en.tplm
   4.2M    models/portageLive/models/tc
   4.9M    models/portageLive/models/lm/lm-train_en-kn-5g.tplm
   4.9M    models/portageLive/models/lm
   35M     models/portageLive/models
   35M     models/portageLive
\end{alltt}
\end{small}

This report differs from a simple \code{du -h} in that it looks specifically
for the model files. It also shows the size of the portageLive models prepared
for deployment on a run-time translation server.

\section*{Final Note}
Because of differences in rounding, optimization, random number generation,
compilers, hardware, etc., results, especially numerical ones, are expected to
vary on different systems and are shown in this document only as an indication
of the type of output to expect, especially given the trivial size of the
corpus used.

\end{document}
